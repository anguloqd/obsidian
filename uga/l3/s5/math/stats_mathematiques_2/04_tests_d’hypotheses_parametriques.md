# 04 // tests d‚Äôhypoth√®ses param√©triques

# Introduction

## Th√©orie de la d√©cision

Dans l‚Äôapproche param√©trique la plus g√©n√©rale, un test statistique consiste a d√©cider d‚Äôaccepter ou de rejeter une hypoth√®se sp√©cifiant que $Œ∏$ appartient √† un ensemble de valeurs $Œò_0$. Cette hypoth√®se de r√©f√©rence est appel√©e *hypoth√®se nulle* et est not√©e $H_0$. √Ä contrario, on d√©finit l‚Äô*hypoth√®se alternative*, not√©e $H_1$, pour laquelle $Œ∏$ appartient a $Œò_1 = Œò\setminus Œò_0$. 

En bref, on identifiera cette situation en √©crivant que l‚Äôon teste :

$$
H_0:\theta\in\Theta_0 \text{ vs. }H_1:\theta\in\Theta_1
$$

Notons qu‚Äôon ne pourra jamais dire avec s√ªret√© que $\theta\in\Theta_0$ ou $\Theta_1$, on ne pourra jamais conna√Ætre $\theta$. Donc, √ßa se lira plut√¥t comme une proposition probabiliste comme ‚Äúon est $95\%$ s√ªrs que $\theta\in\Theta_0$‚Äù et pareil pour $\Theta_1$. Il faudra d√©finir une mani√®re ‚Äúprobabiliste‚Äù de prendre cette d√©cision, souvent par des intervalles de confiance d‚Äôune statistique $T$ √† deux bornes ou juste born√© d‚Äôun c√¥t√©, mais g√©n√©ralement connue comme la **r√©gion de d√©cision**.

Il pourrait se donner trois cas :

- Nulle et alternative simples, o√π $\Theta=\{\theta_0,\theta_1\}$
    
    $$
    H_0:\theta=\theta_0\text{ vs. }H_1:\theta=\theta_1
    $$
    
- Nulle simple et alternative multiple
    
    $$
    H_0:\theta=\theta_0\text{ vs. }H_1:\theta\ne\theta_0
    $$
    
- Nulle et alternative multiples
    
    $$
    H_0:\theta\in\Theta_0\text{ vs. }H_1:\theta\in\Theta_1
    $$
    

# Les tests

## Hypoth√®se simple, alternative simple

### Premi√®res d√©finitions

C‚Äôest le cas o√π

$$
H_0:\theta=\theta_0\text{ vs. }H_1:\theta=\theta_1
$$

Un test pour $H_0$ est une r√®gle de d√©cision fond√©e sur la valeur r√©alis√©e $t\in\mathbb{R}$ d‚Äôune statistique $T$, appel√©e statistique de test, qui est un estimateur de $\theta$. La r√®gle suit :

- Si $t\in A$, une partie de $\mathbb{R}$, donc on accepte $H_0 : \theta=\theta_0$;
- Si $t \in \bar A$, qui est $\mathbb{R}\setminus A$, on rejette $H_0$ et on accepte $H_1:\theta=\theta_1$.

**Si √ßa peut nous servir, $A$ veut dire ‚Äúacceptation‚Äù et particuli√®rement l‚Äôacceptation de la nulle $H_0$, et normalement dans le test on cherche $\bar A$ c√†d. rejeter $H_0$**. Le tableau ci-dessous nous montre les situations d√©sirables et non d√©sirables :

|  | Rejeter $\mathcal{H}_0$ $(PP)$ | ‚ÄúAccepter‚Äù $\mathcal{H}_0$ $(PN)$ |
| --- | --- | --- |
| $\mathcal{H}_0$ est fausse **$(P)$** | OK $(TP)$ : $1-\beta$ | Erreur de Type II $(FN)$ : $\beta$ |
| $\mathcal{H}_0$ est vraie **$(N)$** | Erreur de Type I $(FP)$ : $\alpha$ | OK $(TN)$ : $1-\alpha$ |

![untitled](ressources/04_tests_d‚Äôhypotheses_parametriques_untitled.png)

[Binary classification](https://en.wikipedia.org/wiki/Binary_classification)

[Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)

√âtant donn√©e que la d√©cision se fonde sur un r√©sultat d‚Äôorigine al√©atoire, on caract√©risera chaque erreur par sa probabilit√©. Une probabilit√© d‚Äôerreur est appel√©e *risque.* Particuli√®rement :

- Le risque de Type I est $\alpha=\mathbb P(t\in\mathcal C|H_0)$
√Ä lire : la probabilit√© d‚Äôobserver une valeur de $T$ qui nous m√®ne √† penser que $H_0$ est fausse tant qu‚Äôelle est en fait vraie.
- Le risque de Type II est $\beta=\mathbb P(t\in \bar {\mathcal C} |H_1)$.
√Ä lire : la probabilit√© d‚Äôobserver une valeur de $T$ qui nous m√®ne √† penser que $H_0$ est vraie tant que, en fait, $H_1$ est vraie.

Dans la pratique, on fait

1. On choisit arbitrairement $\alpha$, normalement $\alpha=0.05$.
2. Si $\alpha=0.05$, on d√©termine la r√©gion critique $\mathcal C$ tel que $\mathbb P(t\in\bar {\mathcal C }|H_0)=0.05.$
    1. On voit aussi que la loi de la statistique de test $T$ doit √™tre parfaitement connue sous $H_0$, tant que c‚Äôest pas le cas de $T$ sous $H_1$.
    2. La construction d‚Äôun test consiste donc √† rechercher une statistique pertinente dont on conna√Æt la loi sous $H_0$.

La r√©gion de rejet √©tant ainsi d√©termin√©e, la r√©gion de non rejet l‚Äôest aussi et donc √©galement le risque de deuxi√®me esp√®ce $Œ≤$ . Il est essentiel de garder √† l‚Äôesprit que, dans une proc√©dure de test, **on contr√¥le le risque** $Œ±$ mais pas le risque $Œ≤$.

Il est naturel de poser comme exigence que la statistique ait une plus forte propension a tomber dans la r√©gion de rejet quand $H_1$ est la bonne hypoth√®se, ce que nous transcrivons math√©matiquement par la condition que la probabilit√© de rejeter $H_0$ soit plus √©lev√©e sous $H_1$ que sous $H_0$, et si possible nettement plus √©lev√©e. Ceci est la m√™me chose qu‚Äôun test dit ‚Äúsans biais‚Äù, comme on le mentionne dans la section suivante.

$$
\underbrace{\mathbb P(t\in\mathcal C|H_1)}_{1-\beta}\ge \underbrace{\mathbb P(t\in\mathcal C|H_0)}_\alpha
$$

Toute la recherche d‚Äôune bonne statistique de test, intuitive ou non, repose sur ce principe que nous allons maintenant formaliser avec la notion de puissance.

### Puissance et biais d‚Äôun test

La puissance d‚Äôun test est d√©finie comme la capacit√© de rejeter $H_0$ quand elle est effectivement fausse, donc

$$
\mathbb P(t\in\mathcal C|H_1)=1-\beta
$$

On pourrait aussi d√©finir la puissance d‚Äôun test comme une fonction. Particuli√®rement, une fonction $\phi$ pour des valeurs du param√®tre $\theta$ :

$$
\phi(\theta)=\mathbb P(t\in\mathcal C|\theta)
$$

Un test est dit *sans biais* si sa puissance est sup√©rieure ou √©gale √† son risque $\alpha$, donc

$$
\mathbb P(t\in\mathcal C|H_1)\ge\mathbb P(t\in\mathcal C|H_0) \iff 1-\beta\ge\alpha
$$

En conclusion, **une condition naturelle pour qu‚Äôune statistique soit √©ligible pour tester une hypoth√®se est qu‚Äôelle induise un test sans biais.** Incidemment, ce terme de ‚Äúsans biais‚Äù n‚Äôa pas de rapport direct avec la notion de biais d‚Äôun estimateur. On entrevoit d√®s lors que le choix entre plusieurs tests potentiels, pour une hypoth√®se nulle donn√©e, **se jouera sur la puissance**.

**D√©finition : comparaison de tests par la puissance**. On pourrait comparer la puissance de deux tests : un test $\tau_1$ est plus puissant, au niveau $\alpha$, que le test $\tau_2$ si la signification de $\tau_2$ est $\alpha$ ou inf√©rieur (habituellement normal), et la puissance de $\tau_1$ est sup√©rieure √† celle de $\tau_2$.

L‚Äôobjectif sera finalement de rechercher le test le plus puissant parmi tous. Il existe un tel test dans le cas o√π $H_0$ et $H_1$ sont des hypoth√®ses simples, mais cela n‚Äôest pas n√©cessairement vrai dans le cas o√π l‚Äôhypoth√®se alternative est multiple. Par ailleurs et en g√©n√©ral, quand une statistique de test donne le test le plus puissant a un niveau donn√©, elle reste optimale √† tout autre niveau.

**D√©finition : convergence de tests**. Finalement, on peut dire que la proc√©dure de test est *convergente* quand, pour une suite de tests $\{\tau_n\}$ de taille d‚Äô√©chantillon $n$, dont on peut extraire une suite de leurs puissances $\{1-\beta_n\}$, $\lim_{n\rightarrow\infty} (1-\beta_n) = 1$ ou, √©galement, $\lim_{n\rightarrow\infty} \beta_n=0$. Donc, la puissance tend vers $1$ quand l‚Äô√©chantillon s‚Äôagrandit.

En d‚Äôautre termes, on doit avoir la garantie qu‚Äôon gagne √† observer de tr√®s grands √©chantillons, de sorte qu‚Äôil est pratiquement s√ªr de d√©tecter une hypoth√®se nulle $H_0$ qui serait fausse √† la limite. 

### üîñ Extra : estimateur *exhaustive*

Avant de pr√©ciser cela notons qu‚Äôun test $\tau$, tel que nous avons pr√©sent√© les choses, est parfaitement d√©fini par le couple statistique de test ‚Äî r√©gion d‚Äôacceptation. :

$$
\tau=\{T,A\}
$$

Ceci est valide car $Œ±$, $Œ≤$ et la puissance $(1 ‚àí Œ≤)$ en d√©coulent (m√™me si conceptuellement le choix de $Œ±$ pr√©c√®de celui de $A$, mais pour $Œ±$ fix√© il y a diff√©rentes fa√ßons de choisir une r√©gion‚Äîg√©n√©ralement un intervalle‚Äîde probabilit√© $Œ±$ sur la loi de $T$ sous $H_0$).

En fait, il n‚Äôest m√™me pas n√©cessaire de se r√©f√©rer √† une statistique de test. Mettons en √©vidence la fonction de l‚Äô√©chantillon d√©finissant la statistique $T=h(\mathbf{x})$ et $\mathbb A$ l‚Äôensemble des points de $\mathbb{R}^n$, r√©alisations du vecteur $\mathbf{X}$, d√©fini par

$$
\mathbb A=\{\mathbf{x}:\underbrace{h(\mathbf{x})}_t\in \bar {\mathcal C} \}
$$

Mais notons, $(t\in \bar {\mathcal C})$ est √©quivalent $(\mathbf{X}\in\mathbb A)$. Le test est donc parfaitement d√©fini par la r√©gion d‚Äôacceptation $\mathbb A$ dans $\mathbb{R}^n$. D‚Äôune fa√ßon g√©n√©rale, un test s‚Äôidentifie √† une r√©gion d‚Äôacceptation dans l‚Äôespace des r√©alisations. Cela dit, cette vision des tests est rarement utile, et normalement on passe par une statistique $T$.

## Test de RV : rapport de vraisemblance simple (et multiple)

### D√©finition, cas simple

Dans cette section, on reste sur une hypoth√®se et alternatives simples. Un test de rapport de vraisemblance simple de $H_0$ vs. $H_1$ au niveau $\alpha_0$  fix√© au pr√©alable, est un test d√©fini de la forme suivante :

$$
\tau = \left\{ T=RV=\frac{L(\theta_0|\mathbf{x})}{L(\theta_1|\mathbf{x})},\space  \mathcal C=\{RV: RV \le C\}\right\}
$$

La valeur de $C$ est d√©termin√©e par le risque fixe $\alpha_0 = \mathbb P(\mathcal C|\theta=\theta_0)$, donc **il est vraiment important de fixer $\alpha=\alpha_0$ avant de calculer le rapport de vraisemblance**. Dans la pratique, $C$ est normalement plus petit que $1$, pour garantir un $\alpha$ faible.

Si la r√©gion $\mathcal C$ obtenue par ce test entre hypoth√®se simples ne d√©pend pas de la valeur choisie de $\theta_1$, alors on aura un test UPP.

Quelque chose d‚Äôint√©r√™t c‚Äôest qu‚Äôon peut calculer, dans le cas simple, une fonction de puissance explicite.

Le d√©roulement d‚Äôun exo. est souvent comme suit :

1. Proposer la densit√© des $X_i$, suppos√©es iid., puis calculer la vraisemblance.
2. Dire qu‚Äôon fixe $\alpha=\alpha_0$.
3. Calculer $RV$ et puis affirmer que $RV \le C$.
4. Simplifier $RV$ le plus possible en isolant les $x_i$ d‚Äôun c√¥t√© et la constante de l‚Äôautre c√¥t√© absorbe tous les termes et facteurs n√©cessaires.
    1. Un exemple c‚Äôest d‚Äôarriver √† $\sum_{i=1}^n x_i \ge C_4$. Notons que ceci nous montre la condition pour avoir un vrai positif. $C_4$ est la quantit√© que la $C$ original a absorb√© d‚Äôautre termes.
5. On revient √† la d√©finition de $\alpha_0 =\mathbb P (\sum_{i=1}^n X_i \ge C_4 | \theta=\theta_0)$. On calcule finalement la valeur concr√®te de $C$.

### Cas $H_0$ simple vs. $H_1$ multiple

Il y a deux sous-cas : quand $H_1$ est unilat√©rale (qui est le meilleur cas pour nous) et quand $H_1$ est bilat√©rale.

Faisons le premier sous-cas. Supposons $H_0 : \theta = \theta_0$.

- Si $H_1 : \theta > \theta_0$, on dira que la region critique du test de Neyman-Pearson sera celui du test $H_0 : \theta = \theta_0$ vs. $H_1 : \theta = \theta_1$, o√π $\theta_1 > \theta_0$.
- C‚Äôest analogique si $H_1 : \theta < \theta_0$.

Un probl√®me qui arrive c‚Äôest qu‚Äôon ne peut plus calculer une fonction de puissance explicite si $\theta_1$ n‚Äôest pas simple.

Pour le deuxi√®me sous-cas, il n‚Äôexiste pas de test UPP. La r√©gion critique au risque $\alpha_0$ s‚Äôobtient par la r√©union des regions critiques $\mathcal C_1$ et $\mathcal C_2$ des tests unilat√©raux √† risque $\alpha_0/2$.

### Th√©or√®mes notables

Il y a trois th√©or√®mes importantes cons√©quence de RV :

- **Lemme de Neyman-Pearson**.
Le test de RV est le plus puissant de tous, pour $\alpha\in ]0,1[$.
- Le test de RV est sans biais : $1-\beta\ge\alpha$.
Pour Neyman et Pearson, leur esprit aussi est que $\beta > \alpha$, selon le prof. Quand c‚Äôest pas le cas, on appel ceci ‚Äúincoh√©rent‚Äù.
- Le test de RV est convergent ($n\rightarrow\infty , \beta\rightarrow0$), sous de conditions mineures.

Les preuves des deux premiers th√©or√®mes sont dans le mat√©riel de Michel Lejeune, √† partir de la page 208.

### üîñ Extra : caract√©risation d‚Äôun test

S‚Äôagissant d‚Äôestimer $Œ∏$, certaines statistiques peuvent √™tre exclues du fait qu‚Äôelles n‚Äôutilisent pas de fa√ßon exhaustive toute l‚Äôinformation contenue dans l‚Äô√©chantillon $\mathbf{X}$. √Ä l‚Äôinverse, on peut s‚Äôattendre √† ce qu‚Äôun ‚Äúbon‚Äù estimateur soit une statistique qui ne retienne que ce qui est utile de l‚Äô√©chantillon. Une statistique $T(\mathbf{X})$ est donc dite exhaustive si

$$
\mathbb P \Big(\mathbf{X}=\mathbf{x}\space|\space T(\mathbf{X})=t,\theta\Big)=\mathbb P \Big(\mathbf{X}=\mathbf{x}\space|\space T(\mathbf{X})=t\Big)
$$

ou, alternativement,
$$
I(\theta|S(\mathbf{X}))=I(\theta|\mathbf{X}). 
$$
$$
\text{ Rappel : } 
\\[8pt]
I(\theta|\mathbf{x})=\mathbb E\left[\left( \frac{\partial \ln L}{\partial\theta}(\theta)-\cancel{\mathbb E[s(\theta)]}^{\space0}\right)^2\right]=\int_\Omega \left( \frac{\partial \ln L}{\partial\theta}(\theta)\right)^2L(\theta|\mathbf{x})d\theta
$$

En pratique, on n‚Äô√©voque pas cette √©galit√© pour les calculs. mais on passe plut√¥t par le **th√©or√®me de factorisation**. On peut dire aussi que la statistique $T$ est exhaustive s‚Äôils existent deux fonctions $g$ et $h$ mesurables telles que :

$$
f(\mathbf{x}|\theta)=g(T(\mathbf{x})|\theta)\times h(\mathbf{x})
$$

On voit bien qu‚Äôon parle de ‚Äúfactorisation‚Äù parce qu‚Äôon finit par exprimer $f$ comme le produit de deux facteurs. En plus, $g$ contient information sur la statistique r√©alis√©e (et donc indirectement de l‚Äô√©chantillon) et le param√®tre‚Äîtant que $h$ contient information juste sur l‚Äô√©chantillon r√©alis√©. Tout le membre de droit est appel√© ‚Äúla densit√© conjointe‚Äù dans ce contexte, un nom qu‚Äôon utilisera souvent.

Deux propositions importantes d√©coulent d√©j√† de la d√©finition d‚Äôexhaustivit√© :

- Si $T$ est une statistique exhaustive et $T^\prime$ une statistique telle qu‚Äôon peut r√©√©crire $T$ comme fonction de $T^\prime$, donc $T^\prime$ est exhaustive elle-m√™me.
- Pareillement, si c‚Äôest $T^\prime$ la statistique fonction de $T$ comme $T^\prime(\mathbf{X})=f(T(\mathbf{X}))$ qui est exhaustive, et si $f$ est bijective, donc $T^\prime$ est aussi exhaustive.

En plus, on peut parler de exhaustivit√© minimale d‚Äôune statistique $T^*$ si elle est exhaustive et si, pour tout statistique exhaustive $T$, on peut trouver une fonction $f$ telle que $T^*(\mathbf{X})=f(T(\mathbf{X}))$. Une condition de r√©gularit√© est que le domaine ou support de la densit√© des $\mathbf{X}$ ne d√©pend pas du param√®tre inconnu, ce qui √©limine la loi uniforme.

En r√®gle g√©n√©rale, une statistique exhaustive est minimale. Tout estimateur pertinent est fonction d‚Äôune statistique exhaustive minimale.

## Hypoth√®ses multiples

### Red√©finitions du cas simple

Un test d‚Äôhypoth√®se et alternative simple est souvent peu r√©aliste. Ici, on suppose que $\theta$ peut prendre plusieurs valeurs sous $H_0$ et plusieurs valeurs diff√©rents sous $H_1$. C‚Äôest la d√©finition g√©n√©rale, ou on utilise $\theta\in\Theta_0$ et non pas l‚Äô√©galit√© √† une seule valeur exacte comme $\theta=\theta_0$, et de m√™me pour $H_1$.

$$
H_0:\theta\in\Theta_0 \text{ vs. }H_1:\theta\in\Theta_1
$$

Supposons ici que $H_0$ est vraie, donc $\theta\in\Theta_0$. On n‚Äôutilisera plus $\alpha$ pour parler du risque de premi√®re esp√®ce, mais de $\mathbb P(T\in\bar A|H_0)$. Notons que, pour chaque valeur possible du param√®tre $\theta\in\Theta_0$ on peut associer un risque de premi√®re esp√®ce diff√©rent, donc $\alpha(\theta)$.

Le ***niveau du test*** $\alpha$ est donc d√©fini comme le plus grand risque possible induit par le param√®tre $\theta$ si c‚Äôest le cas que $H_0$ est vraie : 

$$
\alpha=\sup_{\theta\in\Theta_0}\alpha(\theta)
$$

C‚Äôest analogique pour l‚Äôhypoth√®se alternative $H_1:\theta\in\Theta_1$ et le risque de deuxi√®me esp√®ce $\beta(\theta)$. La d√©finition d‚Äôun test sans biais est aussi analogique. On prend une notation de puissance comme suit :

$$
h(\theta)=1-\beta(\theta)
$$

Les d√©finitions de test plus puissants changent un peu, on parlera maintenant d‚Äôun test $\tau_1$ uniform√©ment plus puissant qu‚Äôun autre $\tau_2$ au niveau $\alpha$ si : 

$$
\alpha_{\tau_2} \le \alpha,

\hspace{8pt}

\forall \theta\in\Theta_1:h_1(\theta) \ge h_2(\theta)

\hspace{6pt}
\text{et}
\hspace{6pt}

\exists\theta^*\in\Theta_1 :h_1(\theta^*) > h_2(\theta^*).
$$

Voyons que cela veut juste dire que la puissance de $\tau_1$ est sup√©rieure quand $H_0$ est fausse ou √©galement quand $H_1$ est vraie, donc on suppose que $\theta\in\Theta_1$.

Finalement, on peut parler du test uniform√©ment le plus puissant au niveau $\alpha$, ou du test UPP en $\alpha$ tout court. Ce test est uniform√©ment plus puissante que tous les autres tests.

Ceci √©tant dit, rien ne garantit l‚Äôexistence de ce test et en fait souvent il n‚Äôexiste pas pour toutes les valeurs de $\theta$. Le plus commun est que il existe $\tau_1$ qui est le plus puissant pour quelques valeurs de $\theta$, $\tau_2$ pour quelques autres valeurs de $\theta$, etc.

N√©anmoins, le r√©sultat de Neyman-Pearson obtenu dans la situation simple s‚Äô√©tend assez naturellement √† des situations d‚Äôhypoth√®ses multiples dites unilat√©rales, tr√®s fr√©quentes en pratique.

### Cas de tests unilat√©raux

Le test prend la forme suivante :

$$
H_0:\theta\le\Theta_0 \text{ vs. }H_1:\theta>\Theta_1\text{\hspace{8pt}ou\hspace{8pt}}H_0:\theta\ge\Theta_0 \text{ vs. }H_1:\theta<\Theta_1
$$

Cette structure de test est assez commune et il y a une proposition importante sur ce type de test : l‚Äôexistence d‚Äôun test UPP est garantie si 

- $T=t(\mathbf{x})$ est une statistique exhaustive minimale
- Pour toute couple $(\theta,\theta^\prime)$ tel que $\theta<\theta^\prime$, le RV $L(\theta|\mathbf{x})/L(\theta^\prime|\mathbf{x})$ est monotone de $T$.

Dans ce cas, la r√©gion d‚Äôacceptation est de la forme $T<k$ ou $T>k$.

On peut aussi garantir l‚Äôexistence d‚Äôun test UPP dans une autre situation : si la loi m√®re dont on tire l‚Äò√©chantillon $f$ est de la **classe exponentielle** et si $\eta(\theta)$ est monotone, donc il existe un test UPP et la r√©gion de rejet est $\sum_{i=1}^nT(x_i)<k$ ou $\sum_{i=1}^nT(x_i)>k$. 

> [!note]
> La ‚Äúclasse exponentielle‚Äù est une famille de fonctions qui peuvent s‚Äôexprimer de la forme suivante
>
> $$
> f(x|\theta)=h(x)g(\theta)e^{\eta(\theta)T(x)}\text{ ou } f(x|\theta)=h(x)e^{\eta(\theta)T(x)-A(\eta)}
> $$
>
> La premi√®re forme est celle pr√©sent√©e dans le mat√©riel de Michel Lejeune, tant que la deuxi√®me forme est pr√©sent√©e dans la page de Wikip√©dia. Dans le cas de Wikip√©dia :
>
> - $T(x)$ est une statistique exhaustive.
> Normalement, cette statistique est juste $x$, l‚Äôobservation.
> - $h(x)$ est la ‚Äúmesure de base‚Äù, une fonction positive
> Attention ! Ce $h(x)$ ne doit pas avec la fonction puissance d‚Äôun test !
> - $\eta(\theta)=T^\prime(x) \ln f(x|\theta)$ est le ‚Äúparam√®tre naturel‚Äù.
> Sa d√©finition est la d√©riv√©e de la fonction g√©n√©ratrice cumulante. qui est une fonction qui capture toutes les propri√©t√©s statistiques de la distribution.
> - $A(\theta)$ est le log. du facteur de normalisation.
>
> $$
> A(\eta)=\ln\left( \int_Xh(x)e^{\eta(\theta)T(x)}dx \right)
> $$
>
> - Si jamais on pr√©f√®re la version avec $g(\theta)$, √† savoir que
> $g(\theta)=e^{-A(\eta)} \iff A(\eta)=-\ln(g(\theta))$.
>
> On pourra voir un liste de plusieurs distributions √©crites sous cette forme [ici](https://en.wikipedia.org/wiki/Exponential_family#:~:text=as%20logit.-,Table%20of%20distributions,-%5Bedit%5D).

Il faudrait consacrer un moment pour parler du choix de $H_0$ dans ce cas : si on la choisit $\theta \le\theta_0$ ou $\theta\ge\theta_0$. **G√©n√©ralement, l‚Äôerreur qui serait consid√©r√©e le plus ind√©sirable et probl√©matique serait l‚Äôerreur assign√© √† $\alpha$.** Si on d√©signe une alarme d‚Äôincendie, on voudrait que son erreur de premi√®re esp√®ce soit qu‚Äôelle ne sonne pas tant qu‚Äôil y a du feu, et on laisse l‚Äôerreur de deuxi√®me esp√®ce le cas plus courant de sonner quand il n‚Äôy a pas du feu.

### Cas de tests bilat√©raux

Le test prend la forme suivante :

$$
H_0:\theta=\theta_0 \text{ vs. }H_1:\theta\ne\theta_0

\text{\hspace{8pt}ou\hspace{8pt}}

H_0:\theta\in[\theta_1,\theta_2] \text{ vs. }H_1:\theta\notin[\theta_1,\theta_2]
$$

Le premier test ici est plut√¥t utilis√© quand $\theta$ repr√©sent√© l‚Äô√©cart entre deux param√®tres de la population, disons que $H_0$ est que la diff√©rence entre la moyenne de deux populations est nulle, donc $H_0 : \theta=\mu_1-\mu_2=0$.

Le deuxi√®me test est utilis√© si le param√®tre est dans un intervalle de tol√©rance acceptable. La r√©gion d‚Äôacceptation habituelle prend la forme $t\in[c_1,c_2]$ pour $t$ une r√©alisation de la statistique.

L‚Äôusage veut que l‚Äôon d√©termine les valeurs critiques $c_1$ et $c_2$ en r√©partissant $Œ±/2$ sur chaque extr√©mit√©. Ainsi, pour le cas $H_0 : Œ∏ = Œ∏_0$, ces valeurs seront telles que $\mathbb P (T <c_1|H_0) = \mathbb P(T >c_2|H_0) = Œ±/2$.

Par contre, cette r√®gle ne conduit pas au test UPP-sans biais si la loi de $T$ n‚Äôest pas sym√©trique (le test peut m√™me ne plus √™tre sans biais). Dans la classe exponentielle, la formulation plus g√©n√©rale est que la fonction de r√©partition $F$ doit √™tre telle que la d√©riv√©e par rapport a $Œ∏$ de $\mathbb P(T <c_1) + \mathbb P(T >c_2)$ s‚Äôannule en $Œ∏_0$.

## üîñ Extra : rapport de vraisemblance g√©n√©ralis√©

Si le RV (rapport de vraisemblance simple) √©tait un quotient, le RVG (rapport de vraisemblance g√©n√©ralis√©) le sera aussi. Particuli√®rement, le RVG est une fonction $\lambda$ telle que

$$
\lambda(\mathbf{x})=\frac{\sup_{\theta\in\Theta_0}L(\theta|\mathbf{x})}{\sup_{\theta\in\Theta}L(\theta|\mathbf{x})}
$$

Ainsi, on d√©finit le test du RVG par une r√©gion de rejet de la forme

$$
\lambda(\mathbf{x})<k\le1
$$

Il est √©vident que $\lambda(\mathbf{x})$ est inf√©rieur ou √©gal √† $1$ pour toute r√©alisation $\mathbf{x}$. Notons que le d√©nominateur de $\lambda(\mathbf{x})$ est juste l‚Äôestimation de maximum de vraisemblance ! Le RVG rel√®ve de la m√™me rationalit√© que le RV simple. Si, pour une r√©alisation donn√©e, la vraisemblance atteint un maximum dans $H_0$ qui reste bien inf√©rieur a son maximum absolu (dessous d‚Äôun $k$ arbitraire) dans tout l‚Äôespace param√©trique $Œò$, alors il y a lieu de douter de cette hypoth√®se.

Il faudrait noter que, dans le cas d‚Äôhypoth√®se et alternative simples, le test du RVG est √©quivalent au test du RV simple. Explication dans le mat√©riel de Michel Lejeune, p. 236. 

Le $k$ choisit ici nous donnera notre erreur de premi√®re esp√®ce $\alpha$. On choisit $k$ de telle mani√®re que l‚Äô√©quation suivante est vraie, donc

$$
\sup_{\theta\in\Theta_0} \mathbb P(\lambda(\mathbf{x})<k)=\alpha
$$

Par contre, conna√Ætre la loi du RVG est probl√©matique. Parfois on aura une forme simple, mais le plus normale sera de disposer une approximation asymptotique tr√®s utile.

Le test du RVG n‚Äôa pas de propri√©t√©s d‚Äôoptimalit√© notables mais on constate dans des situations usuelles qu‚Äôil donne le test UPP-sans biais. Cependant, il poss√®de des propri√©t√©s asymptotiques int√©ressantes, notamment sa convergence moyennant des conditions de r√©gularit√© analogues a celles de l‚Äôestimateur du maximum de vraisemblance.

### Param√®tre de nuisance

Dans le cas o√π un param√®tre d‚Äôint√©r√™t suit une loi m√®re √† plusieurs param√®tres, et une hypoth√®se nulle fait r√©f√©rence √† un seul des param√®tres, on dit ‚Äúparam√®tres de nuisances‚Äù √† tout autre param√®tre non concern√©e par $H_0$. 

Par exemple : si on fait une hypoth√®se nulle sur la moyenne d‚Äôun population gaussienne et on ne dit rien sur la variance, la variance est le param√®tre de nuisance.

Une fois on a √©tabli une r√©gion de rejet associ√© √† un test, si l‚Äôhypoth√®se nulle est de la forme $H_0:\theta_1\le\theta\le\theta_2$ et $H_0$ ignore un deuxi√®me param√®tre de la loi m√®re $\rho$, il existe un test UPP-sans biais pour une famille de la classe exponentielle si sa densit√© peut s‚Äô√©crire

$$
f(x|\theta,\rho)=h(x)g(\theta,\rho)e^{\eta_1(\theta)T_1(x)+\eta_2(\rho)T_2(x)}
$$

On notera que ceci n‚Äôest pas v√©rifi√© par la loi de Gauss qui ne s√©pare pas ainsi $Œº$ et $œÉ^2$ dans la partie exponentielle. De fait, il n‚Äôexiste pas de test UPP-sans biais pour $H_0 : Œº_1 ‚â§ Œº ‚â§ Œº_2$ consid√©r√©e ci-dessus. Ces r√©sultats se g√©n√©ralisent a plusieurs param√®tres de nuisance.

### Approchant une r√©gion de rejet de grands √©chantillons

Soit une famille param√®tre $\{f(x|\theta) : \theta\in\Theta, \Theta\subseteq \mathbb{R}^k\}$ et $H_0$ concernant $r$ valeurs composantes de $\theta$, donc $1\le r\le k$. Supposons que les conditions sont telles que l‚ÄôEMV $\theta^*$ est BAN. Donc, la statistique $\Lambda_n=\lambda(\mathbf{X})$ est telle que

$$
\lim_{n\rightarrow\infty}-2\ln(\Lambda_n)\sim\chi^2(r)
$$

Donc, comme la r√©gion de rejet $\lambda < k$ est √©quivalente √† $-2\ln\Lambda >k^\prime$, on rejettera √† un niveau approximatif $\alpha$ si

$$
-2\ln\Lambda>q_{\chi^2(r)}^{1-\alpha}
$$

Ce r√©sultat dont la validit√© s‚Äô√©tend au-del√† de l‚Äô√©chantillonnage al√©atoire simple autorise un test approch√© dans des situations complexes. C‚Äôest pourquoi on trouve le test du RVG de fa√ßon omnipr√©sente dans les logiciels.

# Test param√©triques usuels

> [!note]
> Le tests ci-dessous peuvent se diviser en deux cat√©gories : test o√π on compare un param√®tre contre une valeur num√©rique de r√©f√©rence, comme ‚Äú$\mu=2$‚Äù ; et des tests o√π on compare les param√®tres de deux √©chantillons, comme ‚Äú$p_1 = p_2$‚Äù.

## Loi normale

### Test sur $\mu$, $\sigma^2$ connu $(z$-test$)$

Le test, en version bilat√©rale, est comme suit

$$
\tau=\left\{T=\frac{\bar X-\mu_0}{\sigma/\sqrt n} , A =\{T: -z_{1-\alpha/2}<T< z_{1-\alpha/2} \} \right\}

$$$$

H_0:\mu=\mu_0\text{ vs. }H_1:\mu\ne\mu_0
$$

On utilise telle forme de T car on sait qu‚Äôelle suit une loi normale standard. Mais, r√©arrangeant l‚Äôin√©galit√© qui d√©finit $A$, on peut r√©√©crire le test d‚Äôune mani√®re plus naturelle : 

$$
\tau=\left\{ T=\bar X, A=\{ T : \mu_0-\frac{\sigma}{\sqrt n} z_{1-\alpha/2} < T < \mu_0+\frac{\sigma}{\sqrt n} z_{1-\alpha/2} \} \right\}
$$

Les quantiles $z$ sont √† d√©terminer avec une table √† quantiles. Notons que l‚Äôerreur de premi√®re esp√®ce $\alpha$ est r√©pandu moiti√©-moiti√© des deux c√¥t√©s de la courbe. Ce test est un UPP-sans biais.

![untitled](ressources/04_tests_d‚Äôhypotheses_parametriques_untitled_1.png)

Dans le cas unilat√©rale, qui sont aussi UPP mais pas sans biais, on a que :

- $H_0 : \mu \le \mu_0\text{ vs. }H_1 : \mu > \mu_0$
On suppose que $\mu = \mu_0$ (qui maximise risque de premi√®re esp√®ce), puis la r√©gion de rejet est juste du c√¥te droite de la gaussienne standard. Ceci se refl√®te dans la r√©gion d‚Äôacceptation et, surtout, la r√©gion de rejet (non-acceptation) :  $$
    A=\{ T : T <z_{1-\alpha}\} \iff \bar A =\{ T : z_{1-\alpha } < T\}
    $$
- $H_0 : \mu \ge \mu_0\text{ vs. }H_1 : \mu < \mu_0$
Analogiquement au cas pr√©c√©dent. La r√©gion de rejet de $H_0$ est la queue gauche de la gaussienne normale standard.   $$
    A=\{ T : z_{1-\alpha}<T\} \iff \bar A =\{ T :T<z_{1-\alpha}\}
    $$
    ![Untitled](ressources/04_tests_d‚Äôhypotheses_parametriques_untitled_2.png)

### Test sur $\mu$, $\sigma^2$ inconnu $(t$-test$)$

Dans ce cas et vu pr√©c√©demment, on utilise une variable de Student comme $T$. Ceci est juste valide si et seulement si on est sous $H_0$, car $\mu_0$ serait donc la moyenne de $\bar X$. Ceci arr√™t d‚Äô√™tre le cas quand on consid√®re le cas g√©n√©ral (c√†d. peu importe si $H_0$ ou si $H_1$), car on ne peut donc garantir que $\mu_0$ est la vraie moyenne de $H_0$ et tout la statistique $T$ ne suit plus une loi de Student.  

$$
\tau=\left\{T=\frac{\bar X-\mu_0}{S/\sqrt n} , A =\{T: -t^{(n-1)}_{1-\alpha/2}<T< t^{(n-1)}_{1-\alpha/2} \} \right\}

\\[8pt]

H_0:\mu=\mu_0\text{ vs. }H_1:\mu\ne\mu_0
$$

De m√™me, on pourrait isoler $\bar X$ dans l‚Äôin√©galit√© pour obtenir ce qui suit :

$$
\tau=\left\{ T=\bar X, A=\{ T : \mu_0-\frac{S}{\sqrt n} t^{(n-1)}_{1-\alpha/2} < T < \mu_0+\frac{S}{\sqrt n} t^{(n-1)}_{1-\alpha/2} \} \right\}
$$

Dans le cas des tests unilat√©raux, on d√©finit $A$ comme suit :

$$
H_0 : \mu \le \mu_0 \longrightarrow A=\{ T : T <t^{(n-1)}_{1-\alpha }\} \iff \bar A =\{ T : t^{(n-1)}_{1-\alpha } \le T\}

$$$$

H_0 : \mu \ge \mu_0 \longrightarrow A=\{ T : t^{(n-1)}_{1-\alpha }<T\} \iff \bar A =\{ T :T \le t^{(n-1)}_{1-\alpha }\}
$$

### Test sur $\sigma^2$, $\mu$ inconnu $($test de $\chi^2$$)$

Pour tous le cas, on utilisera la statistique $T$ qui suit :

$$
T=\frac{(n-1)S^2}{\sigma^2_0}\sim\chi^2(n-1)
$$

Notre test devient donc, pour le cas bilat√©ral :

$$
\tau=\left\{T=\frac{(n-1)S^2}{\sigma^2_0}, A =\{T: -{\chi^2}^{(n-1)}_{1-\alpha/2}<T< {\chi^2}^{(n-1)}_{1-\alpha/2} \} \right\}

$$$$

H_0:\sigma^2=\sigma^2_0\text{ vs. }H_1:\sigma^2\ne\sigma^2_0
$$

Et le test est aussi r√©√©crit comme

$$
\tau=\left\{T=S^2, A =\{T: -\left(\frac{\sigma^2_0}{n-1}\right){\chi^2}^{(n-1)}_{1-\alpha/2}<T<\left(\frac{\sigma^2_0}{n-1}\right){\chi^2}^{(n-1)}_{1-\alpha/2} \} \right\}

$$

Particuli√®rement, pour le cas bilat√©ral, une note importante c‚Äôest que la loi $\chi^2$ n‚Äôest pas une loi sym√©trique. Dans le cas normale, on se servait de telle propri√©t√© pour fixer des quantiles correspondant √† $\alpha/2$ des deux c√¥t√©s.

Dans ce cas, on pourrait faire √ßa mais cela nous laisse avec un test qui n‚Äôest pas UPP-sans biais. Un test UPP-sans biais est possible, mais on doit faire un choix de quantiles $\alpha_1$ et $\alpha_2$ tels que $\alpha_1+\alpha_2=\alpha$ qui est compliqu√©e et donc √©vit√©e ici.

Pour le cas unilat√©ral, on a un test UPP-sans biais comme suit :

$$
H_0 : \sigma \le \sigma_0 \longrightarrow A=\{ T : T <{\chi^2}^{(n-1)}_{1-\alpha }\} \iff \bar A =\{ T : {\chi^2}^{(n-1)}_{1-\alpha } \le T\}$$$$H_0 : \sigma \ge \sigma_0 \longrightarrow A=\{ T : {\chi^2}^{(n-1)}_{1-\alpha }<T\} \iff \bar A =\{ T :T \le {\chi^2}^{(n-1)}_{1-\alpha }\}
$$

√Ä titre de curiosit√©, si la moyenne $\mu$ √©tait connue, on utiliserait le fait suivant, les d√©veloppement √©tant analogue aux pr√©c√©dents.

$$
\frac{\sum_{i=1}^n(X_i-\mu)^2}{\sigma^2}\sim\chi^2(n)
$$

### Comparaison de $\mu_1$ et $\mu_2$  (et cas √©chantillons appari√©s)

On suppose que on a deux √©chantillons tir√©s de deux gaussiennes diff√©rentes, et on veut comparer leurs moyennes. Normalement, on pose

$$
H_0:\mu_1-\mu_2=0\text{ vs. } H_1:\mu_1-\mu_2<0
$$

En fait, c‚Äôest de ce fait qu‚Äôon parle d‚Äôune ‚Äúhypoth√®se nulle‚Äù pour faire r√©f√©rence √† que la diff√©rence entre les deux moyennes est nulle.

### Supposition de m√™me variance, inconnue $($$t$-test de variance group√©e$)$

Notre statistique $T$ sera comme suit

$$
T=\frac{(\bar X_{1,n_1}-\bar X_{2,n_2})-(\mu_1-\mu_2)}{S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}}\sim \mathcal T(n_1+n_2-2)
$$

o√π $S_p$ est l‚Äô√©cart-type group√© de $\sigma^2_1$ et $\sigma^2_2$, c√†d. les variances moyenn√©es de leurs d√©gr√©es de libert√©. √Ä savoir, la variance group√© de $\sigma^2_1$ et $\sigma^2_2$ est

$$
S^2_p=\frac{(n_1-1)S^2_1+(n_2-1)S^2_2}{n_1+n_2-2}
$$

Cette statistique (la variance group√©e) est sans biais de la variance suppos√©e commune. Ce n‚Äôest pas le cas si on prend la racine carr√©e pour essayer d‚Äôestimer l‚Äô√©cart-type.

Finalement, on donne la version bilat√©rale du test :

$$
\tau=\left\{T\text{ comme ci-dessus}, A =\{T: -t^{(n_1+n_2-2)}_{1-\alpha/2}<T< t^{(n_1+n_2-2)}_{1-\alpha/2} \} \right\}
$$

Puis, on donne la version unilat√©rale du test en termes de la r√©gion d‚Äôacceptation $A$ :

$$
H_0 : \mu_1 \le \mu_2 \longrightarrow A=\{ T : T <t^{(n_1+n_2-2)}_{1-\alpha }\} \iff \bar A =\{ T : t^{(n_1+n_2-2)}_{1-\alpha } \le T\}
$$$$
H_0 : \mu_1 \ge \mu_2 \longrightarrow A=\{ T : t^{(n_1+n_2-2)}_{1-\alpha }<T\} \iff \bar A =\{ T :T \le t^{(n_1+n_2-2)}_{1-\alpha }\}
$$

Notamment, avec une telle r√©gion de rejet, le risque $Œ±$ maximal est atteint pour $Œº_1 = Œº_2$ et le test propos√© est donc bien de niveau $Œ±$. Ces sont des tests UPP-sans biais.

### Supposition de variances diff√©rentes, inconnues $($$z$-test asymptotique$)$

On devra se contenter d‚Äôune propri√©t√© asymptotique qu‚Äôon utilisera si $n_1,n_2 > 100$. On fera des l√©g√®res modifications √† la statistique $T$ de sorte **qu‚Äôon ne groupe pas** les variances. On renomme $T$ √† $Z$, car on fera un $z$-test asymptotique.

$$
T=\frac{(\bar X_{1,n_1}-\bar X_{2,n_2})-(\mu_1-\mu_2)}{S_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}} \to Z=\frac{(\bar X_{1,n_1}-\bar X_{2,n_2})-(\mu_1-\mu_2)}{\sqrt{\frac{S^2_1}{n_1}+\frac{S^2_2}{n_2}}}
$$

Alors, on a que 

$$
\lim_{n_1,n_2\rightarrow\infty} Z_{n_1,n_2} \sim \mathcal N(0,1)
$$

On se servira de ce fait seulement pour un test unilat√©ral, de la forme suivante.

$$
H_0:\mu_1-\mu_2\le\Delta_0, 

\hspace{8pt}

H_0:\mu_1 - \mu_2 =\Delta_0

\hspace{4pt}
\text{ou}
\hspace{4pt}

H_0:\mu_1-\mu_2\ge\Delta_0
$$

Il suffit pour cela de retrancher $\Delta_0$ de $(\bar X_1 - \bar X_2)$. Le cas bilat√©ral pose de difficult√©s majeures. La puissance de ce test est pauvre quand les $n_1$ et $n_2$ sont petits. Ce probl√®me s‚Äôappelle le probl√®me de Behrens-Fisher.

### Supposition de variances diff√©rentes, connues $($$z$-test proprement$)$

Il suffit de modifier l√©g√®rement la statistique $Z$ pr√©c√©dente pour remplacer les estimateurs $S^2_1$ et $S^2_2$ par les vraies $\sigma^2_1$ et $\sigma^2_2$.

$$
Z=\frac{(\bar X_{1,n_1}-\bar X_{2,n_2})-(\mu_1-\mu_2)}{\sqrt{\frac{\sigma^2_1}{n_1}+\frac{\sigma^2_2}{n_2}}}
$$

### Cas d‚Äô√©chantillons appari√©s

Deux √©chantillons sont appari√©es si on observe un √©chantillon des caract√©ristiques d‚Äôindividus libell√©es, puis on applique une ‚Äútransformation‚Äù et on re-observe les caract√©ristiques des m√™mes individus. On voit un ‚Äúavant‚Äù et un ‚Äúapr√®s‚Äù des individus.

Par exemple, un premier √©chantillon pourrait √™tre le prix des plusieurs fromage un jour, puis le deuxi√®me √©chantillon serait le prix des m√™mes fromages un autre jour. Chaque fromage retrouve son ‚Äúpair‚Äù dans l‚Äôautre √©chantillon.

Dans ce cas, notre statistique est une $T$ qui suit un loi de Student √† $n-1$ degr√©s de libert√©s. La diff√©rence ici avec le cas de comparaison de moyennes avec des variances inconnues suppos√©es √©gales sont les degr√©s de libert√©.

### Comparaison de $\sigma^2_1$ et $\sigma^2_2$ $($$F$-test ou test d‚ÄôANOVA$)$

On s‚Äôint√©resse ici seulement dans l‚Äô√©galit√© des variances, donc

$$
H_0:\frac{\sigma^2_1}{\sigma^2_2}=1\text{ vs. } H_1:\frac{\sigma^2_1}{\sigma^2_2}\ne1
$$

On se servira de la statistique $T$ suivante :

$$
T=\frac{S^2_1/\sigma^2_1}{S^2_2/\sigma^2_2}\sim\mathcal F(n_1-1,n_2-1)
$$

Sous $H_0$, $T$ suit une loi de Fisher. Puisque on s‚Äôint√©resse √† l‚Äô√©galit√© des variance, on mentionne seulement le test bilat√©ral dont la r√©gion d‚Äôacceptation $A$ est donc comme suit. **Fais attention aux souscripts des quantiles ! La diff√©rence des quantiles se trouve l√† !**

$$
\mathcal C =\{T: T\ge f^{(n_1-1,n_2-1)}_{\alpha/2} \text{ ou } f^{(n_1-1,n_2-1)}_{1-\alpha/2} \le T \}
$$

## Loi de Bernoulli (proportions)

### Test sur une proportion $p$

Pour le cas bilat√©ral, le test prend la forme suivante, o√π $\hat p_n = S_n/n$, $S_n$ √©tant la somme des succ√®s dans $n$ observations :

$$
\tau =
\left\{T=
\frac{\hat p_n-p_0}{\sqrt{\frac{p_0(1-p_0)}{n}}},
\space
\mathcal C=\left\{T: T>-z_{\alpha/2} \text{ ou } z_{\alpha/2}<T  \right\}

\right\}

$$$$

H_0 : p=p_0\text{ vs. } H_1:p\ne p_0
$$

On ajuste $\mathcal C$ dans le cas unilat√©ral.

La statistique $T$ est d√©riv√©e bas√©e dans le cas du th√©or√®me de De Moivre-Laplace, sauf que dans ce cas-l√† on s‚Äôint√©resse √† estimer la nombre de succ√®s $S_n$ tant qu‚Äôici on se fixe plut√¥t √† la proportion de succ√®s $P_n$.

Un intervalle de confiance bilat√©ral serait donc:

$$
IC_{95\%}=
\left[ 
\hat p- 1.96 \sqrt{\frac{\hat p(1-\hat p)}{n}},
\hat p+ 1.96 \sqrt{\frac{\hat p(1-\hat p)}{n}}
\right]
$$

### Comparaison de $p_1$ et $p_2$, asymptotique $($$z$-test$)$

On a deux populations qui suivent une loi de Bernoulli de param√®tres $p_1$ et $p_2$ respectivement et on veux tester que $H_0 : p_1=p_2$. Supposons qu‚Äôon r√©alise un √©chantillon de chaque population, **ind√©pendants entre eux**, tels que : 

- Ils sont de taille $n_1$ et $n_2$ respectivement,
- Les proportions de succ√®s observ√©es seront $\hat p_1$ et $\hat p_2$ respectivement,
    - On v√©rifie les conditions de validit√© d‚Äôune gaussienne.
        - $n_1\hat p_1(1-\hat p_1) > 12$
        - $n_2\hat p_2(1-\hat p_2) > 12$

Donc, on a que

$$
\hat p_1-\hat p_2 \sim \mathcal N\left(p_1-p_2, \frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}\right)
$$

Et, sous $H_0$, on a que

$$
\hat p_1-\hat p_2 \sim \mathcal N\left(0,\space p(1-p)\left(\frac{1}{n_1} + \frac{1}{n_2} \right)\right)
$$

o√π $p=p_1=p_2$. Par contre, $p$ est inconnu, donc on fait une moyenne pond√©r√©e pour l‚Äôestimer :

$$
\hat p = \frac{s_1+s_2}{n_1+n_2}=\frac{n_1\hat p_1 + n_2\hat p_2}{n_1+n_2}
$$

o√π $s_1$ et $s_2$ sont les succ√®s observ√©s dans les √©chantillons #1 et #2, respectivement.

Le test bilat√©ral prend la forme qui suit, o√π $T$ est la normalisation de la statistique $(\hat p_1 - \hat p_2)$ :

$$
\tau =\left\{ T=\frac{\hat p_1 - \hat p_2}{\sqrt{\hat p(1-\hat p)\left(\frac{1}{n_1} + \frac{1}{n_2} \right)}}, A= \{ T:-z_{1-\alpha/2}<T< z_{1-\alpha/2} \} \right\}
$$

### R√©sultat exact (loi hyperg√©ometrique)

Pour une situation o√π les √©chantillons ne sont pas suffisamment grands pour faire un approximation gaussienne, on utilise un test sur une loi hyperg√©om√©trique. Les d√©tails sont sp√©cifi√©s dans le mat√©riel de Michel Lejeune, page 237.

Pour des √©chantillons appari√©s (mesures r√©p√©t√©es), on a besoin du test de McNemar.