# inform√°tica 3 // aprendizaje por refuerzo

Date de cr√©ation: June 23, 2022 11:58 PM
Modifi√©: March 10, 2024 1:29 AM

<aside>
üí° Tabla de contenidos

</aside>

# 1. Introducci√≥n

---

Los rasgos del AR son un **agente** que toma **decisiones** e **interact√∫a** con un **ambiente** (donde se presentan los estados y las recompensas). El agente busca lograr un **objetivo expl√≠cito** a pesar de la **incertidumbre** de los efectos de sus acciones sobre el ambiente.

Una decisi√≥n tomada por el agente puede verse como una funci√≥n que corresponde un estado del ambiente a una acci√≥n. Esta funci√≥n es llamada la pol√≠tica y se denota $\pi$. Puede ser determin√≠stica o estoc√°stica. En esencia, la pol√≠tica es un sistema de decisiones.

- **Pol√≠tica determin√≠stica**: $\pi: S \rightarrow A$
- **Pol√≠tica estoc√°stica**: $\pi:S \times A \rightarrow [0,1]$
- Notemos que una pol√≠tica determin√≠stica es una pol√≠tica estoc√°stica donde la acci√≥n tomada tiene probabilidad 1 y el resto de acciones tiene probabilidad 0.

Luego, existe la funci√≥n de **se√±al de recompensa $R_t$**, la cual le da puntos por cada estado alcanzado en tiempo t, negativos o positivos. Esta se√±al de recompensa busca guiar al agente a su objetivo expl√≠cito. El fin del AR es encontrar la pol√≠tica que maximice la esperanza de la recompensa acumulada al pasar por todos los estados futuros.

La transici√≥n entre estados sigue la **propiedad de Markov (p√©rdida de memoria)**: toda la informaci√≥n que determina el pr√≥ximo estado est√° capturada √∫nicamente en el estado y acci√≥n presente. Esta propiedad es conveniente para demostrar la convergencia de ciertos algoritmos para encontrar la pol√≠tica √≥ptima.

# 2. Bases

---

- **Funci√≥n retorno**: es la funci√≥n de las recompensas futuras a partir de un tiempo t hasta un tiempo m√°ximo T. $\gamma$ es una tasa de descuento. Si $\gamma < 1$, las recompensas del futuro lejano valen menos que las recompensas de futuro pr√≥ximo. Es lo contrario si $\gamma > 1$.
La √∫ltima l√≠nea es una identidad recursiva √∫til.

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}... = \sum_{i=t}^T \gamma^{i-1} R_i

\newline

G_t = R_{t+1} + \gamma G_{t+1}
$$

- **Funci√≥n valor-estado**: es el valor esperado cuando el agente sigue la pol√≠tica $\pi$ desde estado $s$. Tambi√©n tiene una identidad recursiva √∫til, particularmente cuando la usamos para ir del √∫ltimo estado hacia atr√°s, hasta llegar a los estados m√°s lejanos.

$$
v_\pi(s) = \mathbb{E}_\pi[G_t | S=s_i]

\newline
\newline

v_\pi(s) = \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S= s_i]
$$

- En la pr√°ctica, los valores-estado est√°n atados a grandes sistemas de ecuaciones, por lo que es dif√≠cil conseguir el valor de un solo estado aislado. Normalmente entonces, usamos una estimaci√≥n del valor-estado.
- **Funci√≥n valor-acci√≥n o funci√≥n Q**: es el valor esperado de tomar una acci√≥n $a$, luego llegar al estado $s$ y seguir la pol√≠tica $\pi$ desde ah√≠. Es muy parecida a la funci√≥n valor-estado. La segunda l√≠nea es una representaci√≥n m√°s simple usando la funci√≥n valor-estado.

$$
q_\pi(s,a) = \mathbb{E}[G_t|S=s, A=a]

\newline

q_\pi(s,a) = R(s,a) + v_\pi(s')
$$

- En la pr√°ctica, usamos la funci√≥n valor-estado para evaluar la calidad de la pol√≠tica, mientras que utilizamos la funci√≥n valor-acci√≥n para mejorar la pol√≠tica.

## Teorema de mejora de pol√≠tica

> Antes de explicar el teorema, tenemos que explicar un poco la notaci√≥n.

Particularmente, notemos que $v_\pi(s) = q_\pi(s,\pi(s))$. Es decir, ‚Äúel valor esperado de seguir $\pi$ empezando desde $s$, es lo mismo que el valor esperado de empezar desde $s$, tomar la acci√≥n a que nos dicta la pol√≠tica $\pi$ en el estado $s$, y luego continuar la pol√≠tica $\pi$‚Äù.

La escritura de la izquierda es m√°s simple que la de la derecha, por lo que queremos usarla cuando podamos. 

Entonces, realmente queremos usar $q_\pi$ cuando estamos en un estado y vamos a tomar una decisi√≥n diferente de la decisi√≥n que nos dicta la pol√≠tica para ese estado $s$, luego continuamos con la pol√≠tica. Un ejemplo ser√≠a $q_\pi(s, \pi'(s))$ para una pol√≠tica $\pi‚Äô$ con una acci√≥n diferente en ese estado.
> 

Si existe una acci√≥n alternativa $a‚Äô$ tal que $q_\pi(s_i, a‚Äô) ‚â• v_\pi(s)$, entonces podemos crear una pol√≠tica alternativa $\pi$‚Äô tal que realice $a‚Äô$ en estado $s_i$ y $\pi(s)$ para el resto de estados $s‚Äô$.

Por ende, $v_{\pi‚Äô}(s) ‚â• v_\pi$, por lo tanto, $\pi‚Äô$ es una mejora sobre $\pi$.

## M√©todos de Monte Carlo

Son algoritmos para estimar cantidades num√©ricas a partir de una muestreos aleatorios. Es un modelo de algoritmo, no un determinado algoritmo en particular. Normalmente, el algoritmo sigue el siguiente patr√≥n:

1. Define un dominio de donde tomar valores aleatorios.
2. Toma la muestra aleatoria.
3. Pasa los valores aleatorios por una computaci√≥n determin√≠stica (una funci√≥n, por ejemplo)
4. Agrega los resultados.

Se aplica al AR porque no es necesario conocer las probabilidades de transici√≥n para poder estimar la mejor pol√≠tica, sea estimando la funci√≥n **valor-estado** o **acci√≥n-estado**. Particularmente, el m√©todo de Monte Carlo aplicado para estimar el valor de un estado bajo una cierta pol√≠tica es el siguiente:

1. Fijamos un estado de inicio $s$ y una pol√≠tica $\pi$ determin√≠stica o estoc√°stica.
2. Tomamos varias simulaciones siguiendo nuestra pol√≠tica, sin necesidad de saber las probabilidades de transici√≥n. 
3. Tomamos las recompensas acumulativas $R$ al llegar a un estado terminal.
4. Finalmente, despu√©s de todas las simulaciones, tomamos un promedio de las recompensas acumuladas de cada simulaci√≥n. Ese ser√° la estimaci√≥n de $v_\pi(s)$.

Sin embargo, hay un problema. Puede que volvamos a regresar al estado s despu√©s de haber comenzado la simulaci√≥n. Por ello, existen dos tipos de MC: primera visita y varias visitas:

- **MC de primera visita**: solo cuenta la recompensa acumulada desde la primera vez que se visita el estado de partida.
- **MC de varias visitas**: se toma la recompensa acumulada a contar por cada vez que se visit√≥ un estado, y finalmente se hace un promedio de ellas.
- MC de primera visita es un estimador no-sesgado. Si el # de simulaciones es peque√±o, el error cuadrado medio de MCVV ser√° m√°s peque√±o. Si el # de simulaciones es grande, el ECM de MCPV ser√° m√°s peque√±o. Ambas convergen al mismo valor cuando el # de simulaciones tiende al infinito.

Otro caso a considerar es cuando no tenemos un modelo de transici√≥n (no sabemos las conexiones entre estados). En ese caso, conviene m√°s calcular la funci√≥n Q de acci√≥n-estado. La estimaci√≥n de $q_\pi(s, a)$ se denota $Q(s,a)$.

- **Control de MC**: m√©todo para encontrar la pol√≠tica √≥ptima, alternando el paso de evaluaci√≥n y el paso de mejora.
    - **Paso de evaluaci√≥n (E)**: los valores de acci√≥n Q son evaluados para la pol√≠tica actual.
    Puedes hacer un mont√≥n de simulaciones y luego promediar sus Q emp√≠ricos.
    Alternativamente, puedes actualizar Q con cada nueva simulaci√≥n. La f√≥rmula ser√≠a esencialmente la primera, aunque la segunda es la que se utiliza en la pr√°ctica y se llama **Q-learning**. (Aqu√≠ $G_t$ cuenta tambi√©n la recompensa actual t con las futuras).
    
    $$
    Q^{nuevo}(s_t,a_t) = Q(s_t,a_t) + \alpha[G_t - Q(s_t,a_t)]
    $$
    
    $$
    Q^{nuevo}(s_t,a_t) = Q(s_t,a_t) + \alpha[r_t + \gamma \max_a Q(s_{t+1}, a) - Q(s_t,a_t)]
    $$
    
    - **Paso de mejora ($I$)**: se actualiza la pol√≠tica actual con la mejor acci√≥n encontrada tal que aumenta $Q(S_t, A_t)$.
    
    $$
    \pi(S_t) = \argmax_{A_t} Q(S_t, A_t)
    $$
    
    - Finalmente, el control de MC toma la siguiente forma:
        
        $$
        \pi_0 \xrightarrow{E}
        q_{\pi_0} \xrightarrow{I}
        \pi_1 \xrightarrow{E}
        q_{\pi_1} \xrightarrow{I}
        \pi_2 \xrightarrow{E}
        q_{\pi_2} \xrightarrow{I}
        ... \space
        \pi^* \xrightarrow{E}
        q_{\pi}^*
        $$
        

## Codicia √©psilon ($\epsilon$-greedy)

Con este **control de MC** explicado, decimos que el algoritmo se vuelve ‚Äúcodicioso‚Äù con respecto a una acci√≥n en estado $s$ cuando la implementa determin√≠sticamente en su pol√≠tica. El problema con esto es que puede ser que no se estime bien todas las posibles combinaciones de $Q(s,a)$ porque la pol√≠tica siempre va a preferir una cierta acci√≥n sobre las otras. Esto se evita con el algoritmo codicia √©psilon. Existen dos maneras: 

- **Aleatoriedad incluida la pol√≠tica (on-policy)**: cada vez que se va a tomar una acci√≥n, la mitad de las veces se sigue la pol√≠tica y la otra mitad se decide tomar una acci√≥n al azar (incluso incluida la opci√≥n determin√≠stica preferida por la pol√≠tica).
- **Aleatoriedad** **fuera de la pol√≠tica (off-policy)**: se entrenan dos pol√≠ticas, llamadas **pol√≠ticas de comportamiento** $b$ ****y **de objetivo $\pi'$**. La de comportamiento es √©psilon-codiciosa, mientras que la de objetivo eval√∫a y mejora la pol√≠tica original $\pi$.

Con este √∫ltimo dise√±o, la pol√≠tica de comportamiento explora acciones diferentes de la √≥ptima hasta al momento. As√≠, la pol√≠tica objetivo aprende de las malas decisiones mientras que guarda en memoria las buenas decisiones. Finalmente, esto resuelve bien el dilema de exploraci√≥n y explotaci√≥n. 

# 3. Extensiones

---

## Diferencia temporal

Los m√©todos de Monte Carlo utilizados aqu√≠ presentan problemas. Particularmente:

1. Las simulaciones o ‚Äúepisodios‚Äù necesitan ser ‚Äúcompletos‚Äù para poder calcular los retornos y actualizar los retornos, especialmente en los episodios que toman mucho tiempo en terminar o simplemente no terminan.
2. Debido a que la estimaci√≥n de Q es un promedio de variables aleatorias, su varianza es grande, y debemos hacer muchas simulaciones de MC para finalmente converger.

Sin embargo, podemos calcular los retornos sin necesidad de completar una simulaci√≥n. Un m√©todo que actualiza Q con cada paso de tiempo es denotado de ‚Äúdiferencia temporal‚Äù o DT.

Recordemos la f√≥rmula modelo de actualizaci√≥n de Q:
No olvidemos que, en este modelo, $G_t$ a√±ade la recompensa del tiempo actual t con las recompensas de tiempos futuros t+1, t+2‚Ä¶ etc.

$$
Q^{nuevo}(s_t,a_t) = Q(s_t,a_t) + \alpha[G_t - Q(s_t,a_t)]
$$

En esta f√≥rmula, $G_t$ es una funci√≥n objetivo. Es decir, la direcci√≥n del cambio va a depender de $G_t$, mientras que el tama√±o del cambio ser√° regulado por $\alpha$.

Con eso explicado, este modelo debe esperar a terminan el episodio. La f√≥rmula de DT m√°s simple es la siguiente.

$$
Q^{nuevo}(s_t,a_t) = Q(s_t,a_t) + \alpha[(R_t + V(S_{t+1})) - Q(s_t,a_t)]
$$

Notemos que la funci√≥n objetivo cambi√≥ de $G_t$ a $(R_t + V(S_{t+1})$. No necesitamos llegar al t final para esta actualizaci√≥n, puesto a que vamos a tomar una estimaci√≥n (valor esperado) de los retornos a partir del estado $S_{t+1}.$

Hay dos m√©todos de diferencia temporal principales: necesitamos los pares estado-acci√≥n de dos tiempos subsecuentes, t y t+1.

- **Sarsa**: el nombre viene de que necesita 5 elementos, que son $\{s_t, a_t, r_t, s_{t+1}, a_{t+1}\}$.

$$
Q^{nuevo}(s_t,a_t) = Q(s_t,a_t) + \alpha[R_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t)]
$$

- **Q-learning (Sarsa max)**: parecido a Sarsa, pero utiliza el m√°ximo posible de $Q(s_{t+1}, a)$.
    
    $$
    Q^{nuevo}(s_t,a_t) = Q(s_t,a_t) + \alpha[r_t + \gamma \max_{A^*} Q(s_{t+1}, A^*) - Q(s_t,a_t)]
    $$
    
- La distinci√≥n entre ambos algoritmos es que Sarsa es $\epsilon$-greedy sobre la pol√≠tica mientras que Q-learning es fuera de la pol√≠tica. Es decir, Sarsa utiliza una pol√≠tica $\epsilon$-greedy para $a_t$ y $a_{t+1}$, mientras que Q-learning utiliza una pol√≠tica $\epsilon$-greedy de comportamiento b para $a_t$ y una pol√≠tica greedy $\pi$ para $A^*$. Si fijamos $\epsilon = 0$, Sarsa y Q-learning son lo mismo.
- **Al aplicar estos m√©todos, se debe pasar o *loopear* por todos los estados**

Nota: la diferencia entre la **funci√≥n objetivo** y **el valor actual de Q** dentro de los corchetes es llamado el error de diferecia temporal o TD-Error.

### Sesgo de maximizaci√≥n y Q-learning doble

Un problema notable con DT, especialmente Q-learning es el sesgo de maximizaci√≥n. Es complicado de explicar, por lo que usaremos un ejemplo.

![Untitled](old/Otras%20libretas%2038b07a4041b949a7aba5ec3a03d2931f/Brilliant%20375bdad652564477b33bcb24abdc6919/inform√°tica%203%20aprendizaje%20por%20refuerzo%203aed2fb8875747a89dae5c276dafb3e0/Untitled.png)

Notemos que $Q(A, \rightarrow) =0$ y $Q(A, \leftarrow) = -0.1$.

Sin embargo, supongamos que por la varianza de las recompensas, nuestras primeras estimaciones de $Q(B, \leftarrow)$ nos llevaron a pensar que el valor esperado de ir a B era mejor que de ir a la derecha.

Ahora, dependemos de que la suerte se nos corrija y tengamos malos resultados del sampleo de las recompensas para corregir y bajar $Q(B, \leftarrow)$ para finalmente llegar al √≥ptimo te√≥rico $Q(A, \rightarrow)$. Y, sin embargo, hay situaciones donde no hay suerte en las recompensas y podemos quedarnos atrapados permanentemente en la decisi√≥n sub-√≥ptima.

La soluci√≥n a este problemas son distintos tipos de **Q-learning doble**. Usaremos el Q-learning original, en donde se crean dos funciones Q, Cuando actualicemos $Q_1$, utilizaremos el $\max Q$  indicado por $Q_2$, y viceversa (invirtiendo los √≠ndices en la expresi√≥n de abajo). Escogemos qui√©n ser√° la funci√≥n actualizada y la funci√≥n maximizadora al azar, con 50% de probabilidad.

$$
Q_1^{nuevo}(s_t,a_t)= Q_1(s_t,a_t) + \alpha[r_t + \max_a Q_2(s_{t+1},a_{t+1}) - Q_1(s_t,a_t)]
$$

Esta soluci√≥n ayuda porque es muy dif√≠cil que ambas Q sobre-estimen la misma acci√≥n simult√°neamente. E incluso si es el caso, es m√°s f√°cil que ambas se desatoren.

## M√©todo de gradiente de pol√≠ticas

Con todo, los m√©todos de DT tienen dos problemas: requieren un espacio de acciones discreto y no aprendren las probabilidades de elecci√≥n que ser√≠an las √≥ptimos en una pol√≠tica $\epsilon$-greedy.

Adem√°s, los m√©todos de DT son **m√©todos basados en el valor**, los cuales mantienen una estimaci√≥n de Q para la pol√≠tica √≥ptima. Sin embargo, los **m√©todos basadas en pol√≠ticas** aprenden directamente sin una estimaci√≥n de valor.

> **Recordatorio**: el gradiente es la matriz columna tal que, desde un cierto punto input $p$, nos indica el vector output tal que muestra el mayor aumento de la funci√≥n.
> 
> 
> $$
> \nabla_\theta J(\theta) =
> \begin{bmatrix}
> \frac{\partial J}{\partial w_1}(p) \\
> \frac{\partial J}{\partial w_2}(p) \\
> \vdots \\
> \frac{\partial J}{\partial w_n}(p) \\
> \end{bmatrix},
> \space
> \theta =
> \begin{bmatrix}
> w_1 \\
> w_2 \\
> \vdots \\
> w_n
> \end{bmatrix}
> $$
> 
> La direcci√≥n del gradiente indica la direcci√≥n de mayor crecimiento de la funci√≥n $J(\theta)$, a partir de $p$, mientras que la magnitud del vector es la raz√≥n de cambio, que es la mayor derivada direccional absoluta.
> 

Sea $\theta$  un vector de par√°metros, entonces el m√©todo del gradiente ser√≠a:

$$
\theta \leftarrow \theta + \alpha  \nabla _\theta J(\theta)
$$

Este proceso hace para entrada en $\theta$ moverse en una direcci√≥n tal que aumente el valor esperado $J(\theta) = v_{\pi_\theta}(s_0)$. Realmente, porque fijamos $s_0$, $v$ termina siendo m√°s bien una funci√≥n de $\theta$: $v_{s_0}(\pi_\theta)$.

Para entrenar esto, a cada peso debemos pasarle todos los episodios de nuestro dataset, ver el promedio de los cambios que cada ejemplo del dataset sugiere, y hacer ese mismo proceso con todos los pesos.
Sin embargo, poder generar todos los episodios a partir de un cierto estado, y luego hacer lo mismo para todos los estados, es casi imposible y costoso. Al igual que las RNA, hacemos **ascenso por gradiente estoc√°stico**: solo tomamos una muestra de los episodios, actualizamos los par√°metros con ascenso por gradiente, y repetimos con una muestra distinta de episodios.