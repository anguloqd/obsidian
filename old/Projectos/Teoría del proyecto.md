# Teor√≠a del proyecto

<aside>
üí° Tabla de contenidos

</aside>

# 1. Modelos de algoritmos de AR

---

Para resolver un problema, un algoritmo de AR puede decidir usar tres m√©todos:

1. **M√©todos del cr√≠tico**: son m√©todos concentrados en la funci√≥n acci√≥n-valor.
Dada una pol√≠tica de base con resultados decentes, busca encontrar una mejor pol√≠tica basada en la expectativa de rendimientos de la pol√≠tica base. Hace esto buscando una pol√≠tica $\pi^*$ que maximice $v_{\pi^*}(s)$ para todo estado posible.
    - **Q-learning** busca la mejor pol√≠tica bas√°ndose en la funci√≥n Q de acci√≥n valor.
    - **Deep Q-Learning** utiliza una red neuronal que toma como input un estado y suelta como output las estimaciones de Q con las acciones posibles.
    
    Un problema de los m√©todos del cr√≠tico es que solo funcionan con conjuntos de acciones y estados que sean finitos y discretos. 
    
2. **M√©todos del actor**: son m√©todos en aprender la pol√≠tica √≥ptima en vez de ‚Äúcaer sobre ella‚Äù, como lo hace el m√©todo del cr√≠tico.
    - **B√∫squeda del gradiente de la pol√≠tica**: b√°sicamente, imaginamos una pol√≠tica como una funci√≥n de un vector de par√°metros $\theta$. Luego, hacemos ascenso por gradiente de una funci√≥n ‚Äúrendimiento global‚Äù o por cada funci√≥n estado-valor por cada estado $s$.
3. **M√©todos del cr√≠tico y actor**: son m√©todos mixtos.
*En esencia, es la versi√≥n de diferencia temporal del gradiente de la pol√≠tica.* 
Bas√≠camente, actualizamos simult√°neamente la estimaci√≥n de la funci√≥n Q (cr√≠tico) y la estimaci√≥n de los par√°metros $\theta^*$ de la pol√≠tica o la distribuci√≥n de probabilidad de las acciones √≥ptima $\pi^*$ (actor).
Existen varias formas de definir la actualizaci√≥n de los gradientes de $\theta$, aqu√≠ hay algunos:
    
    ![Untitled](Teori%CC%81a%20del%20proyecto%207771eff31ff94be8b368ad15086276fe/Untitled.png)
    
    *(En esta escritura, $\pi_\theta(s,a)$ es la probabilidad de tomar acci√≥n $a$ en estado $s$)*
    

# 2. Algoritmos de AR utilizados

---

## A2C: Advantage Actor-Critic

---

Introduce una **funci√≥n ‚Äúventaja‚Äù** para reducir la grand√≠sima varianza que tiene el m√©todo de gradiente de pol√≠tica original. Es una analog√≠a de la diferencia temporal. Intuitivamente, se interpreta como cu√°n mejor es, en estado $s$, tomar una acci√≥n espec√≠fica en en vez de la acci√≥n que la pol√≠tica en expectativa regresar√≠a. Al final, hacemos esto para reducir la variabilidad del gradiente de la pol√≠tica.

### Algoritmo

---

Notemos que $\theta$ es el vector par√°metros de la pol√≠tica $\pi_\theta$ y $w$ es el vector par√°metros de la funci√≥n $Q_w(s,a)$.

1. Empieza en un estado inicial s y toma una acci√≥n $a = \pi(a|s)$.
2. En el tiempo inicial t, samplea una recompensa $R(s,a)$ y un pr√≥ximo estado $s‚Äô \sim P(s‚Äô|s,a)$.
3. Luego samplea una pr√≥xima acci√≥n $a‚Äô$ a partir de tu pol√≠tica $\pi_\theta$.
4. Actualiza $\theta$ seg√∫n el m√©todo de actualizaci√≥n Advantage. 
    
    $$
    \theta \leftarrow \theta + \alpha_\theta(\mathbb{E}_{\pi_\theta}[\nabla_\theta \log_{\pi_\theta}(s,a) \space \cdot \space A_w(s,a)],
    \newline
    A_w(s,a) = Q_w(s, a) - V(s,a)
    $$
    
5. Actualiza $w$ con un m√©todo similar al de $\theta$:
    
    $$
    w \leftarrow w + \alpha_w(\delta_t \space \cdot \space \nabla_wQ_w(s,a)),
    \newline
    \delta_t = r_t + \gamma Q_w(s',a')-Q_w(s,a)
    $$
    
6. Asigna $s \leftarrow s‚Äô, \ a \leftarrow a‚Äô$. Recomienza hasta terminar.

## DDPG: Deep Deterministic Policy Gradient

---

El actor **asigna directamente una acci√≥n determin√≠stica** a un estado, en vez de asignar una distribuci√≥n de probabilidades de acciones (incluso si el espacio de acciones es continuo y no discreto). Tambi√©n usa dos redes ‚Äútarget‚Äù u objetivo, que son las redes para ayudar a las redes originales a ser optimizadas.

### Bases te√≥ricas

---

Se introducen 4 aspectos para la base del algoritmo:

1. **B√∫fer de reproducci√≥n**: durante cada episodio, se van guardando tuplas de experiencia de la forma $\{s_t, a_t, r_t, s_{t+1}\}$ en un cach√© de tama√±o finito. Luego, al actualizar los par√°metros de las redes Q y de pol√≠tica, tomamos una muestra de nuestro cach√© y promediamos los gradientes de cada tupla de experiencia.

2. **Actualizaci√≥n de las redes originales**: para la red Q original, es parecida al modelo de diferencia temporal. SIn embargo, el ‚Äúobjetivo‚Äù en nuestra diferencia temporal (la direcci√≥n hacia donde se mueve nuestros par√°metros) utiliza el Q de la red ‚Äútarget‚Äù Q.
    
    $$
    G_t = r_t + \gamma Q'(s_{t+1}, \space \mu(s_{t+1}|\theta^{\mu'})|\theta^{Q'})
    $$
    
    Luego, calculamos la funci√≥n p√©rdida como la diferencias cuadradas del Q-objetivo temporal con respecto al antiguo Q. L es finalmente lo que queremos optimizar.
    
    $$
    L = \frac{1}{N} \sum_t (G_t-Q(s_i,a_i|\theta^Q))^2
    
    \newline
    
    \theta^Q \leftarrow \theta^Q - \alpha \nabla_{\theta^Q} L
    $$
    
    Para la red pol√≠tica, queremos maximizar $J(\theta) = \mathbb{E}[Q(s,a)|s=s_t, a_t = \mu(s_t)]$.
    El gradiente de un solo ejemplo ser√≠a:
    
    $$
    \nabla_{\theta^\mu} J(\theta) \approx \nabla_aQ(s_t,\mu(s_t)) \space \cdot \space \nabla_{\theta^\mu}\mu(s_t|\theta^\mu)
    $$
    
    Entonces, si los pasamos por varios ejemplos (las tuplas de experiencia del b√∫fer de reproducci√≥n), el gradiente ser√≠a igual al promedio de los gradientes de cada tupla:
    
    $$
    \nabla_{\theta^\mu} J(\theta) \approx \frac{1}{N} \sum_t
    \nabla_aQ(s_t,\mu(s_t)) \space \cdot \space \nabla_{\theta^\mu}\mu(s_t|\theta^\mu)
    
    \newline
    
    \theta^\mu \leftarrow \theta^\mu + \alpha\nabla_{\theta^\mu} J
    
    \newline
    
    \phantom{0}
    $$
    
3. **Actualizaci√≥n de las redes ‚Äútarget‚Äù**: hacemos una copia de los par√°metros de las redes target y que sigan por detr√°s lentamente a las redes originales.
    
    $$
    \theta^{Q'} \leftarrow \tau\theta^Q + (1-\tau)\theta^{Q'}
    
    \newline
    
    \theta^{\mu'} \leftarrow \tau\theta^\mu + (1-\tau)\theta^{\mu'}
    
    \newline
    
    \phantom{0.75}
    $$
    
4. **Exploraci√≥n**: recordemos que el espacio de acciones es considerado continuo en este algoritmo. Por lo tanto, inyectaremos ruido a las acciones de la red target de acci√≥n.

$$
\mu'(s_t) = \mu(s_t|\theta_t^\mu) + \mathcal{N}
$$

### Algoritmo

---

Inicia los par√°metros de las redes originales $\theta^Q$, $\theta^\mu$ , las redes target $\theta^{Q‚Äô}$, $\theta^{\mu‚Äô}$, el b√∫fer de reproducci√≥n R. Por los momentos, los pesos de una red target son los mismos de su an√°logo original.

1. Empieza en un estado inicial $s_1$.
2. Selecciona una acci√≥n $a_1$ = $\mu(s_t|\theta^\mu) + \mathcal{N}_t$.
3. Guarda la transici√≥n $\{s_t, a_t, r_t, s_{t+1} \}$ en el b√∫fer $R$.
4. Calcula la p√©rdida de la red cr√≠tica usando el error temporal con Qs cruzadas. Minimiza esta funci√≥n y actualiza $\theta^Q$.
5. Samplea una muestra de $R$ y actualiza los par√°metros $\theta^\mu$ con ascenso por gradiente de la funci√≥n $J(\theta)$.
6. Actualiza los par√°metros de las redes target $\theta^{Q‚Äô}$ y $\theta^{\mu‚Äô}$.
7. Recomienza hasta llegar al tiempo episodio terminal.

## PPO: Proximal Policy Optimization

---

Es un algoritmo para asegurar que las actualizaciones hechas a la pol√≠tica no sean tan vol√°tiles. Est√° basado en la simplificaci√≥n de otro algoritmo llamado Trust Region Policy Optimization (TRPO). Introduce un **t√©rmino de ‚Äúclip‚Äù** tal que las actualizaciones no sean m√°s extremas que ese t√©rmino. 

### Bases te√≥ricas

---

### Problemas con el gradiente de pol√≠tica cl√°sico y TRPO

---

El problema con la optimizaci√≥n de pol√≠ticas cl√°sica es que **actualiza los par√°metros muy lejos de su valor original**, por lo que toma mucha tiempo, quiz√°s nunca, en converger a los valores te√≥ricamente √≥ptimos. Tales actualizaciones de pol√≠tica son llamadas ‚Äúactualizaciones de pol√≠tica destructivamente grandes‚Äù.

Particularmente, veamos la funci√≥n $J(\theta)$ usando el t√©rmino de ‚Äúventaja‚Äù. La varianza aqu√≠ viene de la estimaci√≥n de valor contenida en el t√©rmino de ventaja.

$$
J(\theta) = \mathbb{E}_t[\nabla_\theta \log(\pi_\theta(a_t|s_t)) \space \cdot \space √Çt]

\newline

\dots
$$

Un intento para corregir este problema fue la introducci√≥n de Trust Region Policy Optimization (TRPO). Se introduce una funci√≥n r(\theta), que se define as√≠:

$$
r(\theta)=\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{viejo}}(a_t|s_t)}
$$

La l√≥gica de este t√©rmino es medir la distancia la probabilidad de una acci√≥n bajo esta nueva distribuci√≥n de probabilidades dependiente de $\theta$, comparada con los anteriores par√°metros. Entre m√°s cerca est√© $r(\theta)$ de 1, menos es la distancia de las probabilidades bajo ambos par√°metros. 

Finalmente, queremos proponer una nueva funci√≥n a maximizar, que es la siguiente. La funci√≥n KL es la divergencia de Kullback-Leiber. $\delta$ es arbitrariamente peque√±o.

$$
\argmax_\theta \mathbb{E}_t[\nabla_\theta\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{viejo}}(a_t|s_t)} \space \cdot \space √Çt],

\newline
\phantom{0}
\newline

\text{sujeto a: } \mathbb{E}_t[\text{KL}[\pi_{\theta_{viejo}}(\cdot|s_t), \pi_{\theta_{viejo}}(\cdot|s_t)]] \le \delta.
$$

El √∫nico problema con esto es que puede a√±adir sobrecarga al proceso de optimizaci√≥n, lo que a veces lleva a un comportamiento de entrenamiento indeseable.

Para el algoritmo PPO, se introduce un nuevo t√©rmino: **objetivo sustituto recortado** (clipped surrogate objective).

$$
J^{CLIP}(\theta) = \mathbb{E}_t[\min(r_t(\theta)\cdot√Ç_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \cdot √Ç_t)]
$$

![Untitled](old/Projects%2091a38a7522204a1eaaeb87fd0f122308/Teor√≠a%20del%20proyecto%207771eff31ff94be8b368ad15086276fe/Untitled%201.png)

Notemos que, si la ventaja para tal acci√≥n es positiva, no queremos alejarnos mucho de los que estamos haciendo actualmente. Sin embargo, si la ventaja para tal acci√≥n es negativa, no queremos quedarnos estancados all√≠, por lo que permitiremos pasos m√°s grandes y as√≠ deshacemos errores en la pol√≠tica.

### Algoritmo

---

1. Inicia un episodio, un estado inicial $s_1$ y $N$ actores paralelos.
2. **Para cada actor**: ejecuta la pol√≠tica $\pi_{viejo}$ por $T$ pasos de tiempo.
3. Computa las estimaciones de ventaja $A_1, ‚Ä¶, A_T$.
4. Optimiza $J_t^{CLIP}$, con muestras de tama√±o $M \le NT$ y con $K$ √©pocas.
**√âpoca**: cantidad de veces que pasas una muestra para actualizar el modelo.
5. Recomienza con otro episodio. As√≠ hasta con el dataset de episodios.

# 3. Implementaci√≥n

---

## Evaluaci√≥n de rendimientos

---

Utilizamos la **raz√≥n de Sharpe** para evaluar nuestras operaciones. En esencia, la raz√≥n de Sharpe compara los rendimientos de una operaci√≥n con respecto al riesgo de esa operaci√≥n. 

$$
S = \frac{R_p - R_f}{\sigma_p}
$$

- $R_p$: rendimiento del portafolio/stock/mercado, etc.
- $R_f$: rendimiento sin riesgo (bonos de la tesorer√≠a).
- $\sigma_p$: volatilidad del portafolio (desviaci√≥n est√°ndar).

De otro lado, tambi√©n utilizamos el **√≠ndice de turbulencia** para inducir aversi√≥n al riesgo. Este √≠ndice de turbulencia se define as√≠:

$$
\mathcal{d}_t = \bold{({y_t} - \mu) \Sigma^{-1} ({y_t} - \mu)'}
$$

- $d_t$: turbulencia de un bien para tiempo t.
- $\bold{y_t}$: vector de los rendimientos de los bienes en tiempo t.
- $\bold{\mu}$: vector de rendimientos promedios hist√≥ricos del bienes en tiempo t.
- $\bold{\Sigma}$: matriz de covarianza de rendimientos promedios hist√≥ricos de los bienes en tiempo t.

## Creando una m√°quina de RL para trading

---

### Datos

---

Se toman los datos del Dow Jones, desde el 01/01/2009 hasta el 08/05/2020. Luego, se divide de esta manera:

1. **Entrenamiento**: del 01/01/2009 al 31/12/2014.
2. **Validaci√≥n**: del 01/10/2015 al 31/12/2015.
3. **Trading**: del 01/01/2016 al 08/05/2020.

### Trading visto como un proceso de decisi√≥n de Markov

---

- **Estados** $s = [\bold{p, h}, b]$: $\bold{p}$ es el vector con los precios, $\bold{h}$ el vector con la cantidad de stocks disponibles y $b$ el balance.
- **Acci√≥n $a$**: un vector de acciones sobre los stocks. Las acciones disponibles por cada stock son comprar, vender y holdear, lo que resulta en aumentar, disminuir o dejar identico la cantidad de stocks $h$, respectivamente.
- **Recompensa $r(s,a,s‚Äô)$**: recompensa directa de tomar acci√≥n $a$ en estado $s$ y llegar a estado $s‚Äô$.
- **Pol√≠tica $\pi(s)$**: distribuci√≥n de probabilidad de acciones en estado $s$.
- **Q-Valor $Q^\pi(s,a)$**: valor esperado de tomar acci√≥n $a$ en estado $s$, y luego continuar seg√∫n la pol√≠tica $\pi$.

![Untitled](old/Projects%2091a38a7522204a1eaaeb87fd0f122308/Teor√≠a%20del%20proyecto%207771eff31ff94be8b368ad15086276fe/Untitled%202.png)

### Restricciones

---

- **Liquidez de mercado**: asumimos que el mercado de acciones no ser√° afectado por nuestro agente.
- **Balance no-negativo**: las acciones permitidas no pueden resultar en balance negativo.
- **Costo de transacciones**: aumismo que el costo de transacci√≥n es 1/1000 del valor de cada transacci√≥n, sea compra o venta.
- **Aversi√≥n al riesgo a la quiebra de mercado**: utilizamos el √≠ndice de turbulencia que mide movimientos de precios extremos en t√≠tulos valores.

### Objetivo: maximizaci√≥n de rendimientos

---

La funci√≥n de recompensa se define como el cambio en el valor del portafolio cuando se toma acci√≥n $a$ en estado $s$ y se llega a estado $s+1$.

El objetivo entonces conseguir la pol√≠tica que maximice el cambio en el cambio del valor del portafolio $r(s_t, a_t, s_{t+1})$. 

### Definici√≥n del espacio de estados y acciones

---

- **Espacio de estados**: utilizamos un vector de 181 dimensiones. Por cada stock, consideramos su precio, la cantidad disponible, MACD, RSI, CCI y ADX. Son 30 stocks, por lo que son 180 inputs. Finalmente, agregamos un √∫ltimo input para nuestro balance.
- **Espacio de acciones**: para un stock determinado, el espacio de acciones es $\{-k,\dots,-1,0,1,\dots,k\}$, donde $k$ y $-k$ representa los stocks que podemos comprar y vender respectivamente. Agregamos la condici√≥n de $k \le h_{max}$, este √∫ltimo es un par√°metro predefinido para una cantidad m√°xima de compra de un stock.

Deducimos entonces que para m√∫ltiples stocks, el tama√±o del espacio de acciones es $(2k+1)^{30}$.

Normalizamos finalmente el el espacio de acci√≥n a $[-1, 1]$. Esto ya que A2C y PPO definen la pol√≠tica direcctamente en una distribuci√≥n gaussiana, por lo que necesita ser normalizada y sim√©trica.

## Estrategia de trading

---

Bas√≠camente, se utilizan los tres agentes mencionados (A2C, DDPG y PPO) en paralelo.

1. **Durante el subconjunto de entrenamiento**: usamos una ventana de n meses que alimentamos a nuestros agentes para entrenar, esto se hace cada 3 meses. Es decir, se entrenan los agentes en los primeros 3 meses, luego se entrenan los agentes con los primeros 6 meses, luego con los primeros 9 meses, as√≠ hasta llegar al final del subconjunto de entrenamiento.
2. **Durante el subconjunto de validaci√≥n**: utilizamos una ventana m√≥vil de 3 meses para evaluar y ajustar par√°metros de cada agente.
3. **Durante el trading**: tomamos el agente que actu√≥ mejor en los √∫ltimos 3 meses (seg√∫n el mayor ratio de Sharpe) y lo seleccionamos para tradear en los pr√≥ximos 3 meses.

# 4. Links relevantes

---

[Deep Reinforcement Learning for Automated Stock Trading](https://towardsdatascience.com/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02)

[Deriving Policy Gradients and Implementing REINFORCE](https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63)

[Deep Q-Network (DQN)-II](https://towardsdatascience.com/deep-q-network-dqn-ii-b6bf911b6b2c)

[Understanding Actor Critic Methods](https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f)

[Deep Deterministic Policy Gradients Explained](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b)

[Deep Deterministic Policy Gradient(DDPG)‚Ää-‚Ääan off-policy Reinforcement Learning algorithm](https://medium.com/intro-to-artificial-intelligence/deep-deterministic-policy-gradient-ddpg-an-off-policy-reinforcement-learning-algorithm-38ca8698131b)

[Understanding and Implementing Proximal Policy Optimization (Schulman et al., 2017)](https://towardsdatascience.com/understanding-and-implementing-proximal-policy-optimization-schulman-et-al-2017-9523078521ce)

[Sharpe Ratio Definition](https://www.investopedia.com/terms/s/sharperatio.asp)

[Measuring Financial Turbulence and Systemic Risk](https://towardsdatascience.com/measuring-financial-turbulence-and-systemic-risk-9d9688f6eec1)