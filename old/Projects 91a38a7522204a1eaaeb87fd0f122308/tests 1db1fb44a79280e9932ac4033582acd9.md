# tests

### 2.3.2

14
2. Overview of Supervised Learning
the component Gaussians to use, and then generates an observation from
the chosen density. In the case of one Gaussian per class, we will see in
Chapter 4 that a linear decision boundary is the best one can do, and that
our estimate is almost optimal. The region of overlap is inevitable, and
future data to be predicted will be plagued by this overlap as well.
In the case of mixtures of tightly clustered Gaussians the story is dif-
ferent. A linear decision boundary is unlikely to be optimal, and in fact is
not. The optimal decision boundary is nonlinear and disjoint, and as such
will be much more diﬃcult to obtain.
We now look at another classiﬁcation and regression procedure that is
in some sense at the opposite end of the spectrum to the linear model, and
far better suited to the second scenario.
2.3.2
Nearest-Neighbor Methods
Nearest-neighbor methods use those observations in the training set T clos-
est in input space to x to form ˆY . Speciﬁcally, the k-nearest neighbor ﬁt
for ˆY is deﬁned as follows:
ˆY (x) = 1
k
X
xi∈Nk(x)
yi, 
(2.8)
where Nk(x) is the neighborhood of x deﬁned by the k closest points xi in
the training sample. Closeness implies a metric, which for the moment we
assume is Euclidean distance. So, in words, we ﬁnd the k observations with
xi closest to x in input space, and average their responses.
In Figure 2.2 we use the same training data as in Figure 2.1, and use
15-nearest-neighbor averaging of the binary coded response as the method
of ﬁtting. Thus ˆY is the proportion of ORANGE’s in the neighborhood, and
so assigning class ORANGE to ˆG if ˆY > 0.5 amounts to a majority vote in
the neighborhood. The colored regions indicate all those points in input
space classiﬁed as BLUE or ORANGE by such a rule, in this case found by
evaluating the procedure on a ﬁne grid in input space. We see that the
decision boundaries that separate the BLUE from the ORANGE regions are far
more irregular, and respond to local clus

ters where one class dominates.
Figure 2.3 shows the results for 1-nearest-neighbor classiﬁcation: ˆY is
assigned the value yℓof the closest point xℓto x in the training data. In
this case the regions of classiﬁcation can be computed relatively easily, and
correspond to a Voronoi tessellation of the training data. Each point xi
has an associated tile bounding the region for which it is the closest input
point. For all points x in the tile, ˆG(x) = gi. The decision boundary is even
more irregular than before.
The method of k-nearest-neighbor averaging is deﬁned in exactly the
same way for regression of a quantitative output Y , although k = 1 would
be an unlikely choice.

### Nearest-Neighbor Methods

14
2. Overview of Supervised Learning
the component Gaussians to use, and then generates an observation from
the chosen density. In the case of one Gaussian per class, we will see in
Chapter 4 that a linear decision boundary is the best one can do, and that
our estimate is almost optimal. The region of overlap is inevitable, and
future data to be predicted will be plagued by this overlap as well.
In the case of mixtures of tightly clustered Gaussians the story is dif-
ferent. A linear decision boundary is unlikely to be optimal, and in fact is
not. The optimal decision boundary is nonlinear and disjoint, and as such
will be much more diﬃcult to obtain.
We now look at another classiﬁcation and regression procedure that is
in some sense at the opposite end of the spectrum to the linear model, and
far better suited to the second scenario.
2.3.2
Nearest-Neighbor Methods
Nearest-neighbor methods use those observations in the training set T clos-
est in input space to x to form ˆY . Speciﬁcally, the k-nearest neighbor ﬁt
for ˆY is deﬁned as follows:
ˆY (x) = 1
k
X
xi∈Nk(x)
yi,
(2.8)
where Nk(x) is the neighborhood of x deﬁned by the k closest points xi in
the training sample. Closeness implies a metric, which for the moment we
assume is Euclidean distance. So, in words, we ﬁnd the k observations with
xi closest to x in input space, and average their responses.
In Figure 2.2 we use the same training data as in Figure 2.1, and use
15-nearest-neighbor averaging of the binary coded response as the method
of ﬁtting. Thus ˆY is the proportion of ORANGE’s in the neighborhood, and
so assigning class ORANGE to ˆG if ˆY > 0.5 amounts to a majority vote in
the neighborhood. The colored regions indicate all those points in input
space classiﬁed as BLUE or ORANGE by such a rule, in this case found by
evaluating the procedure on a ﬁne grid in input space. We see that the
decision boundaries that separate the BLUE from the ORANGE regions are far
more irregular, and respond to local clus

ters where one class dominates.
Figure 2.3 shows the results for 1-nearest-neighbor classiﬁcation: ˆY is
assigned the value yℓof the closest point xℓto x in the training data. In
this case the regions of classiﬁcation can be computed relatively easily, and
correspond to a Voronoi tessellation of the training data. Each point xi
has an associated tile bounding the region for which it is the closest input
point. For all points x in the tile, ˆG(x) = gi. The decision boundary is even
more irregular than before.
The method of k-nearest-neighbor averaging is deﬁned in exactly the
same way for regression of a quantitative output Y , although k = 1 would
be an unlikely choice.

### 2.3.2

14
2. Overview of Supervised Learning
the component Gaussians to use, and then generates an observation from
the chosen density. In the case of one Gaussian per class, we will see in
Chapter 4 that a linear decision boundary is the best one can do, and that
our estimate is almost optimal. The region of overlap is inevitable, and
future data to be predicted will be plagued by this overlap as well.
In the case of mixtures of tightly clustered Gaussians the story is dif-
ferent. A linear decision boundary is unlikely to be optimal, and in fact is
not. The optimal decision boundary is nonlinear and disjoint, and as such
will be much more diﬃcult to obtain.
We now look at another classiﬁcation and regression procedure that is
in some sense at the opposite end of the spectrum to the linear model, and
far better suited to the second scenario.
2.3.2
Nearest-Neighbor Methods
Nearest-neighbor methods use those observations in the training set T clos-
est in input space to x to form ˆY . Speciﬁcally, the k-nearest neighbor ﬁt
for ˆY is deﬁned as follows:
ˆY (x) = 1
k
X
xi∈Nk(x)
yi,
(2.8)
where Nk(x) is the neighborhood of x deﬁned by the k closest points xi in
the training sample. Closeness implies a metric, which for the moment we
assume is Euclidean distance. So, in words, we ﬁnd the k observations with
xi closest to x in input space, and average their responses.
In Figure 2.2 we use the same training data as in Figure 2.1, and use
15-nearest-neighbor averaging of the binary coded response as the method
of ﬁtting. Thus ˆY is the proportion of ORANGE’s in the neighborhood, and
so assigning class ORANGE to ˆG if ˆY > 0.5 amounts to a majority vote in
the neighborhood. The colored regions indicate all those points in input
space classiﬁed as BLUE or ORANGE by such a rule, in this case found by
evaluating the procedure on a ﬁne grid in input space. We see that the
decision boundaries that separate the BLUE from the ORANGE regions are far
more irregular, and respond to local clus

ters where one class dominates.
Figure 2.3 shows the results for 1-nearest-neighbor classiﬁcation: ˆY is
assigned the value yℓof the closest point xℓto x in the training data. In
this case the regions of classiﬁcation can be computed relatively easily, and
correspond to a Voronoi tessellation of the training data. Each point xi
has an associated tile bounding the region for which it is the closest input
point. For all points x in the tile, ˆG(x) = gi. The decision boundary is even
more irregular than before.
The method of k-nearest-neighbor averaging is deﬁned in exactly the
same way for regression of a quantitative output Y , although k = 1 would
be an unlikely choice.

### Nearest-Neighbor Methods

14
2. Overview of Supervised Learning
the component Gaussians to use, and then generates an observation from
the chosen density. In the case of one Gaussian per class, we will see in
Chapter 4 that a linear decision boundary is the best one can do, and that
our estimate is almost optimal. The region of overlap is inevitable, and
future data to be predicted will be plagued by this overlap as well.
In the case of mixtures of tightly clustered Gaussians the story is dif-
ferent. A linear decision boundary is unlikely to be optimal, and in fact is
not. The optimal decision boundary is nonlinear and disjoint, and as such
will be much more diﬃcult to obtain.
We now look at another classiﬁcation and regression procedure that is
in some sense at the opposite end of the spectrum to the linear model, and
far better suited to the second scenario.
2.3.2
Nearest-Neighbor Methods
Nearest-neighbor methods use those observations in the training set T clos-
est in input space to x to form ˆY . Speciﬁcally, the k-nearest neighbor ﬁt
for ˆY is deﬁned as follows:
ˆY (x) = 1
k
X
xi∈Nk(x)
yi,
(2.8)
where Nk(x) is the neighborhood of x deﬁned by the k closest points xi in
the training sample. Closeness implies a metric, which for the moment we
assume is Euclidean distance. So, in words, we ﬁnd the k observations with
xi closest to x in input space, and average their responses.
In Figure 2.2 we use the same training data as in Figure 2.1, and use
15-nearest-neighbor averaging of the binary coded response as the method
of ﬁtting. Thus ˆY is the proportion of ORANGE’s in the neighborhood, and
so assigning class ORANGE to ˆG if ˆY > 0.5 amounts to a majority vote in
the neighborhood. The colored regions indicate all those points in input
space classiﬁed as BLUE or ORANGE by such a rule, in this case found by
evaluating the procedure on a ﬁne grid in input space. We see that the
decision boundaries that separate the BLUE from the ORANGE regions are far
more irregular, and respond to local clus

ters where one class dominates.
Figure 2.3 shows the results for 1-nearest-neighbor classiﬁcation: ˆY is
assigned the value yℓof the closest point xℓto x in the training data. In
this case the regions of classiﬁcation can be computed relatively easily, and
correspond to a Voronoi tessellation of the training data. Each point xi
has an associated tile bounding the region for which it is the closest input
point. For all points x in the tile, ˆG(x) = gi. The decision boundary is even
more irregular than before.
The method of k-nearest-neighbor averaging is deﬁned in exactly the
same way for regression of a quantitative output Y , although k = 1 would
be an unlikely choice.