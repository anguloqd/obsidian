# inform√°tica 2 // redes neuronales artificiales

Date de cr√©ation: June 12, 2022 11:44 PM
Modifi√©: November 8, 2024 10:25 AM

[Artificial Neural Network | Brilliant Math & Science Wiki](https://brilliant.org/wiki/artificial-neural-network/)

[Backpropagation | Brilliant Math & Science Wiki](https://brilliant.org/wiki/backpropagation/)

[Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/chap2.html)

# Introducci√≥n

---

## Contexto

- Hace falta decir que hay una infinidad de funciones que puede pasar por todos los pares ordenados, pero la mejor funci√≥n es la que puede, adem√°s, hacer funciones.
- Las RNA (redes neuronales artificiales) son suficientemente flexibles para dos tipos de problemas:
    - **Clasificaci√≥n**: predicci√≥n de clasificaci√≥n de elementos que no tienen una relaci√≥n num√©rica.
    - **Regresi√≥n**: variables con relaci√≥n num√©rica.
- Un **algoritmo de aprendizaje no supervisado** es lo mismo que con aprendizaje supervisado, pero no utiliza ejemplos conocidos para llegar a la mejor funci√≥n.
- Pueden usar dos tipos de aprendizaje (incluso ambos): aprendizaje por lotes o aprendizaje "en l√≠nea" o continuo.
    - **Aprendizaje por lotes (‚Äùbatch learning‚Äù)**: utiliza un lote de informaci√≥n que ser√° alimentada a la RNA.
    - **Aprendizaje en l√≠nea (‚Äùonline learning‚Äù)**: utiliza informaci√≥n que va llegando continuamente a ella.

# Recordatorio de matem√°ticas

---

## √Ålgebra y c√°lculo b√°sico

Lo siguiente son las bases de matem√°ticas necesarias utilizadas en el cuadro de redes neuronales:

- Tal como aprendimos antes, una neurona tiene una activaci√≥n igual a $f(wx + b)$, donde cada input $x$ es ponderado con $w$, y a toda esa suma se le agrega el umbral $b$ y por √∫ltimo se le aplica una funci√≥n $f$.
- La "magnitud" o m√≥dulo $|| \cdot ||$ de un vector (longitud desde el origen, magnitud en el sentido euclidiano) nos ayuda cuando tratamos tiempos de convergencia de entrenamiento.
    
    $$
    ||\bold x||_2 = ||(x_1, \dots, x_n)||_2 =\sqrt{\sum_{k=1}^n x^2_k}
    $$
    
- La normalizaci√≥n de un vector es el proceso de tomar un vector y luego crear un vector de magnitud 1 con la misma direcci√≥n que el vector original.
    
    $$
    \vec{p} = \left( \frac{a}{\sqrt{a^2 + b^2 + c^2}}, \frac{b}{\sqrt{a^2 + b^2 + c^2}}, \frac{c}{\sqrt{a^2 + b^2 + c^2}} \right)
    $$
    
- El **producto di√°dico** o **producto tensorial** (en ingl√©s, *outer product*, aunque esa traducci√≥n no parece existir en espa√±ol ni en franc√©s), es una operaci√≥n entre un vector columna y un vector fila, pero no se debe confundir con el **producto punto** entre una columna y un vector.

Teniendo dos vectores columnas, $\bold u$ y $\bold v$, tenemos que:
    
    $$
    \bold u = \begin{bmatrix} a \\ b \\ c \end{bmatrix} \quad , \quad \bold v = \begin{bmatrix} d \\ e \end{bmatrix} \implies \bold u \bold v^\mathsf{T} = \begin{bmatrix} a \\ b \\ c \end{bmatrix} \begin{bmatrix} d & e \end{bmatrix} = \begin{bmatrix} a \cdot d & a \cdot e \\ b \cdot d & b \cdot e \\ c \cdot d & c \cdot e \end{bmatrix}
    $$
    
- El **gradiente de una funci√≥n** es el vector columna de todas la derivadas parciales con respecto a sus variables inputs.
    
    $$
    \nabla f(x,y, z) =
    \begin{bmatrix}
    \frac{\partial f}{\partial x}
    \\[5pt]
    \frac{\partial f}{\partial y}
    \\[5pt]
    \frac{\partial f}{\partial z}
    \end{bmatrix}
    $$
    
    - Si tomamos un punto en la funci√≥n, el gradiente evaluado en ese punto muestra el vector con **origen en ese punto** tal que el valor de la funci√≥n aumenta m√°s.
    
    En la pr√≥xima imagen, empezamos arbitrariamente en el punto $(1,1)$, donde el valor de la funci√≥n $C$ es igual $4$. Queremos saber hacia qu√© direcci√≥n el valor actual de la funci√≥n aumenta lo mas posible.
        
        ![El gradiente de C es $\nabla C = \begin{bmatrix} 3x & y\end{bmatrix}^\mathsf{T}$, y luego es evaluado en el punto $(1,1)$, lo que nos dice que la direcci√≥n de mayor crecimiento es la direcci√≥n apuntada por el vector [3, 1].](informa%CC%81tica%202%20redes%20neuronales%20artificiales%200423ddd123fc4fac99c7432906f372dc/image.png)
        
        El gradiente de C es $\nabla C = \begin{bmatrix} 3x & y\end{bmatrix}^\mathsf{T}$, y luego es evaluado en el punto $(1,1)$, lo que nos dice que la direcci√≥n de mayor crecimiento es la direcci√≥n apuntada por el vector [3, 1].
        
        ![Viendo el plot de $C(x,y) = \frac{3}{2}x^2 + \frac{1}{2}y^2$ (donde el eje $z$ son los valores de $C(x,y)$), si imaginamos el punto $p = (1,1,2)$ sobre la funci√≥n y ‚Äúplasmamos‚Äù el vector $[3,1]$ sobre el tejido de la funci√≥n con origen en $p$ (es decir, el vector $[4,2]$), obtendremos la direcci√≥n mas directa al crecimiento del valor de $C$.](informa%CC%81tica%202%20redes%20neuronales%20artificiales%200423ddd123fc4fac99c7432906f372dc/image%201.png)
        
        Viendo el plot de $C(x,y) = \frac{3}{2}x^2 + \frac{1}{2}y^2$ (donde el eje $z$ son los valores de $C(x,y)$), si imaginamos el punto $p = (1,1,2)$ sobre la funci√≥n y ‚Äúplasmamos‚Äù el vector $[3,1]$ sobre el tejido de la funci√≥n con origen en $p$ (es decir, el vector $[4,2]$), obtendremos la direcci√≥n mas directa al crecimiento del valor de $C$.
        
    
    ![Los puntos azules son $(1,1)$, $(1,1,2)$, y el punto verde deber√≠a representar el vector gradiente $(4,2)$ o equivalentemente $[3,1]$ si tomamos como origen $(1,1)$.](60bbd781-439d-4829-8a10-e6abcae39adf.png)
    
    Los puntos azules son $(1,1)$, $(1,1,2)$, y el punto verde deber√≠a representar el vector gradiente $(4,2)$ o equivalentemente $[3,1]$ si tomamos como origen $(1,1)$.
    
    ![El punto verde sobre la funci√≥n es la imagen del vector gradiente evaluado en $(1,1)$, es decir, $C(4,2)$, cuyo valor es $26$.](8edf9476-8d82-45c8-8b20-d67c803910a8.png)
    
    El punto verde sobre la funci√≥n es la imagen del vector gradiente evaluado en $(1,1)$, es decir, $C(4,2)$, cuyo valor es $26$.
    
    - Lo interesante es que si multiplicamos por $-1$ el gradiente, obtenemos la direcci√≥n de mayor descenso.

## Descenso por gradiente, en la teor√≠a

Normalmente, para optimizar una funci√≥n, igualamos su derivada a $0$ y buscamos los valores de las variables. Sin embargo, este √∫ltimo paso suele ser imposible, por lo que tomamos una soluci√≥n heur√≠stica: el descenso por gradiente.

El descenso por gradiente es un algoritmo iterativo para encontrar el m√≠nimo de una funci√≥n.

1. Empezamos en un punto $p$ aleatorio.
2. Computamos $\nabla G(p)$, que es el gradiente en ese punto.
3. Nos movemos al contrario de esa direcci√≥n un poco, es decir, $p_{t+1} = p_t - \varepsilon \nabla G(p)$, donde $\varepsilon$ es el tama√±o del paso o formalmente llamada ‚Äútasa de aprendizaje‚Äù, y que en la practica suele ser un valor peque√±o.
4. Comenzamos desde el paso 2.
5. Nos detenemos en un momento donde la diferencia abs. entre $p_{t+1}$ y $p_t$ sea arbitrariamente peque√±a.

![image.png](old/Otras%20libretas%2038b07a4041b949a7aba5ec3a03d2931f/Brilliant%20375bdad652564477b33bcb24abdc6919/inform√°tica%202%20redes%20neuronales%20artificiales%200423ddd123fc4fac99c7432906f372dc/image%202.png)

# Perceptrones

---

## Algoritmo de aprendizaje de un perceptr√≥n

- Esta es una tarea de clasificaci√≥n, por lo que los puntos forman parte de dos grupos: los llamaremos 1 y -1.
- Primero, queremos definir una funci√≥n de "p√©rdida". Esta funci√≥n de p√©rdida nos indica la suma de cu√°n lejos est√° la observaci√≥n real yi de la proyecci√≥n o frontera de decisi√≥n (w*xi + u), para cada dato de √≠ndice i.
    - Ejemplo: pensemos en un punto bien clasificado cuyo grupo es (1) y est√° por encima de la frontera de decisi√≥n. No nos importa cu√°n lejos est√© el punto de la recta, sino que est√© en el buen lado de la recta.
    - Por lo tanto, no nos interesa el valor num√©rico de la distancia [xi - (w*xi + u)], sino que sea positiva.
    - Similarmente, si xi pertenece al grupo (-1), queremos que est√© por debajo de la frontera., sin importar cu√°n debajo. Entonces, queremos que la distancia [xi - (w*xi + u)] sea negativa.
    - Contamos un punto como malclasificado si, xi es grupo (1) y distancia es negativa, o si xi es grupo (-1) y distancia es positiva.
    - Finalmente, podemos simplificar si aplicamos la funci√≥n signo() a la distancia [xi - (w*xi + b)]. El resultado ser√≠a:
        
        Notemos que lo que est√° dentro de los corchetes es una preposici√≥n. Los corchetes son una funci√≥n indicaci√≥n, que devuelve 1 si la prop. es verdadera y 0 si es falsa. De esa manera, contamos finalmente los puntos malclasificados.El algoritmo se va aplicar hasta que la funci√≥n p√©rdida sea = 0.
        
- Xi ser√≠a el input. Eso s√≠, es un vector input, por lo que no es necesariamente un n√∫mero, sino un vector columna de n dimensiones. Yi ser√≠a finalmente el color o grupo binario al que pertenece la observaci√≥n.
- Para actualizar el peso y el umbral, la f√≥rmula es la siguiente, aplic√°ndola con el par (xi, yi) que fue malclasificado:
- El algoritmo es entonces el siguiente:
    1. Inicializamos el peso w y el umbral b. Por ejemplo, los igualamos ambos a 0.
    2. Luego, vamos a recorrer nuestro conjunto de puntos [(x1, y1), ..., (xn,yn)] y vamos aplicando el criterio de clasificaci√≥n hasta que encontremos el primer punto malclasificado.
    3. Actualizamos el peso y el umbral con respecto a ese punto donde tuvimos un error. Luego recomenzamos desde ese punto i de 1 en 1 hasta el punto (i - 1). Puede ser que el mismo punto i vuelva a fallar, por lo que volvemos a calibrar w y b.
    4. Hemos encontrado una frontera de decisi√≥n v√°lida cuando nuestra funci√≥n de p√©rdida = 0 para ciertos par√°metros w y b.
        
        (El GIF de ejemplo mantiene un umbral siempre = 0)
        
- ¬°La frontera de decisi√≥n no es √∫nica! Hay una infinidad de l√≠neas que separan dos grupos.

## Limitaciones y nueva funci√≥n de p√©rdida

- Cuando los datos siguen una funci√≥n de base que no es lineal, un clasificador lineal no act√∫a de la mejor manera. Hay un par de cambios que podemos hacer:
- Tomamos una nueva funci√≥n p√©rdida llamada "p√©rdida conjunta" o "p√©rdida de eje". En vez de contar los errores, contamos la magnitud del error. Esto penaliza propocionalmente los puntos m√°s lejos, e incluso puede penalizar los puntos bien clasificados pero muy cerca de la frontera de decisi√≥n.
- Luego, tambi√©n queremos que el algoritmo sea robusto a los outliers. Los outliers aumentan considerablemente el tiempo de entrenamiento, pues modifican mucho w y b tal que los valores que ya estaban bien clasificados pasan a estar mal clasificados.
- La nueva funci√≥n de p√©rdida definitiva, donde L es la p√©rdida de eje, ser√≠a entonces la siguente. Es conocido cono el algoritmo "pasivo-agresivo".
    
    
- **Dato**: agregar una capa intermedia de neuronas nos permite modelar fronteras de decisi√≥n no lineales.

# Retropropagaci√≥n

---

## Ingredientes

La retropropagaci√≥n es un m√©todo que nos permite encontrar el gradiente, tal que podemos aplicar el algoritmo de descenso por gradiente.

<aside>
üí° Se llama ‚Äúretropropagaci√≥n‚Äù porque, para calcular el gradiente y actualizar los par√°metros, comenzamos calculando la derivada delos par√°metros de la ultima capa $\bold w^{(L)}$.

Luego, la derivada de la capa $L-1$ depende de los valores de la derivada calculada en la capa $L$, y luego los valores de la capa $L-2$ dependen de los valores de la capa $L-1$, as√≠ hasta llegar a la primera capa oculta.

As√≠, vemos que los valores de las capas de neuronas frontales influencias los valores de las capas de neuronas traseras.

</aside>

### Dataset

El dataset X es una conjunto de pares input-output $(\vec x_i, \vec y_i)$, donde $\vec x_i$ es el input mientras que $\vec y_i$ es el output. Recordemos que cada vector es una lista de valores, entonces los inputs y los outputs pueden ser multidimensionales.

Tal conjunto de inputs-outputs de tama√±o $N$ es notado como sigue :

$$
X = \big\{ (\vec x_i, \vec y_i), \dots, (\vec x_N, \vec y_N)\big\}
$$

### Red neuronal

Un poco obvio, pero evidentemente necesitamos una red neuronal. Lo importante es que sus par√°metros‚Äîtodos los pesos $w^l_{jk}$ y umbrales $b^l_j$‚Äîest√°n contenidos en la variable $\theta$.

### Funci√≥n de costo

Primero, necesitamos una funci√≥n de costo. No la llamamos ‚Äúerror‚Äù puesto a que utilizamos el nombre ‚Äúerror‚Äù para otro t√©rmino importante en la retropropagaci√≥n. La funci√≥n mas cl√°sica utilizada es la funci√≥n de costo cuadr√°tico total $(C_1)$ o costo cuadr√°tico medio $(C_2)$. 

$$
\begin{align*}
&C_1(X, \theta)= \frac{1}{2} \sum_{i=1}^N (\hat y_i - y_i)^2\\
&C_2(X, \theta)= \frac{1}{2N} \sum_{i=1}^N (\hat y_i - y_i)^2\\
\end{align*}
$$

La raz√≥n de por qu√© hay un coeficiente de $1/2$ en frente de la suma de costos, es porque, al derivar la suma de costos, el exponente $2$ que baja se cancela al multiplicarse con el $1/2$ que esta al frente. Esto es mas c√≥modo que si no tuvi√©semos ese coeficiente al frente.

Un punto importante es que, en este contexto, no utilizamos el nombre ‚Äúerror‚Äù para la funci√≥n de costo, puesto a que utilizamos el nombre ‚Äúerror‚Äù para otro t√©rmino importante en la retropropagaci√≥n.

## La retropropagaci√≥n en s√≠

### Primeras derivadas del costo

Lo que estamos buscando es aplicar una iteraci√≥n de actualizaci√≥n de par√°metros. Si aplicamos la teor√≠a en la practica, lo que estamos buscando es :

$$
\theta_{t+1}=\theta_t-\alpha\frac{\partial C(X, \theta_t)}{\partial \theta_t}
$$

Recordemos que $\theta_t$ contiene los pesos $w^{(l)}_{jk}$ y umbrales $b^{(l)}_j$ en tiempo $t$. Ademas, denotaremos $C(X,\theta)$ como simplemente $C$. Si bien podemos derivar $C$ con respecto a $\theta_t$ directamente, es mejor derivar $C$ con respecto a $w^{(l)}_{jk}$ y $b^{(l)}_j$ de manera separada para poder aprender.

Recordemos algunas formulas:

$$
\begin{align*}
& z^{(l)}_j = w^{(l)}_{jk} a^{(l-1)}_j+b^{(l)}_j \\
& a^{(l)}_j= \sigma(z^{(l)}_j)
\end{align*}
$$

Donde $z^{(l)}_j$ es la suma ponderada o ‚Äúsuma intermedia‚Äù, y $a^{(l)}_j$ es la activaci√≥n. $\sigma$ es una funci√≥n de activaci√≥n. Con esto, ya podemos empezar a derivar:

$$
\begin{align*}
& \frac{\partial C}{\partial w^{(l)}_{jk}} = 
\underbrace{
\frac{\partial C}{\partial a^{(l)}_j} \frac{\partial a^{(l)}_j}{\partial z^{(l)}_j} 
}_{\delta^{(l)}_j}
\frac{\partial z^{(l)}_j}{\partial w^{(l)}_{jk}}

\\[35pt]

& \frac{\partial C}{\partial b^{(l)}_{j}} = 
\underbrace{
\frac{\partial C}{\partial a^{(l)}_j} \frac{\partial a^{(l)}_j}{\partial z^{(l)}_j} 
}_{\delta^{(l)}_j}\frac{\partial z^{(l)}_j}{\partial b^{(l)}_{j}}
\end{align*}

\\[-10pt]
$$

Notemos que hay un factor com√∫n entre ambas derivadas parciales: $\delta^{(l)}_j$, tambi√©n llamado el ‚Äúerror‚Äù. Concretamente, lo definimos como sigue:

$$
\delta_j^{(l)} \equiv
\frac{\partial C}{\partial z^{(l)}_j} =
\frac{\partial C}{\partial a^{(l)}_j} \frac{\partial a^{(l)}_j}{\partial z^{(l)}_j}
$$

Calcularemos exactamente lo que es $\delta_j^{(l)}$ mas tarde. Por los momentos, calcularemos solo las derivadas de la suma intermedia con respecto a los pesos $w^{(l)}_ {jk}$ y umbrales $b^{(l)}_j$.

$$
\frac{\partial z^{(l)}_j}{\partial w^{(l)}_{jk}} = a^{(l-1)}_k,\hspace{5pt} \frac{\partial z^{(l)}_j}{\partial b^{(l)}_{j}} = 1

\\[15pt]

\implies \frac{\partial C}{\partial w^{(l)}_{jk}}=\delta^{(l)}_ja^{(l-1)}_k,

\hspace{5pt}

\frac{\partial C}{\partial b^{(l)}_{j}} = \delta^{(l)}_j
$$

**Una cosa importante a saber es que derivamos los calculamos de la capa de salida de manera diferente que las capas ocultas.**

Es √∫til tambi√©n escribir $\delta^{(L)}$ en forma matricial y no en forma componente:

$$
\bold \delta^{(L)}=\nabla_{\bold a^{(L)}}  C\odot \sigma^\prime(\bold z^{(L)}), \text{ donde }\nabla_{\bold a^{(L)}} C\equiv[\partial C/\partial a^{(L)}_j]^{\mathsf{T}}_{\forall j \in [1, n_L]}
$$

Y, dado que definimos $C$ como el error cuadr√°tico, tenemos que:

$$
\delta^{(L)}=(\bold a^{(L)} - \bold y) \odot \sigma^\prime(\bold z^{(L)})
$$

### La capa de salida (outputs)

Suponiendo que hemos definido la funci√≥n de error como $C(X, \theta)= \frac{1}{2} \sum_{i=1}^N (\hat y_i - y_i)^2$, calculando los otros t√©rminos y poniendo todo junto:

$$
\begin{align*}

&& \frac{\partial C}{\partial a^{(L)}_j} = (a^{(L)}_j-y_i),\hspace{5pt} \frac{\partial a^{(L)}_j}{\partial z^{(L)}_j} =\sigma^\prime(z^{(L)}_j)

\\[30pt]

& \implies & \delta^{(L)}_j=(a^{(L)}_j-y_i) \space \sigma^\prime(z^{(L)}_j)

\\[30pt]

& \implies & \frac{\partial C}{\partial w^{(L)}_{jk}} =(a^{(L)}_j-y_i) \space \sigma^\prime(z^{(L)}_j) a^{(L-1)}_j,

\\[15pt]

&& \text{ y tambi√©n } \frac{\partial C}{\partial b^{(L)}_{j}}=(a^{(L)}_j-y_i) \space \sigma^\prime(z^{(L)}_j) 

\end{align*}

$$

### Las capas ocultas

Para las capas ocultas, nos sera muy √∫til la llamada ‚Äúformula de retropropagaci√≥n‚Äù. Recordemos que en las capas ocultas $l$, $1 < l < L$, donde $L$
 es la ultima capa de la red.

Trabajando desde la definici√≥n de $\delta^{(l)}_j$, tenemos que:

$$
\delta^{(l)}_j = 
\frac{\partial C}{\partial z^{(l)}_j} =
\sum_{i=1}^{n_{l+1}} \frac{\partial C}{\partial z^{(l+1)}_i}\frac{\partial z^{(l+1)}_i}{\partial z^{(l)}_j} =
\sum_{i=1}^{n_{l+1}} \delta^{(l+1)}_i\frac{\partial z^{(l+1)}_i}{\partial z^{(l)}_j}
$$

Si alguna vez no te acuerdas de por qu√© $\delta^{(l)}_j = 
\frac{\partial C}{\partial z^{(l)}_j} =
\sum_{i=1}^{n_{l+1}} \frac{\partial C}{\partial z^{(l+1)}_i}\frac{\partial z^{(l+1)}_i}{\partial z^{(l)}_j}$ , toma $\frac{\partial C}{\partial z^{(l)}_j}$ y reemplaza $C$ con su definici√≥n con una red neuronal simple sin capas ocultas, con un input y dos outputs, y trabaja desde ah√≠. La suma que te confunde mucho se presenta naturalmente.

Para determinar la segunda derivada parcial, recordemos que:

$$
z^{(l+1)}_j=\sum_{k=0}^{n_{l}}w^{(l+1)}_{jk}a_k^{(l)}+b^{(l+1)}_j=\sum_{k=0}^{n_{l}}w^{(l+1)}_{jk}\sigma(z_k^{(l)})+b^{(l+1)}_j
$$

Diferenciando, tenemos que:

$$
\frac{\partial z^{(l+1)}_i}{\partial z^{(l)}_j}=w^{(l+1)}_{jk}\sigma^\prime(z^{(l)}_j)
$$

Finalmente, volviendo a la definici√≥n de $\delta^{(l)}_j$, deducimos la popular formula recursiva de retropropagaci√≥n:

$$
\delta^{(l)}_j=\sum_{i=1}^{n_{k+1}} \delta^{(l+1)}_i w^{(l+1)}_{jk}\sigma^\prime(z^{(l)}_j)
$$

La formula de retropropagaci√≥n, tambi√©n puede ser escrito en forma matricial y no en forma componente:

$$
\bold \delta^{(l)}=\left(\left(\bold w^{(l+1)}\right)^\mathsf{T} \bold \delta^{(l+1)} \right)\odot\sigma^\prime(\bold z^{(l)})
$$

## Descenso por gradiente, en la pr√°ctica

Finalmente, conectamos la retropropagaci√≥n con el descenso por gradiente. Antes, recordemos lo que es un paso de descenso de gradiente para un vector de par√°metros $\bold p$ con respecto a una funci√≥n a optimizar $f$:

$$
\bold p \leftarrow \bold p - \eta \space \nabla_{\bold p}f
$$

Queremos hacer esto con $\bold w^{(l)}$ y $\bold b^{(l)}$, por cada capa $l$, por lo que necesitamos saber qu√© es $\nabla_{\bold w^{(l)}}C$ y $\nabla_{\bold b^{(l)}}C$ y, en consecuencia, saber qu√© son $\partial C / \partial w_{jk}^{(l)}$ y $\partial C / \partial b_{j}^{(l)}$. Una definici√≥n m√°s general de estos √∫ltimos es:

$$
\frac{\partial C}{\partial w^{(l)}_{jk}} = 

\delta^{(l)}_j a^{(l-1)}_k,

\text{ y }

\frac{\partial C}{\partial b^{(l)}_j} = 

\delta^{(l)}_j
$$

Dado a que tomamos el gradiente de $C$ con respecto a los vectores $\bold w^{(l)}$ y $\bold b^{(l)}$, vale decir que la forma de estos gradientes, particularmente $\bold w^{(l)}$, es algo complicada e inc√≥moda.

$$
\nabla_{\bold w^{(l)}} C
=
\delta^{(l)}(\bold a^{(l-1)})^\mathsf T
=
\begin{bmatrix}
\delta_1^{(l)} a_1^{(l-1)} & \delta_1^{(l)} a_2^{(l-1)} & \cdots & \delta_1^{(l)} a_n^{(l-1)} \\
\delta_2^{(l)} a_1^{(l-1)} & \delta_2^{(l)} a_2^{(l-1)} & \cdots & \delta_2^{(l)} a_n^{(l-1)} \\
\vdots & \vdots & \ddots & \vdots \\
\delta_m^{(l)} a_1^{(l-1)} & \delta_m^{(l)} a_2^{(l-1)} & \cdots & \delta_m^{(l)} a_n^{(l-1)}
\end{bmatrix}

\\[8pt]

\nabla_{\bold b^{(l)}} C = \delta^{(l)}
=
\begin{bmatrix}
\delta_1^{(l)} \\
\delta_2^{(l)} \\
\vdots \\
\delta_m^{(l)}
\end{bmatrix}

$$

Perfecto. Por ultimo, supongamos que tenemos $m$ ejemplo de entrenamiento y una tasa de aprendizaje $\eta$. Para actualizar los pesos y umbrales, tenemos que, por cada ejemplo de entrenamiento $x$, la siguiente es la regla de actualizaci√≥n, que es simplemente el promedio de los valores de los gradientes por cada ejemplo de entrenamiento.

$$
\bold w^{(l)} \leftarrow \bold w^{(l)} - \eta  \left( \frac 1 m \sum_x 
\underbrace{
\delta^{(x, l)} \left( \bold a^{(x, l-1)} \right)^\mathsf{T}
}_{\nabla_{\bold w^{(l)}}C^{(x)}}
\right)

\\[10pt]

\bold b^{(l)} \leftarrow \bold b^{(l)} - \eta
\left( \frac 1 m \sum_x \underbrace{\delta^{(x, l)}
}_{\nabla_{\bold b^{(l)}}C^{(x)}}
\right)
$$

Notemos que este es el caso ideal, pero no suele ser el caso m√°s aplicable, pues toma demasiado tiempo y recursos computacionales. Piensa que el algoritmo debe usar *todos* los ejemplos de aprendizaje para solo hacer una iteraci√≥n de ajuste de los par√°metros, es decir, un paso en direcci√≥n al gradiente.

Una variante del descenso por gradiente es el **descenso por gradiente estoc√°stico**:

- En vez de utilizar todos los ejemplos de aprendizaje en cada paso de optimizaci√≥n, separamos los ejemplos en lotes, cada lote de tama√±o $m$.
- Luego, computamos un paso hacia el gradiente con *un* solo lote, lo que significa que ajustamos los todos los par√°metros una sola vez.
- Finalmente, volvemos a pasar otro lote de ejemplos. Y as√≠ hasta que los usamos todos.

## Retropropagaci√≥n como algoritmo

Finalmente, si quisi√©ramos implementar la retropropagaci√≥n como algoritmo en un lenguaje de programaci√≥n, el plan seria el siguiente:

1. Escoger al azar un mini-batch de ejemplos de entrenamiento
2. Por cada ejemplo de entrenamiento $x$: fijar el valor correspondiente de activation $\bold a^{(x,1)}$, y ejecutar los pasos siguientes:
    1. Evaluaci√≥n hacia adelante (feedforward): para cada capa $l \in \{2, 3, \dots, L\}$, calcular
        
        $$
        \bold z^{(x,l)} = \bold w^{(l)} \bold a^{(x, l-1)} + \bold b^{(l)}
        
        \\
        
        \bold a^{(x,l)} = \sigma( \bold z^{(x, l)})
        $$
        
    2. Error: calcular el vector de error $\delta$
        
        $$
        \bold \delta^{(x,L)} = \nabla_{\bold a^{(L)}}  C^{(x)} \odot \sigma^\prime(\bold z^{(x, L)})
        $$
        
    3. Retropropagaci√≥n del error: para cada capa $l \in \{L, L-1, L-2, \dots, 2\}$, calcular 
        
        $$
        \bold \delta^{(l)}=\left(\left(\bold w^{(l+1)}\right)^\mathsf{T} \bold \delta^{(l+1)} \right)\odot\sigma^\prime(\bold z^{(l)})
        $$
        
3. Para cada capa $l \in \{L, L-1, L-2, \dots, 2\}$, actualizar los pesos y umbrales con respecto a la regla siguiente:
    
    $$
    \bold w^{(l)} \leftarrow \bold w^{(l)} - \eta  \left( \frac 1 m \sum_x 
    \underbrace{
    \delta^{(x, l)} \left( \bold a^{(x, l-1)} \right)^\mathsf{T}
    }_{\nabla_{\bold w^{(l)}}C^{(x)}}
    \right)
    
    \\[10pt]
    
    \bold b^{(l)} \leftarrow \bold b^{(l)} - \eta
    \left( \frac 1 m \sum_x \underbrace{\delta^{(x, l)}
    }_{\nabla_{\bold b^{(l)}}C^{(x)}}
    \right)
    $$
    

# Redes neuronales convolucionales

---

[A Comprehensive Guide to Convolutional Neural Networks‚Ää‚Äî‚Ääthe ELI5 way](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)

Las RNC se basan en entrenar filtros (neuronas intermedias, por ejemplo) para reconocer rasgos "de base" en una imagen, como l√≠neas, por ejemplo. Luego, podemos extraer un mapa de rasgos de cada filtro/neurona, para saber en qu√© partes de la imagen se encuentra determinado rasgo.

## Operaciones de base

Las operaciones de base son: convoluci√≥n (convolution), relleno (padding) y zancada (stride).

### Convoluci√≥n (convolution)

$$
Y_j = g \left( b_j + \sum_i K_{ij} \otimes Y_i \right)
$$

- $Y_j$ (matriz): activaci√≥n de la neurona posterior $j$.
- $Y_i$ (matriz): activaci√≥n de la neurona anterior $i$.
- $K_{ij}$ (matriz): neurona filtro.
- $g$: funci√≥n no-lineal, normalmente ReLU.
- $\otimes$: operador de convoluci√≥n (producto de Hadamard), donde se multiplican las entradas de mismo √≠ndice de dos matrices $K$ e $Y$.

F√≠jate que es muy parecido a tomar la combinaci√≥n linea de pesos con inputs y luego sumar el umbral.

Primero, se toma cada entrada de la matriz $Y_i$ y se multiplica por su correspondiente en $K$. Luego, tomas la suma de las entradas de la matriz resultante y le a√±ades el umbral, por √∫ltimo aplicando la funci√≥n $g$.

### Zancada (striding)

La convoluci√≥n requiere una imagen y filtro de iguales dimensiones. Esto a veces no suele ser el caso. De hecho, regularmente, este no es el caso. El filtro suele ser mas peque√±o que la imagen a la que aplicamos el filtro.

La zancada la utilizamos cuando nuestro filtro es m√°s peque√±o que nuestra imagen, lo que nos permite aplicar varias veces el filtro a la imagen.

![Aqu√≠, la imagen es $\scriptstyle 3\times3$ y el filtro es $\scriptstyle 2\times2$.](old/Otras%20libretas%2038b07a4041b949a7aba5ec3a03d2931f/Brilliant%20375bdad652564477b33bcb24abdc6919/inform√°tica%202%20redes%20neuronales%20artificiales%200423ddd123fc4fac99c7432906f372dc/image%203.png)

Aqu√≠, la imagen es $\scriptstyle 3\times3$ y el filtro es $\scriptstyle 2\times2$.

En el ultimo ejemplo, la zancadilla de talla $\scriptstyle 1\times1$, la mas simple. Esto quiere decir que movemos el filtro una columna a la derecha hasta llegar al borde derecho de la imagen, y luego nos movemos una fila hacia abajo.

Suponiendo una zancadilla de talla $\scriptstyle 1\times1$, la cantidad de veces que podemos aplicar el filtro de dimensiones $\scriptstyle N\times N$ sobre una imagen $\scriptstyle M \times M$ es:

$$
\#\text{'s de aplicacion del filtro = } (M - N + 1)^2
$$

A veces, tambi√©n queremos un cierto espacio entre dos posiciones. de un filtro sobre una imagen m√°s grande. Es decir, una zancadilla m√°s grande que $1$.

![La imagen es de talla $\scriptstyle 7\times7$, el filtro de talla $\scriptstyle 3\times3$ y la zancada de talla $\scriptstyle 2\times2$.](old/Otras%20libretas%2038b07a4041b949a7aba5ec3a03d2931f/Brilliant%20375bdad652564477b33bcb24abdc6919/inform√°tica%202%20redes%20neuronales%20artificiales%200423ddd123fc4fac99c7432906f372dc/image%204.png)

La imagen es de talla $\scriptstyle 7\times7$, el filtro de talla $\scriptstyle 3\times3$ y la zancada de talla $\scriptstyle 2\times2$.

Las veces que podemos encajar un filtro sobre la direcci√≥n horizontal con una zancadilla horizontal de tama√±o $S_\text{horiz}$ ser√≠a $\lfloor \frac{W-N}{S_\text{horiz}} \rfloor + 1$, $W$ siendo la dimension horizontal de la imagen. Lo mismo con $H$ como dimensi√≥n vertical de la imagen con una zancadilla vertical $S_\text{vert}$. El resultado final ser√≠a el producto de ambos, o sea:

$$
\#\text{'s de aplicacion del filtro =}\left( \left \lfloor \frac{W-N}{S_\text{horiz}} \right \rfloor + 1 \right)  \times \left( \left \lfloor \frac{H-N}{S_\text{vert}} \right \rfloor + 1 \right)
$$

### Relleno (padding)

Por √∫ltimo, puede ser el caso que tengamos una parte de un objeto presente en la imagen, pero no el objeto entero. Por ejemplo, fij√©monos que esta cruz solamente est√° parcialmente presente en la imagen real.

![Filtro $\scriptstyle 3\times3$ de una cruz: los p√≠xeles blancos tienen un valor de $1$ y los p√≠xeles negros un valor de $0$.](24b3bfca-0a73-4faf-ad5b-c585453e4781.png)

Filtro $\scriptstyle 3\times3$ de una cruz: los p√≠xeles blancos tienen un valor de $1$ y los p√≠xeles negros un valor de $0$.

![Imagen $\scriptstyle 5\times5$ donde solo hay cruces parciales (falta siempre un p√≠xel de cada cruz).](d6a6b3aa-73f7-4324-a6c2-400277055fde.png)

Imagen $\scriptstyle 5\times5$ donde solo hay cruces parciales (falta siempre un p√≠xel de cada cruz).

Si intentamos pasar el filtro por la imagen, las activaciones no ser√≠an las mejores. Por activaciones, quiero decir donde el filtro ubica un p√≠xel negro donde lo hay en la imagen, y lo mismo para los p√≠xeles blancos.

Una forma de reducir esto es agregar un relleno a la imagen de la siguiente forma. El relleno $P$ es de tama√±o $1$.

![Imagen original, habiendo aplicado un padding de $1$; aumentando sus dimensiones de $\scriptstyle 5\times5$ a $\scriptstyle 7\times7$.](b3092634-f8fb-4cce-b99a-a35ec9480299.png)

Imagen original, habiendo aplicado un padding de $1$; aumentando sus dimensiones de $\scriptstyle 5\times5$ a $\scriptstyle 7\times7$.

Esto es mejor, ya que si ubicamos el filtro de cruz donde hay cruces parciales, solo nos equivocamos en la activaci√≥n de un p√≠xel en cada una de las tres cruces parciales, lo que es considerablemente mejor.

La formula para la cantidad de veces que podemos pasar un filtro $\scriptstyle N\times N$ sobre una imagen $\scriptstyle W\times H$, con zancadas $\scriptstyle S_\text{horiz} \times S_\text{vert}$, y relleno $P$ ser√≠a:

$$
\#\text{'s de aplicacion del filtro = }

\\[10pt]

\left( \left \lfloor \frac{W - N + 2P}{S_\text{horiz}} \right \rfloor + 1 \right)  \times \left( \left \lfloor \frac{H - N + 2P}{S_\text{vert}} \right \rfloor + 1 \right)
$$

## Dimensiones

### Dimensiones de entrada

Comenzamos con una imagen cuadrada de talla $\scriptstyle N \times N$.

Cada pixel de la imagen puede estar compuesto de una combinaci√≥n de colores, como por ejemplo RGB (Red, Green, Blue) o CYMK (Cyan, Yellow, Magenta, Key/Black). Esto agrega una tercera dimensi√≥n, y la talla de esta tercera dimensi√≥n seria la cantidad de colores considerados por p√≠xel en la imagen. Si consideramos CYMK, entonces las dimensiones de nuestra imagen pasan a ser $\scriptstyle 4 \times N \times N$.

En el caso general, el nombre de la dimensi√≥n que cuenta los colores considerados por cada p√≠xel recibe el nombre de ‚Äúcanales‚Äù, y la representaci√≥n general de cada imagen es $\scriptstyle C\times N \times N$.

Finalmente, puede ser el caso de que estemos evaluando mas de una imagen. Por ejemplo, puede ser que estemos evaluando un video de un perro, y queremos detectar que raza de perro es con una RNC. Un video, como sabemos bien, es una secuencia de im√°genes. Esto implica la introducci√≥n de una cuarta dimensi√≥n a nuestros datos, es decir $\scriptstyle I\times C\times N \times N$.

Las dimensiones $\scriptstyle I\times C\times N \times N$ representan entonces una secuencia de $I$ im√°genes, compuestas cada una de $C$ colores/canales, y de tama√±o $\scriptstyle N \times N$.

### Dimensiones de salida

Para cada imagen, le queremos aplicar varios filtros por cada paso convolucional, supongamos  $K$ filtros. Esto quiere decir que el resultado es una matriz con volumen $K$ y con largo y ancho descrito en la expresi√≥n anterior.

## Reducci√≥n de muestreo (pooling)

Por cuestiones de tiempo, queremos reducir el n√∫mero de pasos convolucionales que realizamos. Una manera de hacer esto, al mismo tiempo que mantenemos el poder de identificaci√≥n, es reducir la calidad de nuestra imagen, haci√©ndola m√°s peque√±a pero donde cada p√≠xel conserva informaci√≥n m√°s importante.

Notemos que una ventaja de esto es reducir el overfitting, porque estamos qued√°ndonos con los rasgos importantes.

La manera en como el pooling funciona es, b√°sicamente, tenemos una imagen de tama√±o $\scriptstyle N\times N$ y la queremos reducir a $\scriptstyle M\times M$, donde $M < N$. Vamos a tomar una muestra de la imagen, y que tambi√©n puede involucrar una zancadilla. Esto se aplica de la misma manera como cuando aplicamos un filtro.

El n√∫mero de veces que podemos aplicar la muestra es nuevamente la f√≥rmula anterior, con $P = 0$. Entonces:

$$
\#\text{'s de aplicacion de muestreo =}\left( \left \lfloor \frac{W-N}{S_\text{horiz}} \right \rfloor + 1 \right)  \times \left( \left \lfloor \frac{H-N}{S_\text{vert}} \right \rfloor + 1 \right)
$$

Luego, a esta muestra le vamos a aplicar una operaci√≥n y, por √∫ltimo, la vamos a reunir en una imagen final. Esta operaci√≥n puede ser una de dos:

- **Valor m√°ximo**: tomamos el valor m√°ximo de la matriz muestra y ese ser√° su representante en la imagen reducida.
- **Valor promedio**: mismo principio, pero con el promedio.

En la pr√°ctica, la dimension de la matriz muestra suele ser $\scriptstyle 2\times 2$ o $\scriptstyle 3\times 3$, y cada valor m√°ximo o valor promedio es mapeado sobre un solo p√≠xel en la imagen reducida. La zancada suele ser $2$. No queremos hacer m√°s grande la matriz muestra, puesto a que perder√≠amos rasgos importantes de la imagen.

![image(4).png](image(4).png)

Ejemplos de mapas de rasgos (feature maps) en la primera capa, con distintas im√°genes.

![image(5).png](image(5).png)

# 6. Redes neuronales recurrentes

---

## Motivaci√≥n

- Hasta ahora, hemos vistos dos tipos de RNA: las redes neuronales prealimentadas (feed-forward), y las redes neuronales convolucionales.
- Ambas de estas dos RNA est√°n limitadas por su propia arquitectura. En particular, en dos aspectos:
    - El input debe tener siempre una misma dimensi√≥n fija.
    - Los inputs son considerados independientes.
- Una manera de incorporar estos dos aspectos es con una **red neuronal recurrente**.

- La manera m√°s sencilla de incorporar secuencialidad y adaptaci√≥n de tama√±o es con la siguiente f√≥rmula:
    
    $$
    h_t = f(h_{t-1}, x_t)
    $$
    
    - $h_t$ : vector oculto en tiempo t
    - $x_t$ : input en tiempo t
    - $f$ : funci√≥n o transformaci√≥n arbitraria. Suele ser definida como un tipo de producto matricial.

- Sin embargo, el mejor modelo para incorporar los dos aspectos importantes suele ser de la siguiente forma:

Recurrencia para el pr√≥ximo vector oculto:
$h_t‚Äã=\tanh(W_{hh}‚Äã‚ãÖh_{t‚àí1‚Äã}+W_{hx}‚Äã‚ãÖx_t‚Äã).$

Output en cada tiempo t:
$y_t = W_{hy}‚ãÖh_t$
    - Mismas asignaciones para h, x y tanh.
    - $W_{ij}$ es una matriz de tama√±o ixj, que guarda los valores de los par√°metros an√°logos a pesos.
    - Suele tambi√©n sumarse un umbral B dentro del tanh de la primera f√≥rmula, pero se omite por simplicidad.

## Entrenamiento: retropropagaci√≥n temporal

$$
\frac{\delta L}{\delta \theta} = \sum_{t=1}^{N} \frac{\delta L_t}{\delta \theta}
$$

- $L$ es la p√©rdida total
- $L_t$ es la p√©rdida en tiempo t
- $\theta$ es el conjunto de par√°metros.
- N es un tiempo m√°ximo hasta el que estamos dispuestos a llegar para actualizar nuestro conjunto de par√°metros.

- **¬øPor qu√© truncamos el tiempo en vez de ir desde el primer t al √∫ltimo t?** Porque sufrimos el problema de gradientes que desaparecen o explotan si lleg√°semos a tomar cada t√©rmino desde el inicio del tiempo hasta el final del tiempo.

- Adem√°s, para cada $\frac{\delta L_t}{\delta \theta}$, la ecuaci√≥n es la siguiente regla de cadena:

$$
\sum_{i=1}^t \frac{\delta L_t}{\delta y_t} \frac{\delta y_t}{\delta h_t} \frac{\delta h_t}{\delta h_i} \frac{\delta h_i}{\delta \theta}
$$

- Derivamos finalmente una f√≥rmula para $\frac{\delta h_t}{\delta h_k}$, y aqu√≠ vemos de manifesto el problema de los gradientes: es un producto de razones, que eventualmente convergen a 0 o divergen al infinito. En la pr√°ctica, esto no le permite crear dependencias de largo plazo.

$$
\frac{\delta h_t}{\delta h_k} = \prod_{i=k+1}^t \frac{\delta h_i}{\delta h_{i-1}}
$$

- Ejemplo: una secuencia de inputs apta para la RNR ser√≠a ‚Äúsoy morena, por lo que el color de mi pelo es‚Ä¶‚Äù, pues sabr√≠a completar con ‚Äúmarr√≥n‚Äù.

Pero si le damos ‚Äúsoy morena, **p√°rrafo largo de por medio*,* por lo que el color de mi pelo es‚Ä¶‚Äù, la RNR tendr√° m√°s problemas para responder con ‚Äúmarr√≥n‚Äù.

## Soluci√≥n: memoria larga de corto plazo (LSTM)

- Una RNR de LSTM est√° compuesta con los mismos componentes de la RNR est√°ndar, pero tambi√©n a√±ade otros componentes:
    - Una c√©lula, la cual recuerda valores por cantidades de tiempo arbitrarias, lo que la hace buena para capturar dependencias de largo plazo. **Conserva sus dimensiones a lo largo de la secuencia de inputs**.
    - Una puerta de input, de output y de olvido. Estas controlan el flujo de informaci√≥n desde y hacia la c√©lula. Todas estas puertas son RNAs.
    - **Para tareas basadas en tiempo, son RNRs muy √∫tiles**.

- La operaci√≥n de una LSTM es la siguiente:
    - Inicializa la c√©lula $c_0 = 0$ y el vector oculto $h_0 = 0$.
    - Cada output $h_t$ es generado de la c√©lula $c_t$.
    - Ahora s√≠:
        - Para un tiempo determinado t, concatenamos el input en tiempo t, $x_t$; y el output pasado , $h_{t-1}$, en una sola matriz, $[h_{t-1}, x_t]$.
        - Luego, aplicamos la siguiente f√≥rmula: $C_t = f_t \circ c_{t-1} + i_t \circ \overline{C_t}$ . Cada multiplicaci√≥n es un producto de Hamadard.
        - Por √∫ltimo, el output correspondiente a la c√©lula $C_t$  ser√≠a igual a $h_t = o_t \circ \tanh(C_t)$ .
        - Para la siguiente recursi√≥n (determinaci√≥n de $C_{t+1}$ y $h_{t+1}$, utilizamos el C y h del tiempo pasado, t.
        - La puerta de olvido (f), de input (i) y de output (o) tienen todas la misma forma: la forma del output de una neurona dado un input, que ser√≠a la matriz horizontal. Particularmente: $z_t = \sigma ( W_z \cdot [h_{t-1}, x_t] + b_z)$. Cambia z por la letra que necesites. Cada peso W y umbral b son diferentes por cada puerta.
        - La puerta de olvido tiene valores de 0 a 1, donde el valor en la posiciones del 0 ser√°n mayormente olvidados y mayormente recordados si est√°n en la posiciones del 1.
        - La puerta de input es casi lo mismo, tan solo es diferente para luego interactuar con $\overline{C_t}$, la cual permite crear nuevas adiciones a la c√©lula.
        - $\overline{C_t}$ es igual = $\tanh ( W_{\overline{C}} \cdot [h_{t-1}, x_t] + b_{\overline{C}})$. A la base, es otra red neuronal.

# 7. Otras arquitecturas

---

## Redes neuronales estoc√°sticas

- Hasta ahora solo hemos estudiado RNAs determin√≠sticas: siempre devolver√°n el mismo output, dado el mismo input. Las redes neuronales estoc√°sticas son m√°s expresivas que esto.
- Un proceso estoc√°stico es un proceso que no est√° solamente determinado por las condiciones iniciales o el estado inicial, sino que hay aleatoriedad involucrada.
- Todo el punto de la RNE es capturar relaciones significativas entre los inputs visibles, al igual que una RNA.
    
    ![Untitled](old/Otras%20libretas%2038b07a4041b949a7aba5ec3a03d2931f/Brilliant%20375bdad652564477b33bcb24abdc6919/inform√°tica%202%20redes%20neuronales%20artificiales%200423ddd123fc4fac99c7432906f372dc/Untitled.png)
    

- La red neuronal estoc√°stica es formalmente llamada la *m√°quina de Boltzmann restringida*.

Se basa en una red neuronal prealimentada, pero su cambio es la decisi√≥n de encender una neurona basada en un vector de inputs v. Espec√≠ficamente, es la siguiente f√≥rmula, donde m es la cantidad de neuronas visibles y h es la neurona oculta en posici√≥n j. Tambi√©n, los inputs est√°n coleccionados en la matriz v y los pesos en la matriz W, donde $W_{ij}$ es el peso de la neurona en posici√≥n i a la posici√≥n j.

**Cabe destacar que todas las neuronas son perceptrones: solo toman valores 0 o 1.**

$$
P(h_j = 1|v) = \sigma(\sum_{i=1}^m W_{ij} \cdot v_i + b_j)
$$

- Se puede calcular la probabilidad en la direcci√≥n contraria: la probabilidad de que la neurona input v_i se encienda, dada la matriz de los valores de las neuronas oculta h. a_i es el umbral de la neurona v_i.
    
    $$
    P(v_i = 1|h) = \sigma(\sum_{i=1}^m W_{ij} \cdot h_j + a_i)
    $$
    
- El proceso de entrenamiento de una RNE es diferente.
    1. Utilizamos $v_0$ para construir las probabilidades de que las neuronas ocultas $h_j$ se enciendan. Tomamos una muestra de $h$ a partir de esta distribuci√≥n de probabilidades lo que ser√≠a $h_0$.
    2. Luego, con $h_0$ intentamos reconstruir el vector de inputs original, intento que se llamar√° $v_1$. Lo reconstruimos al crear una distribuci√≥n de probabilidad de cada neurona encendi√©ndose o no, y similarmente tomamos una muestra.
    3. Creamos otro conjunto de estados ocultos $h_1$ con $v_1$. As√≠ sucesivamente. Este proceso se llama el muestreo de Gibbs.
    
    ![Untitled](old/Otras%20libretas%2038b07a4041b949a7aba5ec3a03d2931f/Brilliant%20375bdad652564477b33bcb24abdc6919/inform√°tica%202%20redes%20neuronales%20artificiales%200423ddd123fc4fac99c7432906f372dc/Untitled%201.png)
    

- Para actualizar par√°metros (pesos y umbrales), lo hacemos de la siguiente manera: primero estimamos los gradientes (no son exactos debido a la estocasticidad), luego actualizamos cada matriz de par√°metros. $\epsilon$ es la tasa de aprendizaje.

$$
\Delta W = v_{(0)}h_{(0)}^T - v_{(1)}h_{(1)}^T
\newline
\Delta a = v_{(0)} - v_{(1)} 
\newline 
\Delta b = h_{(0)} - h_{(1)} 
$$

$$
W \Leftarrow W + \epsilon\Delta W
\newline
a \Leftarrow a + \epsilon\Delta a
\newline
b \Leftarrow b + \epsilon\Delta b
$$

- La intuici√≥n del aprendizaje de una RNE es que ajuste sus par√°metros tal que determine la probabilidad de observar un vector de inputs v de los datos observados (muestras de ejemplo).
- [https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine](https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine)

## Redes generadoras antag√≥nicas

- Este modelo es extremadamente eficiente al momento de "rellenar" la informaci√≥n faltante, por lo que podr√≠a tener una infinidad de aplicaciones fuera de la video anal√≠tica.
- B√°sicamente, creamos dos redes neuronales: la generadora y la discriminadora, y las ponemos a competir en un juego de suma cero; es decir, la p√©rdida de una red es la ausencia de p√©rdida de la otra.
- Para la red generadora:
    1. Vamos a tomar muestras alatorias para cada input de manera normal, lo que termina creando una distribuci√≥n multinomial gaussiana para cada combinaci√≥n de inputs.
    2. Luego, creamos un resultado a partir de los inputs.
    3. Intuitivamente, la funci√≥n de p√©rdida de esta red es cu√°n lejos de un resultado real√≠stico es. Sin embargo, necesitamos entrenar una funci√≥n para saber evaluar cu√°n real√≠stico el resultado es.
    
- Para la red discriminadora:
    1. Sus inputs van a ser la unidad que estamos estudiando (im√°genes, videos, etc.), y su output debe ser la probabilidad de que el input sea del mundo real y no generado.
    Esto implica que D(x) va de 0 a 1, porque es una probabilidad.
    2. La funci√≥n de p√©rdida ser√° entonces definida como $-\log (D(x))$.
    Notemos, para probabilidades de que el input sea real, D(x) estar√° cerca de 1, por lo que la p√©rdida estar√° cerca de 0.
    
- Con esto hecho, la funci√≥n de p√©rdida de la red generadora ser√° $\log(1-D(G(z))$. z es el input de la red generadora, que es tomado aleatoriamente input por input. (De hecho me sali√≥ mejor con log(D(G(z))).

### Peque√±a mejora: InfoGAN

- InfoGAN es una variaci√≥n de la RGA est√°ndar.
    1. Crea una variable c que captura la ‚Äúinformaci√≥n importante de la estructura‚Äù del input.
    2. Redefinimos la generaci√≥n como G(z, c), donde ahora la generaci√≥n tendr√° m√°s en cuenta la estructura esencial del objeto de estudio.
    3. Finalmente, para mejor entrenar la generaci√≥n, crea una funci√≥n I(Input, c), la cual devuelve valores m√°s grandes seg√∫n la cantidad de informaci√≥n de c presente en el input. La nueva funci√≥n de p√©rdida ser√≠a $\log(1-D(G(z)) - I(G(z,c), c)$.

## Auto-encodador variacional

- Un autoencodador es una funci√≥n que toma un vector en dimensiones n y lo mapea a un vector en dimensiones m, donde m < n. B√°sicamente, la encodaci√≥n reduce la dimensi√≥n del vector. 

No solo esto, sino que lo hace intentando mantener la mayor informaci√≥n posible del vector original, **para finalmente reconstruir el vector original de dim. n, que es el proceso de decodaci√≥n.**

- El recorrido de la funci√≥n autoencodaci√≥n es el espacio latente, el espacio donde se puede mapear los vectores de mayor dimensi√≥n. Hay que tener en cuenta que no todo el espacio latente tiene una asignaci√≥n de la encodaci√≥n (la encodaci√≥n no es superyectiva).

- Queremos hacer este proceso de decodaci√≥n una funci√≥n generadora. Es decir, queremos tener un universo de donde tomemos muestras que nos permitan generar un vector de dimensiones originales ‚Äúreal√≠stico‚Äù.

Para ello, necesitamos restrigir nuestro espacio de muestreo a solamente las regiones de las m dimensiones donde se mapean los vectores de n dimensiones originales.

- Esta √∫ltima tarea ‚Äî encontrar la regi√≥n donde se mapean los vectores originales ‚Äî requiere saber la distribuci√≥n real de los datos.

- Un auto-encodador variacional hace entonces lo siguiente:
    1. No mapea un valor real a un valor espec√≠fico en el espacio latente, sino que mapea V a una distribuci√≥n de vectores que, al ser reconstruidos, su resultado est√° cerca del vector original.
    
    Esta distribuci√≥n de vectores dim. m es definida como multinomial gaussiana, con media $\mu$ y varianza $\sigma^2$. La encodaci√≥n es entonces una funci√≥n que devuelve $\mu$ y $\sigma^2$.  Inicializamos $\mu$ y $\sigma^2$, en el entrenamiento los afinaremos.
    
    2. Luego, para crear finalmente una encodaci√≥n del vector V, tomamos una muestra de una multivariable normal est√°ndar, la multiplicamos por $\sigma$ y le sumamos  $\mu$. Todo esto lo hacemos para tomar una muestra de la multinomial restante, lo que ser√° finalmente la encodaci√≥n W de nuestro vector original V. La distribuci√≥n de la encodaci√≥n W, dado el input V, es notada $q_W(V)$.
    
    3. Ahora, la decodadora va a tomar la encodaci√≥n W para intentar reconstruir el vector original V.
        
        ![Untitled](old/Otras%20libretas%2038b07a4041b949a7aba5ec3a03d2931f/Brilliant%20375bdad652564477b33bcb24abdc6919/inform√°tica%202%20redes%20neuronales%20artificiales%200423ddd123fc4fac99c7432906f372dc/Untitled%202.png)
        
    4. Luego, introducimos un t√©rmino de **p√©rdida de latencia**, que indica cu√°nto de la distribuci√≥n encontrada en la encodaci√≥n, $(\mu, \sigma^2)$, diverge de la distribuci√≥n multinomial gaussiana est√°ndar $(0, 1)$. Esto se mide de la siguiente manera, donde la mano izquierda se conoce como la divergencia de Kullback-Leibler.
        
        $$
        D_{KL}(q_W(V)|| \space p_W)=
        \int_{- \infin}^\infin q_W(V)\log(\frac{q_W(V)}{p_W}) \space dx \newline
        $$
        
    5. Finalmente, buscamos $\mu$ y $\sigma^2$ que minimicen la funci√≥n de p√©rdida. De esta manera, nos acercaremos m√°s y m√°s en cada retropropagaci√≥n a la verdadera distribuci√≥n de la encodaci√≥n, $p_W$.
    

## Word2Vec

- Imaginemos que queremos crear una red neuronal que trate las palabras de un idioma. No es necesario comentar la enorm√≠sima cantidad de palabras que hay, lo mismo si decidimos representar cada palabra con un vector ‚Äúone-hot‚Äù, es decir, con una entrada igual a 1 y el resto 0.

Por ello, querr√≠amos aplicar una reduci√≥n dimensi√≥n, como en el auto-encodador.
- Nos gustar√≠a capturar la relaci√≥n entre ciertas palabras con vectores, como por ejemplo:  $bueno = -malo$, o tambi√©n $l_{reina} - l_{mujer} = l_{monarca}$, donde $l$ es la representaci√≥n vectorial de una palabra.
- La primera capa ser√≠a la transformaci√≥n de la palabra, la cu√°l ser√≠a una matriz de dimensiones MxN, donde M es la cantidad de palabras en el idioma, y N la dimensi√≥n del espacio donde nos gustar√≠a mapear nuestras palabras. La encodaci√≥n de la palabra de √≠ndice i estar√≠a en la fila i de la matriz M.
- La segunda capa ser√≠a la predicci√≥n del contexto de las palabras. Definimos el contexto, en este caso, como el conjunto de las palabras alrededor de una palabra. Por ejemplo, el contexto de la palabra sandwich, una palabra hacia atr√°s y dos hacia adelante, en la frase ‚Äú*le gustaba cenar un exquisito s√°ndwich de jam√≥n con zumo de pi√±a y vodka fr√≠a*‚Äù ser√≠a {exquisito, de, jam√≥n}.

Existen dos tipos de modelos de RNAs para incluir el contexto:

1. B**olsa continua de palabras (CBOW)**: es una red neuronal que tiene como output si una palabra pertecene a un contexto o no (clasificaci√≥n). Su input es una cierta palabra (vector one-hot) y el promedio de los vectores que representan el contexto.

2. **Skip-gram**: su input es una palabra (vector one-hot) y su output es el vector con nuestro vocabulario, teniendo una probabilidad en cada entrada de palabra. Esta probabilidad es aquella de que tal palabra x est√© cerca de nuestra palabra input en un texto.

- **Muestreo negativo**: supongamos que hablamos del modelo CBOW. Para entrenar una red neuronal, le mostramos ejemplos de clasificaci√≥n correcta. El muestro positivo es ense√±arle ejemplos de clasificaciones correctas, donde tales ejemplos son ya comprobados por humanos.

El muestro negativo es simplemente ejemplos de clasificaciones incorrectas, pero no necesitamos una comprobaci√≥n humana, nada m√°s es suficiente que tomar un contexto y una palabra aleatoria de nuestro vocabulario, la cual casi seguramente no tendr√° sentido en el contexto, y la clasificamos como incorrecta.

Fijando un vector contexto (el promedio de los vectores de las palabras del contexto), supongamos que estamos en la fase de entrenamiento de clasificaci√≥n, con un cierto dataset que vamos a alimentar a la red. Por cada ejemplo de clasificaci√≥n correcta de nuestro dataset, queremos una cierta cantidad de clasificaci√≥n incorrecta, ¬°pero no para todas las palabras incorrectas en el contexto! Esto va a ayudar a mejorar nuestros tiempos de entrenamiento.

Si nuestro sub-dataset de ejemplos correctos es peque√±o, queremos de 5 a 20 ejemplos de clasificaci√≥n incorrecta por cada ejemplo de clasificaci√≥n correcta. Si nuestra cantidad de ejemplos correctos es grande, con 2 a 5 ejemplos incorrectos por cada ejemplo correcto es suficiente. Esto es pare

## Aprendizaje reforzado

- El aprendizaje reforzado es un proceso de decisi√≥n de Markov que, a su vez, es una modificaci√≥n de una cadena de Markov.
- **Cadena de Markov**: es un modelo estoc√°stico donde la probabilidad de alcanzar un estado en un evento futuro t+1 depende solamente del estado donde se estaba en el evento pasado t (esta es su caracter√≠stica m√°s importante: p√©rdida de memoria). 

Formalmente, es una 2-tupla, cuyos componentes son:
    - Un conjunto de estados: $S = \{s_1, s_2, ‚Ä¶ , s_n\}$.
    - Un conjunto de probabilidades de transici√≥n de estado i a estado j,
     $T = \{P_{0,0}, P_{0,1}, ‚Ä¶, P_{i,j}\}$.
        
        ![Untitled](informa%CC%81tica%202%20redes%20neuronales%20artificiales%200423ddd123fc4fac99c7432906f372dc/Untitled%203.png)
        

- **Proceso de decisi√≥n de Markov**: es una modificaci√≥n de la cadena de Markov. 
Formalmente, es una 4-tupla, cuyos componentes son:
    - Un conjunto de estados, llamado ‚Äúespacio de estados‚Äù y denotado $S$.
    - Un conjunto de acciones, llamado ‚Äúespacio de acciones‚Äù y denotado $A$.
    - Un conjunto de probabilidades de transici√≥n de un estado a otro al haber tomado una cierta acci√≥n, denotado $T$.
    - Una funci√≥n de recompensa por haber tomado una acci√≥n en cierto estado,
    $R:S \times A \rightarrow \R$
    - No necesariamente, pero aqu√≠ podemos incluir una funci√≥n ‚Äúpol√≠tica‚Äù que mapea un estado a una acci√≥n. Esto ser√° √∫til para hablar del aprendizaje por refuerzo, pues aqu√≠ vemos las estrategias.
    
    Esta pol√≠tica puede ser determin√≠stica o probabil√≠stica. Si es determin√≠stica:
     $\pi: S \rightarrow A$
    
    De lo contrario, si es probabil√≠stica: notemos que podemos hacerla determin√≠stica con probabilidades absolutas de 0 y 1.
    $\pi:S \times A \rightarrow [0,1]$
    
    ![Untitled](informa%CC%81tica%202%20redes%20neuronales%20artificiales%200423ddd123fc4fac99c7432906f372dc/Untitled%204.png)
    
    - Finalmente, el **aprendizaje reforzado** es un procedo de decisi√≥n de Markov donde buscamos que el agente maximice el valor esperado de su recompensa ajustando su funci√≥n de pol√≠tica.
    
    Notemos que esta funci√≥n de recompensa se va acumulado con el tiempo. Queremos que el agente maximice su valor de recompensa al largo plazo y no en el corto plazo. Por lo tanto, el agente puede tomar decisiones de recompensa sub-√≥ptima en el presente si el considera que puede alcanzar la recompensa √≥ptima en el largo plazo.
    
    Al haber inicializado las probabilidades de acciones, se hacen muchas simulaciones, para que la red pueda explorar todas las combinaciones de recompensas disponibles. Finalmente, aplicamos ascenso por gradiente para maximizar la recompensa final de los juegos simulados.
    
    El agente tambi√©n aprende la noci√≥n de ‚Äúarrepentimiento‚Äù cuando observa la diferencia de su recompensa con la recompensa de un hipot√©tico agente √≥ptimo.
    
    Todo esto implica que el aprendizaje reforzado es un proceso bien apto para problemas donde hay disputas de largo plazo vs. corto plazo.
    - AlphaGo es una red neuronal que aplica aprendizaje por refuerzo. Notemos que para las redes neuronales, remplazamos la funci√≥n de p√©rdida con la funci√≥n de recompensa y procedemos a hacer ascenso por gradiente, en vez de descenso. Las diferencias con AlphaGo son:
        - Se entrena un modelo experto (no necesariamente una red neuronal) y buscamos que nuestra red original imite al experto a trav√©s de modificar nuestras probabilidades. Llegamos a un cierto conjunto de probabilidades que son aquellas del modelo experto.
        - Durante la simulaci√≥n, la red juega contra una iteraci√≥n previa de si misma, para evitar overfitting y quedarse estancados en locales m√°ximos de la funci√≥n de recompensa y posiblemente llegar al m√°ximo global.
        - Se entrena un modelo de ‚Äúvalor‚Äù Este modelo toma como input un estado y devuelve el valor esperado desde ese estado en el futuro y siguiendo esa misma pol√≠tica. Formalmente,
        
        $$
        V^{\pi}(s)=E[R\space|\space s,\pi]
        $$
        
        - Finalmente, cuando AlphaGo debe tomar una decisi√≥n, realiza un √°rbol de b√∫squeda de Monte Carlo con una gran cantidad de simulaciones. Es una red con nodos y conexiones. Cada nodo representa un estado y contiene una valuaci√≥n, mientras que cada conexi√≥n es la probabilidad de transici√≥n de ese estado a otro, y contiene una valuaci√≥n inicializada a 0.
        
        En cada simulaci√≥n, se empieza del nodo ra√≠z y se sigue le conexi√≥n con el valor m√°s alto de (probabilidad + valor de la acci√≥n), con una penalizaci√≥n por cada n√∫mero de veces que se ha pasado por la conexi√≥n en simulaciones pasadas (con el objetivo de incentivar la exploraci√≥n del √°rbol). Trazamos conexiones hasta llegar al final del juego.
        
        Se actualizan la valuaci√≥n de las acciones con el promedio de las valuaciones de los nodos que estaban debajo de esa conexi√≥n en esa simulaci√≥n. Volvemos a hacer otra simulaci√≥n. As√≠ sucesivamente, con la serie de simulaciones fijada a un l√≠mite de tiempo.
        
        La decisi√≥n final que toma AlphaGo es la acci√≥n que le conecta al nodo que fue m√°s visitado durante la serie de simulaciones.