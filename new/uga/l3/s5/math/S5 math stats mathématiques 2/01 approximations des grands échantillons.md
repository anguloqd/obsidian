# 01 // approximations des grands √©chantillons

Date de cr√©ation: July 16, 2024 11:45 PM
Modifi√©: July 16, 2024 11:45 PM

# Convergence, grands √©chantillons, approx. gaussiennes

## In√©galit√©s : Markov et Bienaym√©-Tchebychev

### Markov

L‚Äôin√©galit√© de Markov est une majoration d‚Äôune V.A. r√©elle (y incluse V.A enti√®re). Soit¬†$*X*$une variable al√©atoire r√©elle, suppos√©e¬†presque s√ªrement positive ou nulle, et avec une esp√©rance d√©finie. La premi√®re formulation est la plus commune. Si $a>0$ :

$$
\space \mathbb{P}(X \ge a) \le \frac{\mathbb{E}[X]}{a} \text{\hspace{8pt}ou\hspace{8pt}} \mathbb P(X\ge\varepsilon\mathbb E[X])\le\frac{1}{\varepsilon}
$$

Dans le mat√©riel du cours, on l‚Äôa vue de mani√®re un peu diff√©rente, et introduisant une fonction $f$ tel que l‚Äôesp√©rance de $f(|X|)$ et $f(a)$ sont aussi bien d√©finies :

$$
\mathbb{P}(|X| \ge a) \le \frac{\mathbb{E}[f(|X|)]}{f(a)}
$$

### Bienaym√©-Tchebychev

Celle-ci est juste un cons√©quence de l‚Äôin√©galit√© de Markov, sous condition que $X$ ait une variance. Prenons la version du cours de l‚Äôin√©galit√© de Markov, fixons une autre V.A. r√©elle $Y = (X - \mathbb{E}[X])$ et $f(x) = x^2$. Notons que le $(X-\mathbb{E}[X])$ est mis au carr√© √† droite de l‚Äôin√©galit√© mais pas √† gauche. Donc : 

$$
\begin{align*}
\mathbb{P}(|Y| \ge a) \le \frac{\mathbb{E}[|Y|^2]}{a^2}

&\iff

\mathbb{P}(|X-\mathbb{E}[X]| \ge a) \le \frac{\mathbb{E}[\left(|X-\mathbb{E}[X]|\right)^2]}{a^2}

\\ &\iff
\mathbb{P}(|X-\mathbb{E}[X]| \ge a) \le \frac{\text{Var}(X)}{a^2}
\end{align*}
$$

Par contre, la version suivante est plus intuitive, o√π $a > 1$. Elle s‚Äôinterpr√®te comme la probabilit√© que la valeur de $X$ soit √† $a$ √©cart-types ou plus de la moyenne est plus petit que $1/a^2$. Encore plus intuitive, la probabilit√© que la valeur de $X$ soit √©loign√©e de la moyenne devient de plus en plus petite. Il convient imaginer une loi normale sur $X$, par exemple.

$$
\mathbb{P}(|X - \mathbb{E}[X]| \ge a\sigma) \le \frac{1}{a^2} \iff \mathbb{P}(|X - \mathbb{E}[X]| \le a\sigma) \ge 1- \frac{1}{a^2}
$$

## Diff√©rent types de convergence

Pour toutes les types de convergence qui suit, on parlera d‚Äôune suite $(X_n)$ de variables al√©atoires.

### Convergence en loi vers $X$

C‚Äôest le cas si la suite des r√©partitions de $X$, $\big(F_{X_n}\big)$, converge √† une fonction existante, appel√©e $F_{X_\infin}$ ou tout simplement $F_X$. Notons que les propositions avec les fonctions g√©n√©ratrice $\mathcal{G}_{X_n}$ et $\mathbb E[f(X_n)]$ sont analogiques.

$$
\begin{array}{lc}
\lim_{n\rightarrow\infin} F_{X_n}(x)=F_X(x) &\iff \\[3pt] \lim_{n\rightarrow\infin} \mathcal G_{X_n}(x)=\mathcal G_X(x) &\iff\\[4pt] \lim_{n\rightarrow\infin} \mathbb E[f(X_n)]=\mathbb E[f(X)]
\end{array}
$$

### Convergence en probabilit√© vers $X$

C‚Äôest le cas si la diff√©rence entre $X_n$ et $X$ devient arbitrairement petite le plus $n$ se rapproche de l‚Äôinfini (1√®re proposition).

$$
\forall\varepsilon >0, \lim_{n\rightarrow\infin} \mathbb P(|X_n-X| < \varepsilon)=1 \iff \lim_{n\rightarrow\infin}\mathbb P(|X_n-X|>\varepsilon)=0
$$

La diff√©rence avec la convergence en loi est que, pour la conv. en loi il se peut que la proba de la diff√©rence entre suite-limite ne soit pas plus petite qu‚Äôun epsilon particulier. 

### Convergence en moyenne d‚Äôordre $p$ vers $X$

C‚Äôest le cas si le moment d‚Äôordre $p$ de $X_n$ existe pour tout $n$, et le moment centr√© d‚Äôordre $p$ devient $0$ √† la limite.

$$
\mathbb E[X_n^p] \in \R\text{\hspace{6pt} et } \lim _{n\rightarrow \infin} \mathbb E[|X_n-X|^p]=0
$$

### Convergence *presque-s√ªrement* vers $X$

$$
\begin{array}{lc}
\mathbb P\left(\lim _{n\rightarrow \infin} X_n=X\right) = 1 &\iff\\[4pt]
\mathbb P \left( \sup_{m\ge n}|X_m-X|<\varepsilon\right)=1 &\iff\\[6pt]
\sum_{n=1}^\infin \mathbb P (|X_n-X|>\varepsilon) < \infin
\end{array}
$$

La premi√®re affirmation est la pr√©sentation la plus populaire. Celle-ci signifie que les valeurs de $X_n$ se rapprochent de la valeur de $X$, au sens o√π les √©v√©nements pour lesquels $X_n$ ne converge pas √† $X$ ont probabilit√© $0$. C‚Äôest la convergence stochastique la plus similaire √† [convergence simple ou ponctuelle d‚ÄôAnalyse R√©elle](https://en.wikipedia.org/wiki/Pointwise_convergence). √Ä savoir :

$$
\lim f_n=f \iff \lim f_n(x)=f(x) \text{ pour tout }x\text{ dans le domaine de }f.
$$

Deux th√©or√®mes sur cette convergence sont √† noter :

- **Th√©or√®me de Slutsky**.
Si $X_n ‚Üí X$ presque s√ªrement et $g$ est une fonction continue sur $\R$, alors $g(X_n) ‚Üí g(X)$ presque s√ªrement.
- **Th√©or√®me de Slutsky ‚Äúen deux dimensions/variables‚Äù**.
Si $X_n ‚Üí X$ presque s√ªrement et $Y_n ‚Üí Y$ presque s√ªrement et $g$ est une fonction continue sur $\R^2$ alors $g(X_n, Y_n) ‚Üí g(X, Y)$ presque s√ªrement.

### Relations entre les convergences

Ces quatre d√©finitions de convergence ont une relation d‚Äôimplication utile et int√©ressante :

- Convergence en probabilit√© $\implies$ Convergence en loi
- Convergence en moyenne d‚Äôordre $p$ $\implies$ Convergence en probabilit√©
- Convergence presque-s√ªre $\implies$ Convergence en probabilit√©

$$
\begin{array}{lc}
&\text{Conv. presque-s√ªre} \Rightarrow\hspace{-6pt} &\text{Conv. en proba.}\hspace{-6pt} &\Leftarrow \text{Conv. } p \\
&&\Downarrow&\\
&&\text{Conv. en loi}&
\end{array}
$$

En outre, la convergence en probabilit√© est √©quivalente √† la convergence en loi dans le cas de la convergence vers une constante. Notons aussi que, dans le cas g√©n√©ral, il n‚Äôy a pas, entre convergence en m.q. et convergence p.s., de domination de l‚Äôune sur l‚Äôautre.

# Loi des grands nombres (faible et forte)

## Pr√©paration et version faible

Pour montrer les deux versions de la loi des grands nombres, on aura besoin de ce qui suit :

1. On commence avec une suite de VAs iid $(X_i)_{i\le n}=\{X_1, \dots, X_n\}$ .
2. On d√©finit sa moyenne empirique comme $\bar X_n=\frac{1}{n} \sum _{i=1}^n X_i$.
3. Finalement, on suppose que l‚Äôesp√©rance de $X_i$ existe, donc $\mathbb E[X_i]$ existe, pour un $i$ n‚Äôimporte quel.

La loi faible dit que la moyenne arithm√©tique d‚Äôune suite de V.A. iid **converge en loi** vers l‚Äôesp√©rance de ces VA. Plus facilement, la moyenne empirique tend vers la moyenne th√©orique si les $n$ essaies tend vers l‚Äôinfini.

$$
\forall \varepsilon >0,\lim_{n \rightarrow \infin} \mathbb{P} \left(\left|\bar{X}_n -\mathbb{E}[X_i] \right| \ge \epsilon \right) = 0
$$

Notons, on fixe $\varepsilon$ en premier lieu (√† une valeur n‚Äôimporte quelle, mais fix√©e), puis un calcule la diff√©rence entre la moyenne et l‚Äôesp√©rance. **Cet ordre est vraiment important** pour comprendre l‚Äô√©quation pr√©c√©dente comme une proposition.

Parfois, on trouve plut√¥t la diff√©rence avec le signe que la diff√©rence en valeur absolue, c√†d $(\bar{X}_n -\mathbb{E}[X_i])$. Cela dit, on peut facilement voir qu‚Äôon arrive on m√™me r√©sultat. Notons que le fait que toute l‚Äôexpression est dans valeur absolue signifie que cette limite est majorant de la $\mathbb{P}$ de la grande expression sans valeur absolue.

$$
0 \le \lim_{n \rightarrow \infin} \mathbb{P} \left(\bar{X}_n -\mathbb{E}[X_i]\ge \epsilon\right), \text{ par d√©finition des probabilit√©s.}

\\[06pt]

\text{En plus, }\lim_{n \rightarrow \infin} \mathbb{P} \left(\bar{X}_n -\mathbb{E}[X_i] \ge \epsilon \right)  \le \underbrace{\lim_{n \rightarrow \infin} \mathbb{P} \left(\left|\bar{X}_n -\mathbb{E}[X_i] \right| \ge \epsilon \right)}_0

\\[02pt]

\text{Finalement, } 0 \le \underbrace{\lim_{n \rightarrow \infin} \mathbb{P} \left(\bar{X}_n -\mathbb{E}[X_i]\ge \epsilon\right)}_0 \le 0.
$$

## Version forte et remarques

On reprend la m√™me pr√©paration pour cette version. La loi forte affirme que la moyenne empirique **converge presque s√ªrement** √† l‚Äôesp√©rance.

$$
\mathbb P \left(\lim_{n\rightarrow \infin} \bar X_n = \mathbb E[X] \right)=1
$$

La loi des grands nombres n‚Äôa pas d‚Äôint√©r√™t pratique pour le calcul statistique, contrairement au th√©or√®me central limite qui vient pr√©ciser la fa√ßon dont la moyenne empirique $\bar X_n$ converge vers $\mathbb E[X]$. Ce th√©or√®me est √† la base de nombreuses propri√©t√©s essentielles des √©chantillons en statistique.

# Th√©or√®me central de la limite

## ‚ÄúUne somme de VAs converge √† une courbe normale‚Äù

### Motivation et d√©rivation

La loi des grands nombres √©nonce qu‚Äôune somme de V.A. iid se rapproche de son esp√©rance quand le nombre de V.A. tend vers l‚Äôinfini, mais ce th√©or√®me ne nous dit pas √† quelle "vitesse". C‚Äôest ici o√π le th√©or√®me central de la limite est utile, car il nous permet de parler sur cette vitesse.

Soit $\{X_1, \dots, X_n\}$ une suite des V.A. telles que $\mathbb{E}[X_i]=0$, $\text{Var}(X_i)=1$ et $\bar{X}_n$ leur moyenne empirique. D‚Äôapr√®s la loi des grands nombres, la distance entre $\bar{X}_n$ et $\mathbb{E}[X]$ tend vers $0$.

On sait aussi que la variance de $\bar{X}_n$ est $\frac{\sigma^2}{n}$. Donc, notons que la variance tend vers $0$ aussi si $n$ tend vers l‚Äôinfini. Ceci est probl√©matique, car la variance par d√©finition est plus grand que $0$ mais on veut aussi qu‚Äôelle soit finie ! 

On peut penser alors √† modifier la statistique $\bar{X}_n$ pour que sa variance dans ce cas soit plus grand que $0$. On arrive √† la modification suivante : on cr√©e une suite $(a_n)$ et une autre suite $(a_n\bar{X}_n)$, ce qui nous donne la variance suivante :

$$
\text{Var}(a_n\bar{X}_n) = \frac{a_n^2}{n}\cdot\text{Var}(X_i)=\frac{a_n^2}{n}
$$

Si $a_n = \sqrt n$, la variance de la modification serait √©gale √† $1$. Donc, on le d√©finit comme cela.

### Finalement, le th√©or√®me

Il en r√©sulte du **th√©or√®me de continuit√© de L√©vy** que la variable $\sqrt{n}(\bar{X}_n-\mathbb{E}[X_i])$ converge en loi vers $\mathcal{N}(0,\sigma^2)$. Finalement, on peut √©noncer le th√©or√®me centrale de la limite :

$$
\lim_{n \longrightarrow \infin} \mathbb{P}(\sqrt{n}(\bar{X}_n-\mathbb{E}[X])\in[a,b])=\frac{1}{\sigma\sqrt{2\pi}}\int_a^be^{-\frac{x^2}{2\sigma^2}}\space dx
$$

Notons qu‚Äôon peut diviser les termes de la suite $(a_n)$ par $\sigma$ pour que la variance soit r√©duite (c√†d. √©gale √† $1$) et distribution finale soit normalis√©e, c√†d. $\mathcal{N}(0,1)$.

Le th√©or√®me de la limite centrale assume que deux conditions essentielles soient v√©rifi√©es :

- Les variables $X_i$ sont iid. : ind√©pendantes et identiquement distribu√©es, ce dernier signifiant que toutes les $X_i$ suivent la m√™me loi, qui n‚Äôest pas forc√©ment normale.
- La variance est finie.

## Th√©or√®me de De Moivre-Laplace

<aside>
üó∫Ô∏è La premi√®re foi que le TLC a √©t√© √©nonc√© a √©t√© sous la forme du th√©or√®me de De Moivre-Laplace, d‚Äôo√π son importance historique.

</aside>

Le th√©or√®me de De Moivre-Laplace est juste un cas du th√©or√®me central de la limite : le cas o√π les VA iid. $(X_i)$ qui sont somm√©es suivent chacune une loi de Bernoulli. Notons que le TLC n‚Äôest pas limit√© √† que les $(X_i)$ suivent cette loi.

Soit $\{X_1, \dots, X_n\}$ une suite des V.A. Bernoulli de param√®tre $p$. Donc la suite de terme g√©n√©rale$\sqrt{n}(\bar{X}_n-p)$ converge en loi quand $n \rightarrow \infin$ vers une distribution normale de moyenne $0$ et variance $p(1-p)$.

$$
Z_n = \frac{X_n - np}{\sqrt{np(1-p)}} = \frac{X_n - \mu}{\sigma} \implies \left(\lim_{n\rightarrow\infin} Z_n\right)\sim\mathcal N(0,1)
$$

Quand $n$ tend vers l‚Äôinfini, $\left(\lim_{n\rightarrow \infin} Z_n \right) = \mathcal N(0,1)$, c‚Äôest une √©galit√© stricte. Par contre, quand $n$ est fini, on parle juste d‚Äôune approximation, donc $Z_n \approx \mathcal N(0,1)$. Cette approximation est accept√©e si les conditions suivantes sont v√©rifi√©s :

- $np \ge 10$.
- $n(1-p) \ge 10$.

## Formulation plus pratique du TLC et sa relation avec la LGN

Soit $\bar{X}_n$ la moyenne empirique de $n$ V.A. iid. Donc :

- Loi des Grands Nombres : tant que $n \rightarrow \infin$, la distance entre $\bar{X}_n$ et $\mathbb{E}[X]$ devient plus petite que tout nombre r√©el $a>0$ avec probabilit√© $1$.
- Th√©or√®me Centrale de la Limite : tant que $n \rightarrow \infin$, la V.A. $\bar{X}_n$ converge en loi vers $\mathcal{N}(0, \frac{\sigma^2}{n})$.

Il faut laisser clair une chose : la moyenne empirique toujours va converger vers la moyenne th√©orique. Toujours. On peut r√©√©crire les deux th√©or√®mes comme suite, $a > 0$ :

$$
\begin{align*}

\text{LGN :} & \lim_{n\rightarrow\infin} \mathbb{P}(-a\le\bar{X}_n-\mathbb{E}[X] \le a) = 1

\\

\text{TCL :} & \lim_{n\rightarrow\infin} \mathbb{P}(-a\le\bar{X}_n-\mathbb{E}[X] \le a) = \int_{-a}^a \mathcal{N}\left(0,\frac{\sigma^2}{n}\right)dx \space (*)

\end{align*}

\\
\\[8pt]
(*): \text{cette derni√®re formulation du TCL est incorrecte ! voir note dessous.}
$$

D‚Äôun c√¥t√©, il semble que $(\bar{X}_n-\mathbb{E}[X])$  tend vers $0$. Au m√™me temps, il semble que c‚Äôest √©gale √† l‚Äôint√©grale. Donc, lequel des deux ? La premi√®re.

Il existe un probl√®me avec la deuxi√®me : si $n \rightarrow \infin$, la variance devient $0$, et la variance comme telle n‚Äôest pas utile
$$. Imaginons la courbe qui viendrait si la variance √©tait $0$ : il n‚Äôexiste pas de dispersion autour de la moyenne et $\bar{X}_n$ serait toujours √©gal √† $\mathbb{E}[X]$, c√†d. $(\bar{X}_n-\mathbb{E}[X])$ serait toujours √©gal √† $0$.

![Untitled](new/uga/l2/s3/math/S3%20math%20probabilit√©s%202/05%20cadre%20g√©n√©ral%20th√©or√®me%20centrale%20de%20la%20limit/Untitled.png)

$$
F(x)=
\begin{cases}
0, x < 0 \\
1, x \ge 0
\end{cases}
$$

![Untitled](new/uga/l2/s3/math/S3%20math%20probabilit√©s%202/05%20cadre%20g√©n√©ral%20th√©or√®me%20centrale%20de%20la%20limit/Untitled%201.png)

$$
"f(x)"=
\begin{cases}
1, x=0 \\
0, x \ne 0
\end{cases}
$$

Il est correct de construire une fonction de r√©partition qui repr√©senterait la fonction de r√©partition de $(\bar{X}_n-\mathbb{E}[X])$. En fait, une variable al√©atoire avec telle fonction de r√©partition est appel√©e une **variable al√©atoire d√©g√©n√©r√©e**.

Par contre, il n‚Äôest pas possible de construire une densit√©, car il devrait avoir une aire sous la courbe √©gale √† $1$ quand $x=0$, mais ce n‚Äôest pas possible, car le ‚Äúrectangle‚Äù sous $f(x)$ n‚Äôas pas de ampleur, donc son aire est toujours $0$.

Le choix de multiplier $(\bar{X}_n-\mathbb{E}[X])$ par $\sqrt{n}$ permet de laisser tendre $n$ vers l‚Äôinfini et que la variance ne soit pas nulle. Particuli√®rement, on garanti l‚Äôexistence d‚Äôune variance non-nulle, mais aussi non-infinie, c‚Äôest qui nous est utile.

<aside>
üí° On pourrait concevoir deux formes de pr√©senter le TCL : la r√©elle et la pratique.

$$
\begin{align*}

\text{TCL r√©el : }
\lim_{n\rightarrow\infin} \mathbb{P}(-a\le\sqrt{n}(\bar{X}_n-\mathbb{E}[X]) \le a) &= \int_{-a}^a \mathcal{N}\left(0,\sigma^2\right)dx

\\

\text{TCL pratique : }
\mathbb{P}(-a\le\bar{X}_n-\mathbb{E}[X] \le a) &\approx \int_{-a}^a \mathcal{N}\left(0,\frac{\sigma^2}{n}\right)dx

\end{align*}
$$

Il ne faut absolument pas appliquer une limite $\lim_{n \rightarrow \infin}$ dans la formulation pratique. Il sert comme une bonne approximation √† partir de $n \ge 30$, mais **il ne fait objectivement plus de sens si on laisse $n$ tendre vers l‚Äôinfini !** Je l‚Äôavais fais en dessus pour expliquer le besoin d‚Äôajouter le facteur $\sqrt{n}$.

</aside>