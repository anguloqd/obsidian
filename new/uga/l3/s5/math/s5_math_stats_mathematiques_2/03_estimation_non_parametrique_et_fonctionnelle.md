# 03 // estimation non paramÃ©trique et fonctionnelle

# Motivation

## La loi mÃ¨re est inconnue !

Lâ€™estimation non paramÃ¨tre vient quand on considÃ¨re que la loi mÃ¨reâ€”la loi suivie par les observations iid. qui composent un Ã©chantillonâ€”ne fait pas partie dâ€™une famille paramÃ©trable de lois.

Il ne peut donc plus sâ€™agir ici dâ€™estimer un paramÃ¨tre qui dÃ©terminerait totalement la loi et par suite toute caractÃ©ristique de celle-ci. DÃ¨s lors deux orientations sont possibles :

- Estimation non paramÃ©trique ponctuelle : on sâ€™intÃ©resse uniquement Ã  quelques valeurs caractÃ©ristiques de la loi
- Estimation fonctionnelle : on veut estimer la loi dans sa globalitÃ© par sa fonction de rÃ©partition ou sa densitÃ©, ou sa fonction de probabilitÃ©

# Estimation non paramÃ©trique (ponct. et par IC)

## Estimation de $\mu$ et $\sigma^2$

### Estimant $\mu$

Rappelons quelques faits importants pour la suite : 

- Les moments empiriques simples ou centrÃ©s, sâ€™ils existent, sont des estimateurs sans biais des moments correspondants de la loi mÃ¨re
- Comme consÃ©quence de la LGN, ces estimateurs sont convergents presque sÃ»rement
- Pour que le moment dâ€™ordre $k$ converge, il suffit que $\mathbb E[|X^k|]$ existe.

Pour un sondage dâ€™une population ***finie***, ces conditions sont donc rÃ©unies, ceux-ci Ã©tant des caractÃ©ristiques descriptives de la population dans son ensemble. Soit $\mu$ et $\sigma^2$ la moyenne et la variance de la loi inconnue en question (supposant quâ€™elles existent), et soit $n\ge 30$, donc

$$
\mathbb E[\bar X] = \mu\hspace{8pt}\text{ et\hspace{8pt} Var}(\bar X)=\frac{\sigma^2}{n}
$$

La premiÃ¨re relation reflÃ©tant un biais nul et la deuxiÃ¨me montrant directement la convergence en moyenne quadratique. En conclusion, nous utiliserons naturellement la moyenne empirique pour estimer la moyenne de la loi, nous satisfaisant de ces propriÃ©tÃ©s.

<aside>
ğŸ’¡ Il nâ€™est pas possible de dire si tel est le meilleur choix possible, sauf Ã  imposer des conditions restrictives sur la nature de la loi mÃ¨re, ce qui nâ€™est pas dans lâ€™esprit de lâ€™estimation non paramÃ©trique.

</aside>

En plus, et aussi grÃ¢ce Ã  $n\ge 30$ (pour TCL) et la convergence de $S^2$ vers $\sigma^2$, pour n'importe quelle loi des $X_i$, on a que

$$
\frac{\bar X-\mu}{S/\sqrt n}\sim\mathcal T_{n-1}
$$

Donc, indiffÃ©rent de la loi des $X_i$, on peut avoir un IC classique (Ã  $95\%$) pour le paramÃ¨tre $\mu$ :

$$
IC_{95\%}(\mu)=\left[\bar X-t_{97.5\%}^{(n-1)}\frac{S}{\sqrt n}, \bar X+t_{97.5\%}^{(n-1)}\frac{S}{\sqrt n} \right]
$$

### Influence de valeurs extrÃªmes ou aberrantes sur $\bar X$

Il a Ã©tÃ© dit quâ€™en principe $n â‰¥ 30$ suffit. Cependant, lâ€™approximation par une loi de Student posera problÃ¨me pour des VA dont les queues de distribution sont allongÃ©es et peuvent produire des observations trÃ¨s Ã©loignÃ©es du centre.

Une valeur trÃ¨s excentrÃ©e va influencer fortement la valeur de $X_i$ et encore plus, car  interviennent des Ã©carts au carrÃ©, celle de la variance $S^{2^\prime}$, rendant ainsi ces statistiques trop instables pour garantir lâ€™approximation par une loi de Student si la taille de lâ€™Ã©chantillon nâ€™est pas trÃ¨s Ã©levÃ©e.

Pour les mÃªmes raisons, si les observations sont contaminÃ©es par des valeurs aberrantes lâ€™approximation sera dÃ©faillante. Ceci peut provenir, par exemple, dâ€™erreurs dans le recueil des informations ou de prÃ©sences de valeurs Ã©trangÃ¨res au phÃ©nomÃ¨ne Ã©tudiÃ© (dans les sondages, prÃ©sence dâ€™individus distincts nâ€™appartenant pas Ã  la population).

Si lâ€™on soupÃ§onne la prÃ©sence de valeurs trÃ¨s extrÃªmes ou aberrantes, on peut soit Ã©liminer purement et simplement les valeurs trop Ã©loignÃ©es par examen de la distribution des  observations (histogramme), soit rÃ©duire leurs poids dans le calcul de la moyenne et de la variance. On dÃ©finit ainsi des $M$-estimateurs dont lâ€™Ã©tude des propriÃ©tÃ©s fait lâ€™objet de la thÃ©orie de la robustesse.

### Estimant $\sigma^2$

De la mÃªme maniÃ¨re quâ€™on construit une statistique sur $\bar X$ â€œcentrÃ©e-rÃ©duiteâ€,$\left(\frac{\bar X - \mu}{S/\sqrt n} \right)$, qui suit une loi connue (Student), on peut faire de mÃªme avec la variance empirique $S^{2^\prime}$, **sous la condition stricte que les $X_i$ suivent forcÃ©ment une loi gaussienne** :

$$
\frac{(n-1)S^{2^\prime}}{\sigma^2}\sim\chi^2_{n-1}
$$

Par contre, dans cette section, on a prÃ©sumÃ© quâ€™on ne connaÃ®t absolument pas la loi qui suivent les $X_i$. Si cette loi nâ€™est pas gaussienne, on ne peut plus affirmer que telle statistique sur $S^{2^\prime}$ suit une loi de $\chi^2$, **mÃªme si $n$ tend vers lâ€™infini**.

Ok, on laisse tomber telle statistique de $\frac{(n-1)S^{2^\prime}}{\sigma^2}$. Ceci dit, on peut dire que $S^{2^\prime}$ tout courte suit elle-mÃªme une loi gaussienne. Ici, on devrait faire une petite note :

<aside>
ğŸ’¡ Comme pour la loi des grands nombres il existe diffÃ©rentes versions du TCL partant de conditions plus ou moins restrictives.

En particulier, il nâ€™est pas nÃ©cessaire que les VA soient de mÃªme loi ni mÃªme quâ€™elles soient indÃ©pendantes dans la mesure ou leur degrÃ© de dÃ©pendance reste faible. Ceci explique que certains phÃ©nomÃ¨nes naturels rÃ©pondent bien a un
modÃ¨le gaussien du fait que la variable Ã©tudiÃ©e rÃ©sulte de lâ€™addition dâ€™effets alÃ©atoires multiples.

Ainsi, on peut Ã©tablir un comportement asymptotique gaussien pour dâ€™autres types de statistiques dans la mesure ou elles sont des moyennes de VA qui, sans Ãªtre nÃ©cessairement indÃ©pendantes pour $n$ fini, tendent Ã  Ãªtre iid. quand $n â†’ âˆ$.

</aside>

Ceci est le cas pour $S^{2^\prime}$ : les Ã©lÃ©ment $(X_i-\bar X)$  (et donc leur carrÃ©s) tendent Ã  devenir indÃ©pendants du fait que $\bar X$ converge vers $\mu$. Il est toutefois nÃ©cessaire que $\text{Var}(S^{2^\prime})$ existe, mais ceci est garanti si $\mu_4=\mathbb E [X^4]$ existe aussi, qui est le raccourci quâ€™on utilise le plus souvent.

Avant dâ€™Ã©tablir la loi de $S^{2^\prime}$ qui nous intÃ©resse, il faut mentionner un fait important pour le calcul :

$$
\mathbb E[S^{2^\prime}]=\sigma^2 \hspace{8pt}\text{et}\hspace{8pt}\text{Var}(S^{2^\prime})=\frac{1}{n}\left(\mu^\prime_4-\frac{n-3}{n-1}\sigma^4\right), \text{oÃ¹ }\mu^\prime_4=\mathbb E[(X-\bar X)^4]
$$

Finalement, on peut Ã©tablir la loi de $S^{2^\prime}$ asymptotique Ã  une Gaussienne comme suit, qui est juste une application du TCL â€œthÃ©orique et originalâ€ :

$$
\lim_{n\rightarrow\infin}

\frac{\sqrt n (S^{2^\prime}-\sigma^2)}{\sqrt{\mu^\prime_4-\sigma^4}}

\sim\mathcal N(0,1)
$$

### Un IC dans le cas non paramÃ©trique pour $\sigma^2$

On peut rÃ©Ã©crire cette derniÃ¨re statistique pour arriver Ã  une plus pratique :

$$
\frac{\sqrt n (S^{2^\prime}-\sigma^2)}{\sqrt{\mu^\prime_4-\sigma^4}}=\frac{\sqrt n (S^{2^\prime}-\sigma^2)}{\sigma^2\sqrt{\underbrace{(\mu^\prime_4/\sigma^4)}_\text{Curt.}-1}}
$$

Le terme marquÃ© $(\mu^\prime_4/\sigma^4)$ est connu comme le *coefficient de curtose*. Il vaut $3$ pour la loi Gaussienne, plus de $3$ pour un loi Ã  pic plus prononcÃ©, et moins de $3$ pour un pic plus plat.

Dans le cas Gaussien, cette expression est bien la version centrÃ©e et rÃ©duite de $\frac{(n-1)S^{2^\prime}}{\sigma^2}$ au facteur $\sqrt{(n-1)/n}$ prÃ¨s, puisque la loi $\chi^2_{n-1}$ est de moyenne $(n-1)$ et de variance $2(n-1)$. Par ailleurs, la loi de $\chi^2$ tend Ã  devenir gaussienne quand $n$ tend vers lâ€™infini.

Finalement, pour construire un intervalle de confiance asymptotique, on peut envisager dâ€™estimer $(\mu^\prime_4-\sigma^4)$ par sa version empirique $(M^\prime_4-S^4)$, ou Ã©galement le coefficient de curtose par la curtose empirique $(M^\prime_4/S^4)$, mais la convergence est lente et un $n$ trop grand sera nÃ©cessaire pour espÃ©rer une bonne approximation.

On peut recourir Ã  une approche dite par rÃ©Ã©chantillonnage dont lâ€™intÃ©rÃªt est gÃ©nÃ©ral et câ€™est pourquoi nous y consacrons une section spÃ©cifique. Cette approche sera Ã©galement appropriÃ©e pour lâ€™estimation de lâ€™Ã©cart-type.

## Estimation de quantiles

### DÃ©finition et notes

Un quantile dâ€™ordre $p$ est la quantitÃ© sur la loi de distribution tel quâ€™elle â€œsÃ©pareâ€ la population et contient $p\%$ de la population Ã  sa gauche.

$$
\kappa_{p} : F(\kappa_p)=p  \iff \kappa_p=F^{-1}(p)
$$

Notons que, Ã©tant donnÃ© une loi, la probabilitÃ© dâ€™observer une valeur $x\le \kappa_p$ est Ã©gale Ã  $p$. Une autre interprÃ©tation importante sont les quantiles comme des â€œpoints de coupureâ€ car la valeur $\kappa_p$ divise la loi en $p\%$ Ã  sa gauche et $(1-p)\%$ Ã  sa droite. 

Il faut clarifier, Ã  nouveau, que les quantiles sont des points de coupure sur la loi (densitÃ©, dans ce cas). Mais ce chapitre traite les cas dâ€™une loi non connue, et quâ€™on en observe un Ã©chantillon.

**Note pratique**. On sait que la fonction de rÃ©partition devrait avoir une inverse car elle est injective (croissante) et surjective, donc bijective et donc inversible.

### Notation et premiers thÃ©orÃ¨mes

Normalement on adopte la notation â€œle $k$-iÃ¨me $q$-quantileâ€ thÃ©orique pour le point $k/q$, ou simplement $p$ pour exprimer un nombre rÃ©el sans se soucier de trouver $k$ et $q$ pour reprÃ©senter le quantile. Si on parle dâ€™un Ã©chantillon/population de taille $n$ particulier, son $k$-iÃ¨me $q$-quantile est $n\frac{k}{q}$ ou $np$.

Avant de continuer, on note $\kappa_p$ le quantile $p$ de la population (et donc $\kappa_p=F^{-1}(p)$) et $\hat\kappa_p$ lâ€™estimateur du prÃ©cÃ©dent, qui on va dÃ©finir comme tout simplement le quantile $p$ de lâ€™Ã©chantillon. Si on suppose une densitÃ© continue $f$, on a donc

$$
\lim_{n\rightarrow\infin} n\text{Var}(\hat\kappa_{\lfloor np\rfloor +1})=\frac{p(1-p)}{f(\kappa_p)^2}

\text{\hspace{6pt}et\hspace{6pt}}

\lim_{n\rightarrow\infin}\frac{f(\kappa_p)}{\sqrt{p(1-p)}}\sqrt n(\hat\kappa_{\lfloor np\rfloor +1}-\kappa_p)\sim\mathcal N(0,1)

\\[10pt]

\text{Formulation plus pratique : } \hat\kappa_p \sim \mathcal N\left(\mu=\kappa_p,\hspace{4pt} \sigma^2=\frac{p(1-p)}{nf(\kappa_p)^2}\right) \text{si }n\text{ fini}
$$

Ceci nous permet de conclure que la quantile empirique $\hat\kappa_p$ est asymptotiquement sans biais et converge en moyenne quadratique vers sont estimande $\kappa_p$ puisque sa variance tend vers $0$ (on peut aussi montrer quâ€™elle converge presque sÃ»rement).

### Construire un IC de $\kappa_p$

Par contre, on a un souci : $f(\kappa_p)$ est inconnu. Lâ€™objectif câ€™est donc dâ€™estimer ponctuellement $f(\kappa_p)$ pour construire un IC de $\kappa_p$. On sait que si on prend une observation $X_i$ dâ€™une loi, celle-ci a probabilitÃ© $p$ dâ€™Ãªtre infÃ©rieur ou Ã©gale Ã  $\kappa_p$ et $1-p$ sinon. Ceci est donc juste une loi de Bernoulli.

Pour un Ã©chantillon $n$ Ã  plusieurs observations, on appelle $n_\text{inf}$ la quantitÃ© dâ€™observations infÃ©rieures ou Ã©gales Ã  $\kappa_p$, ce qui serait une loi binomiale. Donc $n_\text{inf}\sim\mathcal B(n, p)$ ou 

$$
\mathbb P(\ell_1\le n_\text{inf} \le \ell_2) = \sum_{k=\ell_1}^{\ell_2} \binom{n}{k}p^k(1-p)^{n-k}
$$

Les $\ell_1$ et $\ell_2$ sont des limites pour encadrer. Ici, on veut se fixer deux tÃ¢ches :

- On veut choisir leurs valeurs tel que $\mathbb P(\ell_1\le n_\text{inf} \le \ell_2) \ge 0.95$ et quand mÃªme bien proche aussi de $0.95$.
- On veut $\ell_1+\ell_2=n$ ou au moins une approximation de $n$, ce qui permettra que lâ€™intervalle $[\ell_1, \ell_2]$ soit le plus symÃ©trique possible autour de $n/2$, et donc le plus Ã©troit.

Une observation clÃ© câ€™est que, si on choisit $\ell_1$ tel quâ€™il est vrai que $(X_{(\ell_1)} \le\kappa_p)$, donc il est impliquÃ© quâ€™il existe ***au moins*** une quantitÃ© $\ell_1$ dâ€™observations infÃ©rieures ou Ã©gales Ã  $\kappa_p$, et donc câ€™est une Ã©quivalence avec la proposition $(\ell_1\le n_\text{inf})$. De mÃªme avec $(\kappa_p \le X_{(\ell_2+1)})$ et $(n_\text{inf} \le \ell_2)$. Mettant tout ensemble :

$$
\mathbb P(\ell_1\le n_\text{inf} \le \ell_2) = \mathbb P(X_{(\ell_1)}\le\kappa_p\le X_{(\ell_2+1)}),

\\[10pt]

\text{et donc}

\\[6pt]

\mathbb P(X_{(\ell_1)}\le\kappa_p\le X_{(\ell_2+1)})=\sum_{k=\ell_1}^{\ell_2} \binom{n}{k}p^k(1-p)^{n-k}
$$

La derniÃ¨re Ã©galitÃ© tient pour quelque soit $\kappa_p$. Câ€™est en jouant avec $\ell_1$ et $\ell_2$ quâ€™on peut trouver un IC Ã  nâ€™importe quel niveau de signification, normalement $95\%$.

## MÃ©thodes de rÃ©Ã©chantillonnage

Le rÃ©Ã©chantillonnage est le fait de tirer de sous-Ã©chantillons Ã  partir dâ€™un Ã©chantillon, lâ€™objectif Ã©tant de simuler la variabilitÃ© des estimateurs et de rÃ©duire le biais dâ€™un estimateur rÃ©fÃ©rence. **ParticuliÃ¨rement, on va sâ€™intÃ©resser Ã  estimer la variance dâ€™un estimateur $\hat\theta$**.

Il existent deux mÃ©thodes : la mÃ©thode jackknife et la mÃ©thode bootstrap. Pour les deux mÃ©thodes, **on va parler de â€œlâ€™estimateur de maximum de vraisemblanceâ€**. Ceci nâ€™a rien Ã  voir avec lâ€™estimation de maximum de vraisemblance comme le $\max L(\theta|\bold x)$, mais câ€™est plutÃ´t la version empirique $\hat\theta$ dâ€™une statistique paramÃ©trique $\theta$.

### La mÃ©thode du jackknife

### â†’ La mÃ©thode en soi

Lâ€™objectif ici est de crÃ©er un estimateur $\hat\theta^*$ avec moins de biais que $\hat\theta$. Par la suite, on considÃ¨re que $\theta=\varphi([X_i])$ (le paramÃ¨tre se dÃ©duit avec un algorithme $\varphi$ sur les $X_i$) et que $\hat\theta=\varphi([x_i])$.

Pour la mÃ©thode du jackknife, on aura besoin de :

- Un paramÃ¨tre $\theta$;
- Un estimateur du paramÃ¨tre $\hat\theta$ qui est ***convergent et biaisÃ©***
- Un Ã©chantillon rÃ©alisÃ© $[x_i]$

En premier, on crÃ©e lâ€™idÃ©e de â€œ$j$-Ã¨me sous-Ã©chantillonâ€ $[x_i]_{-j}$ comme le sous-Ã©chantillon qui contient tous les Ã©lÃ©ments de lâ€™Ã©chantillon original sauf le $j$-Ã¨me Ã©lÃ©ment.

DeuxiÃ¨me, on dÃ©finit une nouvelle statistique $\hat\theta_j$ appelÃ©e â€œ$\hat\theta$ privÃ© de $j$â€ : cette statistique calcule lâ€™estimateur sur $[x_i]_{-j}$. Câ€™est comme si on considÃ©rait lâ€™Ã©chantillon tirÃ© comme tout la population et puis on en prend $n$ Ã©chantillons de taille $(n-1)$.

$$
\hat\theta_{-j}=\varphi([x_i]_{-j})
$$

TroisiÃ¨me, on dÃ©finit autre statistique qui sâ€™appelle le â€œ$j$-Ã¨me pseudovaleurâ€ (en anglais, le â€œ$j$-th leave-out-oneâ€) comme suit. Ceci est une combinaison linÃ©aire convexe. Pour rappel, la combinaison linÃ©aire dâ€™estimateurs sans biais est aussi sans biais ssi. la somme des coefficients est Ã©gal Ã  1 (i.e. convexe).

$$
\hat\theta^*_{j}=n\hat\theta_n-(n-1)\hat\theta_{-j}
$$

Finalement, on dÃ©finit lâ€™estimateur jackknife $\hat\theta^*$ comme la moyenne des pseudovaleurs :

$$
\hat\theta^*=\frac{1}{n}\sum_j \hat\theta_{j}^*
$$

### â†’ CrÃ©er un IC approchÃ© avec les pseudovaleurs

Notons que la statistique finale $\hat\theta^*$ est une moyenne. On peut estimer la variance de cette statistique comme

$$
s^2_{\hat\theta^*}=\frac{1}{n-1}\sum_{i=1}^n(\hat\theta_j-\hat\theta^*)^2
$$

Pour finalement arriver Ã  la construction dâ€™un intervalle de confiance :

$$
IC_{95\%}(\theta)\approx[\hat\theta^*-(t^{n-1}_{2.5\%})s_{\hat\theta^*}, \hat\theta^*+(t^{n-1}_{97.5\%})s_{\hat\theta^*}]
$$

### â†’ Lâ€™intÃ©rÃªt de cette mÃ©thode

<aside>
ğŸ’¡ La proposition importante câ€™est que, si le biais de $\hat\theta$ est de la forme $\frac c n$, oÃ¹ $c$ constante, **alors lâ€™estimateur jackknife $\hat\theta^*$ est sans biais**. Si le biais contient au moins un terme $\frac c n$, donc $\hat\theta^*$ rÃ©duira le biais en $\frac c n$. La preuve est la proposition 8.2 du livre de Michel Lejeune, page 178.

Le rÃ©sultat du jackknife sur un estimateur est un autre estimateur avec un biais plus baisse. De faire du jackknife sur la variance non corrigÃ©e nous retourne la variance corrigÃ©e, par exemple.

Pour un estimateur sans biais comme $\bar X$, de faire du jackknife sur $\bar X$ nous retourne lâ€™estimateur lui-mÃªme.

</aside>

En gÃ©nÃ©ral, la loi Ã©tant totalement inconnue, on ne connaÃ®t pas la forme du biais, mais on sâ€™attend Ã  ce quâ€™il soit de toute faÃ§on rÃ©duit par la procÃ©dure dÃ©crite.

Outre la rÃ©duction du biais, lâ€™intÃ©rÃªt du jackknife, primordial ici, est de **permettre lâ€™estimation de lâ€™Ã©cart-type de lâ€™estimateur de base**, câ€™est-Ã -dire $s_{\hat\theta}$, et la possibilitÃ© de construire un intervalle de confiance approchÃ©.

### â†’ Observations finales

Il y a une proposition trÃ¨s importante admise sur le livre de Michel Lejeune. Soit $\hat\theta^*$ lâ€™estimateur jackknife de $\theta$ et soit $s^2_{\hat\theta^*}$ la variance de lâ€™estimateur jackknife. Donc,

$$
\frac{\hat\theta^*-\theta}{s_{\hat\theta^*}/\sqrt n}\sim\mathcal T_{n-1}, \text{ mais rappelons que} \lim_{n\rightarrow\infin}\frac{\hat\theta^*-\theta}{s_{\hat\theta^*}/\sqrt n}\sim\mathcal N(0,1)
$$

Notons que la statistique elle est trÃ¨s pareille Ã  

$$
\frac{\bar X-\mu}{\sigma/\sqrt n}
$$

Et que celle-ci aussi suit une loi normale. Donc, la proposition rÃ©sulte du fait que les pseudovaleurs tendent Ã  Ãªtre indÃ©pendantes et gaussiennes pour une grande variÃ©tÃ© de
statistiques, on peut donc voir lâ€™analogie que les pseudovaleurs $[\hat\theta^*_j]$ sont comme les observations $[X_i]$ de $\bar X$.

En ce qui concerne lâ€™approximation asymptotique de lâ€™intervalle de confiance issu du jackknife, il est difficile de savoir Ã  partir de quelle taille dâ€™Ã©chantillon elle devient satisfaisante. Pour les petits Ã©chantillons, le bootstrap offre une alternative plus sÃ»re.

### La mÃ©thode du bootstrap

### â†’ PrÃ©paration

Juste avant de commencer, on va faire une modification importante Ã  lâ€™espÃ©rance $\mathbb E[X]$ ou plutÃ´t la moyenne $\mu$ dans ce contexte :

$$
\overbrace{\int_\Omega x\cdot f(x) \space dx}^{I_1} = \overbrace{\int _\Omega x\space dF(x)}^{I_2}, \text{ car }

\\[8pt]

\frac{d}{dx}F(x)=f(x) \iff dF(x)=f(x)dx
$$

Le deuxiÃ¨me intÃ©grale est lâ€™intÃ©grale de Riemman-Stieltjes, est câ€™est une gÃ©nÃ©ralisation de la premiÃ¨re intÃ©grale. Bref, on peut utiliser $I_1$ seulement si $X$ a une fonction de densitÃ©, tant que on peut toujours utiliser $I_2$ (continu ou discrÃ¨te) avec la seule condition que $X$ admet une espÃ©rance finie.

Finalement et dâ€™ailleurs, on considÃ¨re que â€œlâ€™estimateur de maximum de vraisemblanceâ€ dâ€™un paramÃ¨tre est juste sa version empirique sur lâ€™Ã©chantillon.

### â†’ La mÃ©thode en soi

On commence par observe un Ã©chantillon $\bold x=[x_i]_{1\le i\le n}$. On crÃ©e un sous-Ã©chantillon $\bold x^*$ **de la mÃªme taille** que $\bold x$ dont les valeurs sont des **tirages avec remise** de $\bold x$. Notons que le plus probable est que le nouveau sous-Ã©chantillon aura des valeurs rÃ©pÃ©tÃ©s, mais cela ne pose pas des problÃ¨mes.

Ayant fait Ã§a, on estime $\theta$ en calculant $\hat\theta^*$ sur le nouveau Ã©chantillon. On rÃ©pÃ¨te cette estimation $M$ fois jusquâ€™Ã  ce quâ€™on finit avec $M$ estimations de $\theta$, quâ€™on garde dans un vecteur $\hat{\Theta}^*=[\hat\theta^*_i]_{1\le i\le M}$

Finalement, on estime la variance de $\hat\theta$ en calculant la variance empirique des Ã©lÃ©ments dans $\hat\Theta^*$, donc

$$
s^2_{\hat\theta^*}=\frac{1}{M-1}\sum_{i=1}^M(\hat\theta^*_i-\mu_{\hat\theta^*})^2 \text{\hspace{8pt}et\hspace{8pt} }\lim_{n\rightarrow\infin}s^2_{\hat\theta^*}=^\text{p.s.}s^2_{\hat\theta}
$$

On montre que, lorsque $M$ tend vers lâ€™infini, lâ€™estimateur issu de cette procÃ©dure tend presque sÃ»rement vers lâ€™estimateur du maximum de vraisemblance du paramÃ¨tre $\text{Var}(\hat\theta)$. **Oui, il faut rappeler que $\text{Var}(\hat\theta)$ est un paramÃ¨tre**.

En pratique, $M = 100$ fournit une approximation suffisante de cet EMV car lâ€™Ã©cart sera alors nÃ©gligeable par rapport Ã  lâ€™erreur dâ€™estimation du maximum de vraisemblance lui-mÃªme.

### â†’ PremiÃ¨re construction dâ€™un IC pour $\theta$

En supposant que $\hat\theta$ est asymptotiquement sans biais et gaussien, on peut construire un IC de $\theta$ comme suit

$$
IC_{95\%}(\theta)\approx[\hat\theta-1.96s^*_{\hat\theta},\hat\theta+1.96s^*_{\hat\theta}]
$$

Lâ€™approximation gaussienne est frÃ©quemment lÃ©gitime, en particulier si $\hat\theta$ est lui-mÃªme une estimation du maximum de vraisemblance de $\hat\theta$. 

Par contre, celle-ci nâ€™est pas assurÃ©e pour tous les types de statistiques ou bien elle peut Ãªtre trop lente pour fournir une approximation satisfaisante au vu de la taille $n$ de lâ€™Ã©chantillon.

### â†’ DeuxiÃ¨me construction dâ€™un IC pour $\theta$

Si on peut admettre que 

- la distribution de $\hat\theta$ tend vers une gaussienne quand $n\rightarrow\infin$;
- la variance a une forme Ã©quivalente Ã  $\sigma^2_\theta/n$, oÃ¹ $\sigma^2_\theta$ est constante
- on connaÃ®t un estimateur convergent de $\sigma^2_\theta$ quâ€™on note simplement $s^2_\theta$

Donc on peut appliquer une mÃ©thode *studentisÃ©e*. On modifie la mÃ©thode de sorte que, Ã  chaque fois quâ€™on calcule une instance de $\hat\theta^*$ sur un nouveau Ã©chantillon, on calcule aussi $s^2_{\hat\theta^*}$. On finira donc avec un vecteur $\hat\Theta^2$ et un autre vecteur $\bold s^2_{\hat\theta^*}$ On dÃ©finie un troisiÃ¨me vecteur $\bold T^*$ tel que

$$
\bold T^*=[T^*_i]_{1\le i \le n},\text{ oÃ¹ }T_i^*=\frac{\hat\theta^*_i-\hat\theta}{s^*_{\hat\theta^*, i}/\sqrt n}
$$

Finalement, on calcule les quantiles empiriques $t^*_{2.5\%}$ et $t^*_{97.5\%}$ pour Ã©tablir lâ€™IC comme suit :

$$
IC_{95\%}(\theta)\approx[\hat\theta-(t^*_{2.5\%})\frac{s}{\sqrt n},\hat\theta+(t^*_{2.5\%})\frac{s}{\sqrt n}]
$$

Il faudrait par contre que $M \ge 1000$ pour avoir des approximations prÃ©cises.

### â†’ Observations finales

- Lâ€™approche bootstrap est appropriÃ©e dans des situations trÃ¨s complexes, ou il nâ€™y aura gÃ©nÃ©ralement pas dâ€™alternative, et pas uniquement dans le cadre de lâ€™Ã©chantillonnage alÃ©atoire simple. Câ€™est donc un outil extrÃªmement prÃ©cieux qui est devenu viable avec les capacitÃ©s de calcul actuelles.
- Pour des types trÃ¨s divers de statistiques, il a Ã©tÃ© dÃ©montrÃ© que la distribution gÃ©nÃ©rÃ©e par les valeurs $\hat\Theta^*=[\hat\theta^*_i]_{1\le i \le M}$ en faisant tendre $M$ vers lâ€™infini, appelÃ©e distribution bootstrap de la statistique $\theta$, converge vers la vraie distribution de $\theta$ quand $n$ tend vers lâ€™infini et ceci de faÃ§on rapide.
    - Câ€™est-Ã -dire que, quand $M\rightarrow\infin$, lâ€™estimateur bootstrap $\hat\theta^*$ tend vers lâ€™estimateur de maximum de vraisemblance $\hat\theta$ et, quand $n\rightarrow\infin$, lâ€™estimateur de maximum de vraisemblance $\hat\theta$ tend vers le vrai paramÃ¨tre $\theta$.
- Signalons en particulier quâ€™alors que le jackknife Ã©choue pour obtenir un intervalle de confiance pour la mÃ©diane (ou un quantile) de la loi mÃ¨re, le bootstrap donne un rÃ©sultat trÃ¨s proche de lâ€™intervalle proposÃ© vers la fin de la section prÃ©cÃ©dente.

# Estimation fonctionnelle

## Estimation de la densitÃ©

### Lâ€™histogramme

Si bien lâ€™histogramme sâ€™intÃ©resse plutÃ´t Ã  lâ€™estimation de pourcentage entre les intervalle que le mÃªme histogramme Ã©tablit, on va lâ€™Ã©tudier comme une estimation de densitÃ©.

La dÃ©finition mathÃ©matique dâ€™un histogramme est comme suit : on prend un Ã©chantillon et on crÃ©e une suite de valeurs qui seront la point de coupes des barres de lâ€™histogramme. On crÃ©e des intervalles basÃ©es sur ces points de coupe et finalement on peut dÃ©finir la fonction histogramme.

$$
\begin{align*}
\text{Points de coupure : }&\{a_i\}_{1\le i\le {m}} \text{ tel que } a_i < a_{i+1}

\\[7pt]

\text{ Intervalles : }&\{I_i\}_{1\le i\le {m-1}}=\{I_1=]a_1,a_2], \cdots, I_m=]a_{m-2},a_{m-1}]\}

\\[7pt]

\text{Fonction longueur : }
&\ell(I_i)=a_{i+1}-a_i
\\[7pt]

\text{Effectif dans interv. : }

&\{n_i\}_{1\le i\le {m-1}} : n_i=|\{x_i : x_i\in I_i\}|

\\[3pt]

\text{Fonction histogramme : }

&\hat f_n(x)=

\frac{n_i/n}{\ell(I_i)} \text{, oÃ¹ }x\in I_i
\end{align*}
$$

Le plus souvent sera que la grille de dÃ©coupage $\{a_i\}$ sera de longueur rÃ©guliÃ¨re $h$, donc 

$$
\forall i\in\N,\space a_{i+1}-a_i=h \implies \hat f_n(x)=\frac{n_i}{nh} \text{, oÃ¹ }x\in I_i
$$

Notons que si on somme lâ€™aire sous toute les barres, on doit arriver Ã  $1$. Enfin, il sâ€™agit dâ€™une estimation dâ€™une densitÃ©, donc la somme des toutes les probabilitÃ©s doit valoir $1$.

En plus, si on tire juste une observation $x_1$ de la population, la vrai probabilitÃ© que telle valeur tombe dans lâ€™intervalle $I_i$ est notÃ©e $p_i$, quâ€™on estime comme $n_i/n$. Pour un Ã©chantillon de taille $n$, on va dire que la variable alÃ©atoire $n_i \sim \mathcal B(n,p_i)$ compte le nombre dâ€™observations qui tombe dans lâ€™$i$-Ã¨me intervalle. Voyons la consÃ©quence :

$$
\begin{align*}
\hat f_n(x) = \frac{n_i}{nh} \text{ et } n_i\sim\mathcal B(n,p_i)\implies

&\space\mathbb E[\hat f_n(x)]=\frac{1}{nh}\mathbb E[n_i]=\frac{np_i}{nh}=\frac{p_i}{h}

\\[7pt]

&\space \text{Var}[\hat f_n(x)]=\frac{np_i(1-p_i)}{n^2h^2}=\frac{p_i(1-p_i)}{nh^2}
\end{align*}
$$

On sait, en plus, que

$$
p_i=\int_{a_i}^{a_{i+1}}f(x)dx
$$

Dâ€™oÃ¹ $p_i/h$ serait la valeur moyenne de $f$ sur lâ€™intervalle $I_i$. Il faut rappeler que la dÃ©finition dâ€™un estimateur sans biais est

$$
\mathbb E[\hat f(x)]=f(x), \text{ et voyons que } \mathbb E[\hat f_n(x)]=\frac {p_i} h
$$

Donc $\hat f_n(x)$ nâ€™est donc sans biais que pour les valeurs de $x\in I_i$ oÃ¹ la vraie densitÃ© $f$ prend cette valeur moyenne, que câ€™est juste un point particulier dans un intervalle rÃ©elleâ€¦ donc juste un point $x^*_i$ ou un ensemble de points. Pour la plupart dâ€™un intervalle, $\hat f_n(x)$ est biaisÃ©. Le biais sur un point $x$ diffÃ©rent de $x^*_i$ est $f(x^*_i)-f(x)=p_i/h-f(x)$.

### Comportement asymptotique de lâ€™histogramme

Rappelons quâ€™il y a deux paramÃ¨tres pour lâ€™histogramme qui peuvent tendre vers lâ€™infini ou zÃ©ro : lâ€™effectif $n$ et la taille rÃ©guliÃ¨re $h$.

### Par rapport Ã  lâ€™espÃ©rance et au biais de $\hat f$

Si le biais sur un point $x_i\ne x^*_i$, le biais est donc $f(x^*_i)-f(x_i)$. Notons que câ€™est biais ne dÃ©pend pas de $n$, donc si $n\rightarrow\infin$ ne nous sert pas. En fait, il faudrait câ€™est plutÃ´t faire tendre simultanÃ©ment $n\rightarrow\infin$ et $h\rightarrow 0$.

### Par rapport Ã  la variance de $\hat f$

Par rapport Ã  la variance de lâ€™estimateur, cÃ d $\frac{p_i(1-p_i)}{nh^2}$, elle ne tend vers zÃ©ro que quand $nh\rightarrow \infin$.  Voyons que cela le rend donc difficile pour $h$, qui doit tendre vers zÃ©ro mais â€œinfiniment moins rapideâ€ que la fonction $1/n$ pour que $nh$ tende vers lâ€™infini. On pourrait donc fixer la longueur rÃ©guliÃ¨re des intervalles $h$ comme $c/n$, oÃ¹ $c$ constante.

ConcrÃ¨tement, cela se traduit comme â€œplus $n$ est grand plus, il y a avantage a resserrer les intervalles mais pas trop, afin de garder de grandes valeurs de $n_i$ (effectif dans intervalle $i$)â€.

Il est Ã  noter que ces conditions qui assurent la convergence en moyenne quadratiqueâ€”soit
$n â†’ âˆ$, $h â†’ 0$, $nh â†’ âˆ$â€”restent nÃ©cessaires pour assurer dâ€™autres modes de convergence, notamment en probabilitÃ© ou presque sÃ»rement.

### Par rapport Ã  lâ€™EQM de $\hat f$ (erreur quadratique moyenne)

Si f est deux fois dÃ©rivable, donc pour tout $x$ on a que

$$
\text{EQM}(\hat f_n(x))= \frac{h^2}{12}(f^\prime(x))^2+\frac{f(x)}{nh}+o(h^2)+o(\frac{1}{nh})
$$

et, si on laisse tendre $n â†’ âˆ$, $h â†’ 0$ et $nh â†’ âˆ$ on finit donc avec

$$
\text{EQM}(\hat f_n(x))=\frac{h^2}{12}(f^\prime(x))^2+\frac{f(x)}{nh}
$$

Donc les restes de sÃ©ries disparaissent. Le premier terme est dÃ» au biais au carrÃ© et le deuxiÃ¨me terme Ã  la variance.

Maintenant, on ne sâ€™intÃ©resse plus a laisser $h\rightarrow\infin$ mais Ã  savoir la vitesse de convergence de lâ€™EQM Ã  zÃ©ro en choisissant une valeur concrÃ¨te de $h$. Si on prend la dÃ©rivÃ© de EQM par rapport Ã  $h$ et on lâ€™annule, le $h$ quâ€™on obtient pour atteindre le minimum de EQM serait

$$
h=
\left(
\frac{6f(x)}{f\prime(x)^2}
\right)^{1/3}n^{-1/3}
$$

Si on choisit tel $h$, lâ€™EQM serait asymptotiquement Ã©quivalent Ã  $g(x)n^{-2/3}$, oÃ¹ g est une fonction seulement dâ€™argument $x$ qui dÃ©pend de $f(x)$ et $f^\prime(x)$, et donc on dit que la vitesse de convergence est $n^{-2/3}$.

### Observations finales

Le choix de $h$ mentionnÃ© ci-dessus est juste utile en thÃ©orie, on ne pourra pas sâ€™en servir Ã©tant donnÃ© quâ€™on ignore $f(x)$ et $f^\prime(x)$. Il y a dâ€™autres maniÃ¨res proposÃ©es populaires pour choisir $h$.

Finalement, si $f$ est discontinue aux bornes de son support (voir la loi uniforme, la loi exponentielle ou la loi de Pareto) les rÃ©sultats dÃ©veloppÃ©s ne sont pas valables en ces bornes. De plus, si celles-ci sont inconnues, lâ€™histogramme pose problÃ¨me.

### Les estimateurs Ã  noyaux

Lâ€™une des caractÃ©ristiques de lâ€™histogramme est ne de pas Ãªtre continu ni lisse. Ici, on va proposer un autre estimateur de la densitÃ© qui est lisse.

### PrÃ©paration

Un noyau est juste une fonction rÃ©elle non nÃ©gative et intÃ©grable. En plus, dans le contexte dâ€™estimation fonctionnelle, on ajoute deux autres conditions :

- Lâ€™aire sous la fonction noyau est Ã©gale Ã  $1$.
Ceci est pour assurer que le noyau est lui-mÃªme une densitÃ©.
- La fonction noyau est symÃ©trique autour dâ€™une de ses valeurs $x$.

En plus, si la fonction $K$ est un noyau, la fonction $K^*$ dÃ©finie comme $K^*(u)=\lambda K(\lambda u)$ est aussi un noyau. Ceci nous permettra dâ€™appliquer un â€œscaleâ€ appropriÃ© aux donnÃ©es.

### Lâ€™estimateur lui-mÃªme

Maintenant, on suppose avoir tirÃ© un Ã©chantillon $[x_i]$ de taille $n$ dâ€™une population. On veut estimer la densitÃ© rÃ©elle avec lâ€™estimateur $\hat f$ dÃ©fini comme

$$
\hat f(x)=\frac{1}{nh} \sum_{i=1}^nK\left(\frac{x-x_i}{h}\right)
$$

oÃ¹ il nous reste le choix de h, appelÃ© la â€œfenÃªtre de comptageâ€ ou â€œbandwidthâ€ en anglais. 

Pour illustrer lâ€™objectif, supposons quâ€™on dÃ©finit K comme une gaussienne $\mathcal N(0,2.25)$ et quâ€™on tire un Ã©chantillon $[-2.1,-1.3,-0.4,1.9,5.1,6.2]$. Voyons câ€™e que Ã§a donne pour un histogramme $(h=2)$ et un estimateur Ã  noyau.

![untitled](new/uga/l3/s5/math/s5_math_stats_mathematiques_2/ressources/03_estimation_non_parametrique_et_fonctionnelle_untitled.png)

Notons que lâ€™estimateur somme toutes les courbes normaux dâ€™aire $1/6$ pour avoir finalement une courbe dâ€™aire $1$.

Intuitivement, on voudrait choisir un $h$ aussi petit comme les donnÃ©es permettent, mais ceci est un choix compliquÃ© Ã  faire et explorÃ© aprÃ¨s.

### Le choix de noyau

Le noyau ne doit pas forcÃ©ment Ãªtre une gaussienne. Il existe plusieurs choix de noyaux, chacun avec ses avantages. On appelle â€œsupportâ€ le domaine du noyau oÃ¹ $K(u)\ne 0$ et, si $u$ est dehors, donc le noyau lâ€™assigne $0$.

![untitled](new/uga/l3/s5/math/s5_math_stats_mathematiques_2/ressources/03_estimation_non_parametrique_et_fonctionnelle_untitled_1.png)

- Le premier et deuxiÃ¨me noyau sont simples, mais seul le noyau triangulaire fournit une estimation continue de la vrai densitÃ©.
- Le troisiÃ¨me a une optimalitÃ© thÃ©orique mais sans grand intÃ©rÃªt pratique.
- Le quatriÃ¨me est peut-Ãªtre le meilleur choix, car il a les avantage du noyau gaussien mais sont support nâ€™est pas infini Ã  diffÃ©rence du dernier. En plus, son avantage thÃ©orique est trÃ¨s proche du troisiÃ¨me noyau.

### Comportement asymptotique des estimateurs Ã  noyau

Avant de commencer, il faut une rÃ©Ã©criture utile de lâ€™estimateur noyau comme suit :

$$
\hat f(x)=\frac{1}{nh}\sum_{i=1}^nK\left(\frac{x-X_i}{h}\right) \iff\hat f(x)=\frac{1}{n}\sum_{i=1}^n Z_i, \text{ oÃ¹ }Z_i=\frac{1}{h}K\left(\frac{x-X_i}{h}\right) 
$$

Notons que lâ€™estimateur $\hat f$, qui est une variable alÃ©atoire, sera juste la moyenne de la somme des $Z_i$.

### Par rapport Ã  lâ€™espÃ©rance et le biais de $\hat f$

Il y a beaucoup de dÃ©veloppements dans le matÃ©riel, mais on saut jusquâ€™Ã  la fin pour comprendre le rÃ©sultat. Ici, $x$ est la variable indÃ©pendante de lâ€™estimateur $\hat f$, et $t$ la variable indÃ©pendante de la vrai densitÃ© $f$. On a donc, **pour un $x$ fixÃ©**,

$$
\begin{align*}
\mathbb E[\hat f(x)]-f(x)=\frac{h^2}{2}f^{\prime\prime}(x)\int_\R u^2K(u)du+o(h^2), \text{ oÃ¹ }u=\frac{x-t}{h}
\end{align*}
$$

Pour h petit le biais dÃ©pend donc de f(x) et du moment dâ€™ordre 2 du noyau. Le biais est du signe de fâ€(x) : si f est concave en x le biais est nÃ©gatif, si elle est convexe le biais est positif.

On voit que le biais se prÃ©sente tel quâ€™il sous-estime les maximums et sur-estime les minimums de la vrai densitÃ© $f$. La mÃ©thode tend Ã  Ã©crÃªter les creux et les pics de la densitÃ©, ce qui est un inconvÃ©nient majeur.

### Par rapport Ã  la variance et lâ€™EQM de $\hat f$

Pour la variance, on a que :

$$
\text{Var}(\hat f(x))=\frac{1}{nh}f(x)\int_\R K(u)^2du+O\left(\frac 1 n\right)
$$

Pour lâ€™erreur quadratique moyenne et **un $x$ fixÃ©**, on a que

$$
\begin{align*}
\text{EQM}(\hat f(x))
&=\frac{h^4}{4} \left(\int_\R u^2K(u)du\right)^2 f^{\prime\prime}(x)^2+\frac{f(x)}{nh}\int_\R K(u)^2du

\\[8pt]

&+o(h^4)
+O\left(\frac 1 n\right)
\end{align*}
$$

Faisant abstraction des termes o(h4) + O( 1n ) nÂ´egligeables dans les conditions de convergence, on voit que plus la largeur de fenË†etre h est faible plus le biais diminue mais plus la variance augmente et, inversement, lâ€™Â´elargissement de la fenË†etre augmente le biais et diminue la variance.

Si on prend telle expression pour cherche le h qui la minimise, telle h est dâ€™ordre n^{-4/5}, qui est plus vite que le n^{-2/3} de lâ€™EQM de lâ€™histogramme.

Un autre indicateur de performance Ã  considÃ©rer serait le EQIM : lâ€™erreur quadratique intÃ©grÃ©e moyenne. Bref, on prend lâ€™intÃ©grale de lâ€™EQM pour tous les $x$, et donc on nâ€™aura plus besoin de parler de â€œun $x$ fixÃ©â€.

$$
\begin{align*}
\text{EQIM}(\hat f)&=\int_\R\mathbb E\left[\left(\hat f(x)-f(x)\right)^2\right]dx=\int_\R\text{EQM}(\hat f(x))dx

\\[10pt]

&=\frac{h^4}{4}

\left(\int_\R u^2K(u)du\right)^2

\int_\R f^{\prime\prime}(x)^2dx

+\frac{1}{nh}\int_\R K(u)^2du

\\[8pt]

&+o(h^4)
+O\left(\frac 1 n\right)
\end{align*}
$$

De mÃªme, si on prend telle expression pour cherche le $h$ qui la minimise, telle $h$ est dâ€™ordre $n^{-1/5}$, qui est plus vite que le $n^{-2/3}$ de lâ€™EQIM de lâ€™histogramme si on lâ€™aurait calculÃ©e.

Ainsi, **en tant quâ€™estimateur fonctionnel, un estimateur a noyau converge plus vite vers la vraie densitÃ© f que lâ€™histogramme**. Par contre, ce rÃ©sultat repose sur un choix optimal trÃ¨s thÃ©orique (puisque dÃ©pendant de lâ€™inconnue $f$) et de conditions de convergence artificielles. Câ€™est pourquoi nous considÃ©rons maintenant les aspects pratiques.

### Choix pratiques du noyau $K$ et fenÃªtre $h$

Pour le choix de $K$, câ€™est plutÃ´t indiffÃ©rent si on se concentre Ã  minimiser lâ€™EQIM. Ceci a pu Ãªtre constatÃ© par calcul direct ou par simulation sur une grande variÃ©tÃ© de lois mÃ¨res et est dâ€™ailleurs confirmÃ© sur lâ€™expression asymptotique ci-dessus. Le noyau de Tukey est recommandÃ© par son quasi-optimalitÃ© et son avantage de lissage.

Le choix de $h$ est bien plus difficile et moins satisfaisante : la plupart des propositions de choix de h reposent sur lâ€™optimisation de lâ€™EQIM (ou sur lâ€™un des critÃ¨res mentionnÃ©s plus loin, mais aucun nâ€™est la panacÃ©e) et, par expÃ©rience, on constate souvent quâ€™elles ne fournissent pas nÃ©cessairement une estimation graphiquement satisfaisante, laissant subsister des variations locales (tendance a sous-lisser).

La mÃ©thode la plus sÃ»re reste donc celle des essais et erreurs oÃ¹, partant dâ€™une valeur de toute Ã©vidence trop faible de h donnant des fluctuations locales indÃ©sirables, on augmente progressivement cette valeur jusquâ€™au seuil de disparition de telles fluctuations.

MÃªme si les expressions asymptotiques sont Ã  prendre avec prÃ©caution, elles permettent de vÃ©rifier sur diverses lois que la mÂ´ethode des noyaux, outre le fait quâ€™elle peut donner une estimation lisse, est nettement plus efficace, au sens de lâ€™EQIM, que lâ€™histogramme.

## Estimation de la rÃ©partition

### La rÃ©partition empirique $F_n$, et lâ€™utiliser comme estimateur $\hat F_n$

Pour estimer la vraie fonction de rÃ©partition $F$, on utilisera la rÃ©partition **empirique** $F_n$, **mÃªme si câ€™est une loi continue** ! Ici, $I$ est une fonction indicatrice, qui vaut $1$ si la valeur rÃ©alisÃ© de $X_i$ se trouve dans $(-\infin,x]$ et vaut $0$ sinon.

$$
F_n(x)=\frac 1 n \sum_{i=1}^nI_{(-\infin,x]}(X_i)
$$

Notons que la fonction de rÃ©partition empirique nâ€™est pas forcÃ©ment un estimateur, donc pas besoin dâ€™Ã©crire un chapeau sur $F_n$ que du moment oÃ¹ on lâ€™utilise comme estimateur $\hat F_n$.

Visuellement, câ€™est une fonction en escalier qui augmente de $1/n$ chaque fois que la valeur de la variable indÃ©pendante $x$ rencontre une valeur rÃ©alisÃ©e $x_i$.  

Notons que les $X_i$ sont des v.a. iid., et donc de les passer par la fonction indicatrice (une v.a. de Bernoulli masquÃ©e avec $p=F(x)$) et finir avec des $I(X_i)$ ne change pas le fait quâ€™elles sont des v.a. iid. En plus, on est en train de les additionner, donc une somme de v.a. iid. On peut donc Ã©voquer le TCL, qui nous  mÃ¨ne a des faits remarquables :

Si on laisse $x$ fixÃ© a une valeur arbitraire,

$$
n\hat F_n(x)\sim\mathcal B(n,F(x)) \implies \lim_{n\rightarrow\infin} \hat F_n(x)\sim\mathcal N\left(F(x),\frac{F(x)(1-F(x))}{n}\right)
$$

**Ceci est juste variable si $n$ est grand**. Notons donc, $\hat F_n(x)$ est asymptotiquement sans biais et la variance tend vers zÃ©ro. Donc, le MSE tend vers zÃ©ro, et donc $\hat F_n(x)$ est un estimateur convergent de $F(x)$. 

En plus, on peut construire un intervalle de confiance pour $p=F(x)$ tout de mÃªme comme on le ferait dans le cas dâ€™une binomiale simple â€œde manuelâ€.

Finalement et le plus important, la rÃ©partition empirique comme estimateur $\hat F_n$ est un estimateur sans biais, convergent de $F(x)$ et le maximum de vraisemblance de $F$. Ce dernier rÃ©sultat rÃ©sulte trop compliquÃ©, mais on dira juste que $\hat F_n$ est la fonction qui maximise la probabilitÃ© dâ€™observer un Ã©chantillon rÃ©alisÃ©.

$$
\hat F_n : \max_F L(F|[x_i])=\prod_{i=1}^nF^\prime(x_i)
$$

### Construire un intervalle de confiance pour chaque valeur $F(x)$

Avant, on voudrait montrer la convergence uniforme de $\hat F$ vers $F$. Ce thÃ©orÃ¨me est celui de Glivenko-Cantelli, qui no dit que la diffÃ©rence entre la rÃ©partition rÃ©elle et son estimateur tend vers 0 si on augmente la taille de lâ€™Ã©chantillon.

$$
\lim_{n\rightarrow\infin}\left(\sup_{x\in\R}|\hat F_n(x)-F(x)|\right)=^\text{p.s.}0
$$

GrÃ¢ce Ã  Kolmogorov-Smirnov, la borne $\sup$ de la diffÃ©rence est utilisÃ©e pour crÃ©er lâ€™intervalle de confiance de $F(x)$ comme suit

$$
\text{Soit la statistique }D_n=\sup_{x\in\R}|\hat F_n(x)-F(x)|. \text{ Donc, }

\\[8pt]

\lim_{n\rightarrow\infin}\mathbb P(\sqrt{n}D_n<x)=
\underbrace{1-2\sum_{k=1}^\infin(-1)^{k-1}e^{-2{(kx)}^2}}_{G(x)}
$$

Notons que $G(x)$ est la rÃ©partition de la statistique $\sqrt{n}D_n$. La fonction $G(x)$ est vraiment puissante, du fait quâ€™elle ne dÃ©pend pas de la vrai rÃ©partition de dÃ©part $F(x)$, mÃªme pour $n$ fini. De ce fait, elle a Ã©tÃ© tabulÃ©e. Ã€ partir de $n = 40$, lâ€™approximation par $G(x)$ est correcte Ã  $10^{âˆ’2}$ prÃ¨s. En plus, son quantile de $95\%$ est aussi tabulÃ©. Voyons,

$$
\mathbb P(\sqrt{n}\sup_{x\in\R}|\hat F_n-F(x)|< g_{95\%})\simeq95\% \text{ Mais, voyons que }

\\[10pt]

\sqrt{n}\sup_{x\in\R}|\hat F_n-F(x)|< g_{95\%}\iff\forall x \in\R, \sqrt{n}|\hat F_n-F(x)|< g_{95\%}

\\[8pt]

\text{Finalement, } IC_{95\%}(F(x))=\left[\hat F_n(x)-\frac{g_{95\%}}{\sqrt{n}}, \hat F_n(x)+\frac{g_{95\%}}{\sqrt{n}}\right]
$$

En plus, pour la fonction $G(x)$ on peut faire une simplification : si $x>0.8$ et $n=40$, $G(x)$ vaut presque juste $1-2e^{-2{x}^2}$. Ã‰tant donnÃ© quâ€™on sâ€™intÃ©resse quand $x=0.95$, $G(x)\approx 1.36$.

$$
IC_{95\%}(F(x))=\left[\hat F_n(x)-\frac{1.36}{\sqrt{n}}, \hat F_n(x)+\frac{1.36}{\sqrt{n}}\right]
$$

### Lissage de $F_n$

TrÃ¨s pareil Ã  la section des estimateurs Ã¢ noyau, on dÃ©finit un nouveau estimateur $\hat F_n$ diffÃ©rent de la rÃ©partition empirique $F_n$. Cette estimateur sera une intÃ©grale dâ€™un estimateur Ã  noyau $\hat f_n$ :

$$
\begin{align*}
\hat F_n(x) &= \int_{-\infin}^x\hat f_n(t)dt=\int_{-\infin}^x\frac{1}{nh}\sum_{i=1}^nK\left( \frac{t-x_i}{h}\right)dt

\\

&=\frac{1}{nh}\sum_{i=1}^n\int_{-\infin}^xK\left( \frac{t-x_i}{h}\right)dt

\\

&= \frac 1 n \sum_{i=1}^n\int_{-\infin}^{\frac{x-x_i}{h}}K(v)dv, \text{ oÃ¹ }v=\frac{t-x_i}{h}
\end{align*}
$$

Notons que, dans la derniÃ¨re ligne, on a mis le $1/h$ Ã  lâ€™intÃ©rieur de lâ€™intÃ©grale, puis il disparaÃ®t avec le changement de variable.

DÃ©finissons  le â€œnoyau intÃ©grÃ©â€ comme $H$ et on aura un meilleure Ã©criture de $\hat F_n$

$$
H(u)=\int_{-\infin}^uK(x)dx \implies \hat F_n(x)=\frac 1 n\sum_{i=1}^nH\left( \frac{x-x_i}{h}\right)
$$

Ici, comme pour la densitÃ©, se pose le problÃ¨me de la largeur de fenÃªtre optimale. Il est toutefois moins crucial en raison de la plus faible sensibilitÃ© de lâ€™estimation a ce paramÃ¨tre de lissage.

Par exemple, on sâ€™Ã©tait dit que le noyau de Tukey est le plus pratique. Sa version intÃ©grÃ©e serait donc

$$
H(u)=
\begin{cases}
0,\text{ si }u\le-1
\\
\frac{1}{16}(3u^5-10u^3+15u+8), \text{ si } -1\le u\le1
\\
1, \text{ si } 1 \le u
\end{cases}
$$

Lorsquâ€™on examine le graphe obtenu avec diffÃ©rents noyaux, on constate que la diffÃ©rence est imperceptible. Ceci sâ€™explique par le fait que lâ€™estimation dâ€™une fonction de rÃ©partition est fortement contrainte par la condition de croissance de $0$ Ã  $1$, et par sa continuitÃ© Ã  droite. De ce fait, le problÃ¨me est beaucoup plus simple que pour la densitÃ©.

En particulier, la croissance implique de faibles courbures et donc peu ou pas de problÃ¨me de biais, contrairement a la densitÃ©. Il nâ€™y a donc pas dâ€™avantage tangible Ã  utiliser des noyaux ou autres instruments de lissage sophistiquÃ©s et nous prÃ©conisons donc lâ€™emploi du noyau de Tukey.

Notons bien, toutefois, que les estimations de densitÃ©s obtenues par dÃ©rivation seront,
quant Ã  elles, trÃ¨s sensibles aux variations jugÃ©es mineures pour la fonction de rÃ©partition. MalgrÃ© ce constat, il nâ€™est pas inutile dâ€™examiner le biais et la variance, de faÃ§on asymptotique, comme cela a Ã©tÃ© fait pour la densitÃ©.

### Comportement asymptotique du lissage de $F_n$

Pour la suite, on suppose un noyau de Rosenblatt, mais les rÃ©sultats sont valables pour les noyaux les plus courants.

### Par rapport Ã  lâ€™espÃ©rance et au biais

Pour le biais, on trouve que

$$
\mathbb E[\hat F_n(x)-F(x)]=\frac {h^2}2 f\prime(x)\int_\R u^2K(u)du+o(h^2)
$$

Si bien la rÃ©partition empirique $F_n$ est sans biais, un lissage $\hat F_n$ introduit un faible biais. En fait, ce biais sâ€™annule aux extrema de la vrai rÃ©partition $F$.

### Par rapport Ã  la variance et lâ€™EQM

Pour la variance, on a que

$$
\text{Var}(\hat F_n(x))=\frac{1}{n}\left(
F(x)[1-F(x)]
+hf(x)\left[\int_{-1}^1H^2(u)du-1\right]+o(h)
\right)
$$

On retrouve dans le premier terme de son expression asymptotique la variance de $F_n$. Par consÃ©quent, on gagne sur la variance de $F_n$ si le deuxiÃ¨me terme est nÃ©gatif, soit  $\left[\int_{-1}^1H^2(u)du\right]<1$, ce qui est vÃ©rifiÃ© pour les noyaux intÃ©grÃ©s courants.

Pour ce qui concerne lâ€™erreur quadratique moyenne, elle sera amÃ©liorÃ©e si la diminution de variance compense le biais au carrÃ©.