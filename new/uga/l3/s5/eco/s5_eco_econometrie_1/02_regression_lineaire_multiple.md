# 02 // r√©gression lin√©aire multiple

[√âconom√©trie 1 - Chap. #2](2_chapitre2_econometrie1_20232024.pdf)

# Le mod√®le

## Rappel : les cinq hypoth√®ses fondamentales

Par la suite, on va supposer les cinq hypoth√®ses ‚Äùfondamentales‚Äù qui suivent :

- $H_1$ : $\mathbb E [u_i] = 0$
- $H_2$, la homosc√©dasticit√© ou variance constante : $\text{Var}(u_i)=\mathbb E[u_i^2]=\sigma^2_u$
- $H_3$,: la variable explicative $x_i$ est non al√©atoire
- $H_4$, sp√©cificit√© : le mod√®le est correctement sp√©cifi√©.
Dans ce cas c‚Äôest lin√©aire, donc $y_i = \beta_0+\beta_1x_i+u_i$.
- $H_5$, non colin√©arit√©: la variable explicative $x_i$ n‚Äôest pas constante pour toutes les observations.

<aside>
‚ö†Ô∏è Les estimateurs $\hat \beta_0$ et $\hat \beta_1$ peuvent √™tre biais√©s. La source normalement vient du non respect du mod√®le sp√©cifie (utiliser $\ln x$ quand √ßa devrait √™tre $x$, ou vice-versa) ou de H3 : $\mathbb E [\bold x u] \ne 0$ donc $\text{Cov}(\bold x, u) \ne 0$.

</aside>

Sous ces hypoth√®ses, il d√©coule que : 

- Les estimateurs $\hat Œ≤_0$ et $\hat Œ≤_1$ des MCO sont **sans biais et a variance minimale**, aussi appel√©s les estimateurs de Gauss-Markov**.** Donc :
    
    $$
    \mathbb E[\hat\beta_0]=\beta_0 \text{\hspace{8pt}et\hspace{8pt}}\mathbb E[\hat\beta_1]=\beta_1 
    $$
    
- Les variances sont les suivantes, mais il nous manque un param√®tre $\sigma^2_u$.
    
    $$
    \text{Var}(\beta_1)=\frac{\sigma^2_u}{\sum_{i=1}^N(x_i-\bar x)^2}
    
    \text{\hspace{8pt}et\hspace{8pt}}
    
    \text{Var}(\beta_0)=\frac{\sigma^2_u}{n}\frac{\sum_{i=1}^Nx_i^2}{\sum_{i=1}^N(x_i-\bar x)^2}
    $$
    
- Si on prend la variances des r√©sidus, ce serait un estimateur biais√© de $\sigma^2_u$.
N√©anmoins, Il existe un estimateur sans biais de $\sigma^2_u$, o√π $k$ le nombre de var. explicatives. Dans ce cas, $k=1$.
    
    $$
    \hat\sigma^2_u=\frac{\sum_{i=1}^n\hat u_i^2}{N-(k+1)} \implies \mathbb E[\hat\sigma^2_u]=\sigma^2_u
    $$
    

## G√©n√©ralisation √† $n$ variables explicatives, changement des $H_i$

C‚Äôest juste l‚Äôinclusion de plus de variables explicatives dans le mod√®le :

$$
y_i=\beta_0+\beta_1x_{i1}+\beta_2 x_{i2}+\cdots+\beta_k x_{ik}+u_i
$$

- $y_i$ : observation individuelle de la variable √† expliquer pour l‚Äôindividu $i$
- $k$ : le nombre de variables explicatives du mod√®le
- $x_{ik}$ : les $k$ variables explicatives qui correspondent √† l‚Äôindividu $i$
- $N$ : nombre d‚Äôobservations
- Indice $i$ : pour les individus.
Fixer un $i$ est de se fixer sur un individu (coupe transversale)
- Indice $t$ : pour les p√©riodes temporelles.
Fixer un $t$ est de voir une photographie des individus au moment $t$.

Pour √©crire l‚Äô√©quation qui tient en compte de tous les individus $i$, on √©crit sous forme matricielle :

$$
\bold y=\bold X\beta+\bold u

\\[10pt]

\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{bmatrix}

=

\begin{bmatrix}
1&x_{11}&x_{12}& \cdots &x_{1k} \\
1&x_{21}&x_{22}& \cdots &x_{2k} \\
\vdots&\vdots&\vdots&\ddots&\vdots \\
1&x_{N1}&x_{N2}& \cdots &x_{Nk} \\
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta _1 \\
\vdots \\
\beta_k
\end{bmatrix}
+
\begin{bmatrix}
u_1 \\
u_2 \\
\vdots \\
u_n
\end{bmatrix}
$$

<aside>
üìñ Le vecteur $\hat\beta$ qui contient les estimateurs des MCO de $\beta$ et qui r√©sous ce syst√®me d‚Äô√©quations lin√©aires utilise la [pseudo-inverse](https://en.wikipedia.org/wiki/Generalized_inverse) ou l‚Äôinverse de [Moore-Penrose](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) de $\bold X$ (une g√©n√©ralisation de l‚Äôinverse pour les matrices pas forc√©ment carr√©s. Elle est √©gale √† l‚Äôinverse r√©guli√®re si la matrice est carr√©).

$$
\hat\beta=(\bold X^T \bold X)^{-1}\bold X^T\bold y
$$

</aside>

Les cinq hypoth√®ses du cas lin√©aire simple sont l√©g√®rement chang√©es dans le cas g√©n√©ral :

- $H_1$ : $\mathbb E [u_i] = 0$
- $H_2$, variance constante : $\text{Var}(\bold u)=\mathbb E[\bold u\bold u ^T]=\sigma^2_u I_n$.
- $H_3$,: la matrice $\bold X$ est non al√©atoire.
- $H_4$, sp√©cificit√© : le mod√®le est correctement sp√©cifi√©.
Dans ce cas c‚Äôest lin√©aire, donc $\bold y = \beta \bold X + \bold u$.
- $H_5$, non colin√©arit√©: la matrice $\bold X$ est de plein rang, c√†d $k+1 < n$.
Comme rappel, $\bold X$ est de dimension $(n, k+1)$.

# L‚Äôestimateur des Moindres Carr√©s Ordinaires (MCO)

## Pr√©paration : d√©riv√©es pour les vecteurs et scalaires

[Par rapport aux d√©riv√©s des vecteurs et matrices](https://en.wikipedia.org/wiki/Matrix_calculus#Layout_conventions), il existe trois pr√©sentations de d√©riv√©e avec vecteurs: la d√©riv√© vecteur-par-scalaire, la d√©riv√© scalaire-par-vecteur et la d√©riv√© vecteur-par-vecteur. Les d√©riv√©s avec matrice d√©coulent facilement √† partir de celles avec vecteurs.

- Dans la suite, un vecteur contient plusieurs fonctions de x (y compris $\text{Id}(x)=x$ elle-m√™me) et un scalaire est juste une fonction de x (qui peut √™tre aussi ).
    - (En fait, un vecteur est une matrice colonne et un scalaire est un matrice ligne, mais pour le scalaire on suppose que il n‚Äôa qu‚Äôune seule entr√©e, donc c‚Äôest une matrice ligne $(1\times1)$ d‚Äôune seule entr√©e et donc une seule valeur)
- Le premier composant est la variable d√©pendante et la deuxi√®me la variable ind√©pendante, donc si on parle de la d√©riv√© vecteur-par-scalaire, un vecteur est une variable d√©pendante et le scalaire la variable d√©pendante.

La d√©riv√© d‚Äôun vecteur $\bold y$ par rapport √† un scalaire $x$ est utilis√© dans le cas o√π  contient √† chaque entr√©e des fonctions de  diff√©rentes. On prend donc la d√©riv√© partielle de chaque fonction par rapport √† . Ceci nous donne aussi le ‚Äúvecteur tangent‚Äù. Si on suppose qu‚Äôn est en physique, est  sont les coordonn√©s de position, le vecteur tangent montre la vitesse vers chaque direction.

$$
\frac{\partial \bold y}{\partial x}=
\begin{bmatrix}
\frac{\partial y_1}{\partial x}
\\[8pt]
\frac{\partial y_2}{\partial x}
\\[8pt]
\vdots
\\[8pt]
\frac{\partial y_m}{\partial x}
\end{bmatrix}
$$

![Pour chaque point de la position en 2 dimensions, on a le vecteur tangent (qui repr√©sente la vitesse) et un vecteur normal qui n‚Äôa pas d‚Äôimportance ici.](new/uga/l3/s5/eco/s5_eco_econometrie_1/s5_eco_econometrie_1/02_regression_lineaire_multiple_acbe670dd69d4e1b83e97400c1e52272/untitled.png)

Pour chaque point de la position en 2 dimensions, on a le vecteur tangent (qui repr√©sente la vitesse) et un vecteur normal qui n‚Äôa pas d‚Äôimportance ici.

Apr√®s, on a la d√©riv√© d‚Äôun scalaire par rapport √† un vecteur, o√π les entr√©es du vecteur sont les arguments du scalaire $y=f(x_1,\cdots, x_n)$. Le r√©sultat prend la d√©riv√© partielle du scalaire par rapport √† chaque variable ind√©pendante. On note que le gradient est un cas de d√©rive vecteur-par-scalaire.

$$
\frac{\partial y}{\partial \bold x}=
\begin{bmatrix}
\frac{\partial y}{\partial x_1}
\\[8pt]
\frac{\partial y}{\partial x_2}
\\[8pt]
\vdots
\\[8pt]
\frac{\partial y}{\partial x_n}
\end{bmatrix}
=\nabla y
$$

Finalement, la d√©riv√© d‚Äôun vecteur $\bold y$ par rapport √† autre vecteur $\bold x$ suppose que chaque entr√© de  est une fonction √† plusieurs variables qui se trouvent dans . Pour chaque ligne, on d√©rive un fonction $y_i$ par rapport √† toutes ses variables ind√©pendantes dans  (c√†d, ligne $1$ contient le gradient de $y_1$, ligne $2$ le gradient de $y_2$, etc). Concr√®tement, l‚Äôentr√©e $ij$ est la d√©riv√© de la fonction $i$ par rapport √† la variable ind√©pendante $j$.

$$
\frac{\partial \bold y}{\partial \bold x} =
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & 
\frac{\partial y_1}{\partial x_2} &
\cdots &
\frac{\partial y_1}{\partial x_n}

\\[8pt]

\frac{\partial y_2}{\partial x_1} & 
\frac{\partial y_2}{\partial x_2} &
\cdots &
\frac{\partial y_2}{\partial x_n}

\\[8pt]

\vdots & \vdots & \ddots & \vdots

\\[8pt]

\frac{\partial y_m}{\partial x_1} & 
\frac{\partial y_m}{\partial x_2} &
\cdots &
\frac{\partial y_m}{\partial x_n}
\end{bmatrix}
$$

## Comment calculer les MCO

Les MCO sont juste les coefficients d‚Äôune r√©gression lin√©aire mais appliqu√© sur un √©chantillon, dans l‚Äôespoir que ces valeur seront pas si √©loign√©es des vraies valeurs qu‚Äôon obtiendrait si on appliquait la r√©gression lin√©aire sur toute la population.

Partons du terme SCE pour trouver ce qui nous int√©resse :

$$
SCE=\sum_{i=1}^n (y_i-\hat y_i)^2= \sum_{i=1}^n √ª^2_i
$$

Voyons que maintenant on traite avec un vecteur d‚Äôerreurs $\bold u$. Pour que chaque entr√©e devienne $\hat u^2_i$ on peut √©crire une op√©ration avec $\bold u$ comme suit : 

$$
\bold u^T \bold u = \begin{bmatrix}
\hat u^2_1 \\
\hat u^2_2 \\
\vdots \\
\hat u^2_n
\end{bmatrix}
$$

Et finalement, on veut minimiser tel vecteur.

$$
\min_\beta(\bold u^T \bold u) 

\iff

\beta : \frac{\partial}{\partial \beta} (\bold u^T \bold u) = 0 

\text{ et }

\frac{\partial^2}{\partial \beta^2} (\bold u^T \bold u) > 0

\\[10pt]

\begin{align*}
\min_\beta (\bold u^T \bold u)
&=
\min_\beta (\bold y - \bold X\beta)^T(\bold y - \bold X\beta) 
\\
&=
\min_\beta (\bold y ^T \bold y - 2\bold y^T\bold X \beta+\beta^T\bold X^T\bold X \beta)

\end{align*}
$$

On applique la condition de premier ordre √† l‚Äôexpression pour arriver au r√©sultat. On omet la condition de deuxi√®me ordre.

$$
\begin{align*}
&\frac{\partial}{\partial\beta} (\bold y ^T \bold y - 2\bold y^T\bold X \beta+\beta^T\bold X^T\bold X \beta) = 0
\\
\iff
&-2\bold X^T\bold y + 2 \bold X^T \bold X\hat \beta =0
\\
\iff

&\bold X^T \bold X \hat\beta = \bold X^T \bold y
\\
\iff
&\hat \beta = (\bold X^T \bold X)^{-1}\bold X^T \bold y, \text{ sous } H_5
\end{align*}
$$

---

Pour faire le passage de la premi√®re ligne √† la deuxi√®me ligne, voici quelques op√©rations avec d√©riv√©es, vecteur et scalaires √† savoir, sorti du PDF dessous, page 12/61.

[Ch3slides-multiple-linear-regression.pdf](ch3slides-multiple-linear-regression.pdf)

Pour ce premier, notons on fait une combinaison lin√©aire de coefficients $\bold a$. Le r√©sultat final de l‚Äôop√©ration $\bold a^T \bold x$ est juste $[a_{j=1}x_{i=1}+a_{j=2}x_{i=2}+\cdots+a_{j=m}x_{i=n}]$. Si on d√©rive cette expression par $\bold x$ (scalaire-par-vecteur), on devrait avoir de retour le vecteur avec les coefficients, donc $\bold a$.  

$$
\begin{align*}
&\frac{\partial}{\partial \bold x}(\bold x^T \bold a)= \frac{\partial}{\partial \bold x} (\bold a^T\bold x)=\bold a
\end{align*}
$$

Pour ce deuxi√®me, notons que l‚Äôexpression qui r√©sulte de $\bold x^T \bold x$ (comb. lineaire) est √©gale √† $[x_1^2+x_2^2+\cdots+x_n^2]$. La d√©riv√© de cette expression devrait nous retourner $2x_i$ pour l‚Äôentr√©e $i$, donc $2\bold x$.

$$
\begin{align*}
&\frac{\partial}{\partial \bold x}(\bold x^T \bold x) = \frac{\partial}{\partial \bold x}(||\bold x||^2)=2\bold x
\end{align*}
$$

Pour ce troisi√®me, la forme de chaque entr√©e de $\bold A \bold x$ tout seul est un vecteur colonne$(\sum_{i,j=1}^{i=n, j=m} a_{ij}x_i)$. Apr√®s, la forme de chaque entr√©e de $\bold x^T(\bold A \bold x)$ est donc $(\sum_{i,j=1}^{i=n, j=m} a_{ij}x_i^2)$. Si on d√©rive chaque terme par $x_i$, on finit avec le vecteur colonne $(\sum_{i,j=1}^{i=n, j=m} 2a_{ij}x_i)$, donc $2\bold A\bold x$.

$$
\begin{align*}
&\frac{\partial}{\partial \bold x}

(\bold x^T \bold A \bold x)=2\bold A\bold x
\end{align*}
$$

Finalement, on utilise un raisonnement pareil pour ce dernier.

$$
\begin{align*}
&\frac{\partial}{\partial \bold x}(\bold x^T \bold B^T \bold B \bold x)=\frac{\partial}{\partial \bold x}(||\bold B\bold x||^2)=2 \bold B^T \bold B \bold x
\end{align*}
$$

## Variance des MCO

La variance de la matrice des estimateurs $\hat B$ est √©gale √† la matrice de covariance entre les estimateurs $\hat\beta_i$ et $\hat\beta_j$, pour $k$ variables explicative.

$$
\text{Var}(\hat\beta)=\hat{\sigma^2_u}(\bold X^T\bold X)^{-1}=
\begin{bmatrix}

\text{Var}(\hat \beta_0) & \text{Cov}(\hat\beta_0, \hat\beta_1) & \cdots & \text{Cov}(\hat\beta_0, \hat\beta_k)

\\

\text{Cov}(\hat\beta_1, \hat\beta_0) & \text{Var}(\hat \beta_1) & \cdots & \text{Cov}(\hat\beta_1, \hat\beta_k)

\\

\vdots&\vdots&\ddots&\vdots

\\

\text{Cov}(\hat\beta_k, \hat\beta_1) &\text{Cov}(\hat\beta_k, \hat\beta_2) &\cdots &\text{Var}(\hat \beta_k)

\end{bmatrix}
$$

Comme rappel, l‚Äôestimateur des √©carts-types des perturbations $u$ est :

$$
\hat{\sigma^2_u}=\frac{\hat{\bold{u}}^T\hat{\bold{u}}}{n-(k+1)}=\frac{SCR}{n-(k+1)}
$$

# Hypoth√®ses et propri√©t√©s

## Esp√©rance et variance de $\hat \beta$

Preuve que $\hat \beta$ est un estimateur sans biais de $\beta$ :

![untitled](new/uga/l3/s5/eco/s5_eco_econometrie_1/02_regression_lineaire_multiple/untitled_1.png)

Calcul de la variance de $\hat \beta$ :

![untitled](new/uga/l3/s5/eco/s5_eco_econometrie_1/02_regression_lineaire_multiple/untitled_2.png)

## Estimation de la variance des perturbations $\sigma^2_u$

Tout comme dans le cas de r√©gression simple, $\bold u$ n‚Äôest pas observable. Donc, on estime avec $\hat {\bold u}$. Avant, on se pr√©pare avec le calcul d‚Äôune certaine matrice $\bold M$ telle que :

![untitled](new/uga/l3/s5/eco/s5_eco_econometrie_1/02_regression_lineaire_multiple/untitled_3.png)

Le $T$ dans $I_T$ est le $n$ dans les dimensions $(n, k+1)$ de la matrice $\beta$. Avec cette d√©finition de M, on peut dire que :

- M est sym√©trique : $\bold M=\bold M^T$
- M est idempotente : $\bold M \bold M^T=\bold M^T \bold M$
- $\text{Rg}(\bold M)=\text{tr}(\bold M)=N-k$

Finalement, on peut calculer $\hat{\bold u}$ :

![untitled](new/uga/l3/s5/eco/s5_eco_econometrie_1/02_regression_lineaire_multiple/untitled_4.png)

Et donc, finalement :

$$
\hat {\bold u}^T \hat {\bold u}=\bold u^T \bold M^T \bold M \bold u=\bold u^T \bold M \bold u 
$$

# Qualit√© de l‚Äôajustement

## Rappel : analyse de la variance et $R^2$

La populaire √©quation de la variance, qui d√©compose la variance comme la somme de deux termes, est la suivante :

$$
\underbrace{\sum_{i=1}^N(y_i-\bar y)^2}_{SCT}=\underbrace{{\sum_{i=1}^N(\hat y_i-\bar y)^2}}_{SCE}+\underbrace{\sum_{i=1}^N(y_i-\hat y_i)^2}_{SCR}

\\[8pt]

\text{Remarque : } \sum_{i=1}^N(y_i-\hat y_i)^2=\sum_{i=1}^N(\hat u_i - \cancel{\bar{\hat u}}^{\space0})^2
$$

O√π $SCT$ est la somme des carr√©s totaux, $SCE$ est la somme des carr√©es expliques, et $SCR$ est la somme des carr√©s des r√©sidus.

Avec cette terminologie, on rappelle le coefficient de d√©termination :

$$
R^2=\frac{SCE}{SCT}=1-\frac{SCR}{SCT}
$$

- $R^2 = 1$ : si tous les points correspondant aux donn√©es se trouvent sur la droite d‚Äôajustement.
- $R^2 = 0$ : les variations entre les $\bar y_i$ ne capturent quasiment rien de la variation observ√©e entre les $y_i$.
- Remarque : un faible $R^2$ n‚Äôimplique pas forc√©ment que la r√©gression des MCO ne sert √† rien, mais que d‚Äôautres ‚Äúprobl√®mes‚Äù peuvent expliquer ce r√©sultat.

## Ajustement de $R^2$

Un fait important par rapport √† $R^2$ est que sa valeur augmente si le nombre de variables explicatives augmente. Ceci ne nous est pas pratique, car on ne pourra pas savoir si l‚Äôune des variables explicatives a un impacte presque nul sur la variable √† pr√©dire.

Pour $R^2$, toute variable explicative nous rapproche de la meilleure pr√©diction, ce qui n‚Äôest pas d√©sirable, car on veut distinguer entre les variables explicatives qui sont des bons pr√©dicteurs et celles qui ne sont pas.

La mani√®re populaire d‚Äôajuster $R^2$ est comme $\bar{R^2}$, un $R^2$ ajust√© aux degr√©s de libert√© :

$$
\bar{R^2}=R^2-\frac{k-1}{n-k}(1-R^2)=1-\frac{SCR/(n-k)}{SCT/(k-1)}
$$

# Inf√©rence statistique

## Test statistiques

La grande diff√©rence de ce cas avec le cas de regression simple c‚Äôest que un estimateur $\beta_i$ pourrait ne pas √™tre significatif individuellement (sous un test de Student) mais le mod√®le en tout pourrait √™tre significatif globalement (sous un test de Fisher).