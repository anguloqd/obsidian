# 04 // matrices

# Op√©rations basiques entre deux matrices

## Addition de matrices

<aside>
ü§∑‚Äç‚ôÇÔ∏è En vrai, je connaissais d√©j√† que c‚Äôest la somme entr√©e par entr√©e entre deux matrices.

</aside>

## Multiplication de matrices

<aside>
ü§∑‚Äç‚ôÇÔ∏è Pareil, je savais d√©j√† que le produit entre deux matrices est le produit matriciel ou produit *point* de chaque ligne de la matrice gauche avec chaque colonne de la matrice droite.

</aside>

<aside>
üí° Pi√®ges communes :

- Le produit n'est pas commutatif en g√©n√©ral. Juste avec une matrice et son inverse, et aussi une matrice et l'identit√©.
- $AB = 0$ n'implique pas $A = 0$ ou $B = 0$.
- $AB = AB$ n'implique pas $B = C$. On peut avoir $AB = AC$ et $B \ne C$.
</aside>

Propri√©t√©s :

- Associativit√© : $A(BC) = (AB)C$
- Distributivit√© : $A(B + C) = AB + AC \text{ et } (B + C)A = BA+ CA$.
- $A \cdot 0 = 0 \text{ et } 0 \cdot A = 0$.

# **Formule du bin√¥me**

## Lorsque $AB=BA$‚Ä¶

Comme la multiplication n‚Äôest pas commutative, les identit√©s binomiales usuelles sont fausses. En particulier, $(A + B)^2$ ne vaut en g√©n√©ral pas $A^2 + 2AB + B^2$, mais on sait seulement que $(A+B)^2 = A^2 + AB + BA+ B^2$.

Si $AB = BA$ (si $A$ et $B$ commuent sous la multiplication), alors la formule du bin√¥me applique. Dans la pratique, l‚Äôune des deux matrices est souvent la matrice identit√© qui commute avec tout.

# **Inverse d'une matrice**

## $A^{-1}$ telle que $A A^{-1}=I$

Plus g√©n√©ralement, quand $A$ est inversible, pour tout $p ‚àà N$, on note $A^{-p} = (A^{-1})^p$.

- L‚Äôensemble des matrices inversibles de $M_n(K)$ est not√© $G L_n (K)$.

**Simplification par une matrice inversible.** Soient $A$ et $B$ deux matrices de $M_n(K)$ et $C$ une matrice inversible de $M_n(K)$. Alors l‚Äô√©galit√© $AC = BC$ implique l‚Äô√©galit√© $A = B$ (on multiplie par la droite par $C^{-1}$ aux deux c√¥t√©s). 

- Pour les matrices 2x2, la formule est :

$$
A = \begin{pmatrix} a &b \\ c & d \end{pmatrix} \iff A^{-1}=\frac{1}{ad-bc} \begin{pmatrix} d &-b \\ -c & a \end{pmatrix}
$$

- Pour tout autre matrice, fais la m√©thode de Gauss-Jordan.

# **Types de matrices**

Avant de les pr√©senter, on devrait conna√Ætre l‚Äô√©quivalence par lignes. Deux matrices $A$ et $B$ sont dites ***√©quivalentes par lignes*** si l‚Äôune peut √™tre obtenue √† partir de l‚Äôautre par une suite d‚Äôop√©rations √©l√©mentaires sur les lignes. On note $A ‚àº B$.

- Les **op√©rations √©l√©mentaires** sont : escalader une ligne, sommer une autre ligne escalad√©e et d√©placer deux lignes.

## Matrices √©chelonn√©es

**Matrices √©chelonn√©es** : le nombre de z√©ros commen√ßant une ligne cro√Æt strictement ligne par ligne jusqu‚Äô√† ce qu‚Äôil ne reste plus que des z√©ros (si c'est le cas, mais il peut se passer que il n'y a pas des lignes purement des z√©ros). Par exemple :

$$
\begin{bmatrix}
1&a_0&a_1&a_2&a_3 \\
0&0&2&a_4&a_5 \\
0&0&0&1&a_6
\end{bmatrix}
$$

**Matrices √©chelonn√©es r√©duites** : est une matrice √©chelonn√©e tel que

- le premier coefficient non nul d‚Äôune ligne (non nulle) vaut $1$
- et c‚Äôest le seul √©l√©ment non nul de sa colonne.

$$
\begin{bmatrix}
1&0&a_1&0&b_1 \\
0&1&a_2&0&b_2 \\
0&0&0&1&b_3
\end{bmatrix}
$$

- Soit $A ‚àà M_n (K)$. La matrice $A$ est inversible si et seulement si sa forme √©chelonn√©e r√©duite est la matrice identit√© $I_n$.
    
    ![untitled](new/uga/l1/s1/math/s1_math_algebre_lineaire_1/ressources/04_matrices_untitled.png)
    

## Matrices triangulaires

Les m**atrices triangulaires sup√©rieures** sont celles dont leur coefficients strictement au dessous de la diagonal sont $0$. La d√©finition est analogue pour les matrices triangulaires inf√©rieures, juste que dans ce cas-ci au-dessous de la diagonale principale.

Un type de matrice int√©ressant sont les m**atrices diagonales,** qui sont des matrices qui ont des $0$ dans toutes les entr√©es hors de la diagonale. Notons que par cons√©quence, elles sont triangulaires sup√©rieures et inf√©rieures simultan√©ment. Par exemple :

$$
\begin{bmatrix}
2&0&0 \\
0&3&0\\
0&0&5
\end{bmatrix}, \begin{bmatrix}
1&0&0 \\
0&0&0\\
0&0&1
\end{bmatrix} \text{et}
\begin{bmatrix}
0&0&0 \\
0&0&0\\
0&0&0
\end{bmatrix}
$$

Si $D$ est une matrice diagonale, il est tr√®s facile de calculer ses puissances $D^p$ (par r√©currence sur $p$) :

$$
D = \begin{bmatrix}
a_1&0&0 \\
0&a_2&0\\
0&0&a_3
\end{bmatrix}
\implies
D^p = \begin{bmatrix}
a_1^p&0&0 \\
0&a_2^p&0\\
0&0&a_3^p
\end{bmatrix}
$$

Une matrice $A$ de taille $n √ó n$, triangulaire, est inversible si et seulement si ses √©l√©ments diagonaux sont tous non nuls.

# Op√©rations d‚Äôune seule matrice

## **Transposition : $A^T$**

C'est juste inverser les indices $i$ et $j$ de chaque entr√©e. La matrice $n \times p$ va se transformer √† $p \times n$.

![untitled](new/uga/l1/s1/math/s1_math_algebre_lineaire_1/ressources/04_matrices_untitled_1.png)

## **Trace d‚Äôune matrice : $\text{tr}(A)$**

C'est la somme des entr√©es de la diagonal principale d'une matrice.

![untitled](new/uga/l1/s1/math/s1_math_algebre_lineaire_1/ressources/04_matrices_untitled_2.png)

## Notion de m**atrice sym√©trique et antisym√©trique**

Une matrice sym√©trique est une matrice $A$ telle que $A = A^T$ , et une matrice antisym√©trique est telle que $-A = A^T$ Les deux (sym√©triques et antisym√©triques) sont forc√©ment carr√©s.

**Th√©or√®me int√©ressant**. Toute matrice est la somme d‚Äôune matrice sym√©trique et d‚Äôune matrice antisym√©trique.

![untitled](new/uga/l1/s1/math/s1_math_algebre_lineaire_1/ressources/04_matrices_untitled_3.png)