# 01 // matrices et applications lin√©aires

# Rang d'une famille de vecteurs

## D√©finition

Soit $E$ un $\mathbb{K}$-espace vectoriel et soit $\{v_1 , . . . , v_p\}$ une famille finie de vecteurs de $E$. Le rang de la famille $\{v_1 , . . . , v_p\}$ est la dimension du sous-espace vectoriel $\text{Vect}(v_1, . . . , v_p)$ engendr√© par les vecteurs $\{v_1 , . . . , v_p\}$. Autrement dit : $\text{rg}(v_1, . . . , v_p) = \text{dim } \text{Vect}(v_1, . . . , v_p)$.

- **Propositions importantes**  :
    - $0 \le \text{rg}(v_1, . . . , v_p) \le  p$, o√π $p$ est nombre d‚Äô√©l√©ments dans la famille
    - $\text{rg}(v_1, . . . , v_p) \le \text{dim }E$, si $E$ de dimension finie.
- Remarques :
    - Le rang d‚Äôune famille vaut $0$ si et seulement si tous les vecteurs sont nuls.
    - Le rang d‚Äôune famille $\{v_1, . . . , v_p\}$ vaut p si et seulement si la famille $\{v_1, . . . , v_p\}$ est libre.

## Rang d'une matrice

On d√©finit le rang d‚Äôune matrice comme √©tant le rang de ses vecteurs colonnes. C‚Äôest-√†-dire, la dimension de l‚Äôespace engendr√© des vecteurs colonnes de la matrice.

Une matrice int√©ressante par rapport √† son rang est une **matrice √©chelonn√©e**. On dit qu‚Äôune matrice est √©chelonn√©e par rapport aux colonnes si le nombre de z√©ros commen√ßant une colonne cro√Æt strictement colonne apr√®s colonne, jusqu‚Äô√† ce qu‚Äôil ne reste plus que des z√©ros. Autrement dit, la matrice transpos√©e est √©chelonn√©e par rapport aux lignes. Par exemple :

$$
\begin{bmatrix}
+&0&0&0&0&0\\
*&0&0&0&0&0\\
*&+&0&0&0&0\\
*&*&+&0&0&0\\
*&*&*&0&0&0\\
*&*&*&+&0&0
\end{bmatrix}
$$

Ici, les $‚àó$ d√©signent des coefficients quelconques (nuls ou non), et les $+$ des coefficients strictement non nuls. Le rang d‚Äôune matrice √©chelonn√©e par colonnes est √©gal au nombre de colonnes non nulles.

## Op√©rations conservant le rang

Telles op√©rations sont les op√©rations √©l√©mentaires :

1. **Escalade** : $C_i ‚Üê ŒªC_i$ avec $Œª \ne 0$. On peut multiplier une colonne par un scalaire non nul.
2. **Somme escalad√©e** : $C_i ‚Üê C_i + ŒªC_j$ avec $Œª ‚àà K$ (et $j \ne i$). On peut ajouter √† la colonne $C_i$ un multiple d‚Äôune autre colonne $C_j$.
3. **√âchange** : $C_i ‚Üî C_j$. On peut √©changer deux colonnes.

Plus g√©n√©ralement, on peut dire que l‚Äôop√©ration $C_i ‚Üê C_i + ‚àë_{i \ne j} Œª_jC_j$ conserve le rang de la matrice. L‚Äôespace vectoriel engendr√© par les vecteurs colonnes est conserv√© par ces op√©rations.

## Rang, matrices inversibles et transpos√©es

Par rapport aux matrices inversibles, une matrice carr√©e de taille $n$ est inversible si et seulement si elle est de rang $n$.

Par rapport aux matrices transpos√©es, on a que $\text{rg}A = \text{rg}(A^T)$. Cela n'implique pas que l'E.V. repr√©sent√© par $A$ est le m√™me que celui repr√©sent√© par $A^T$ !

# Application lin√©aires en dimension finie

## Construction et *caract√©risation*

Une application lin√©aire cr√©e une correspondance entre les vecteurs d'un espace $A$ √† un autre espace $B$. C‚Äôest juste une fonction.

Supposons que $A$ n'a que deux vecteurs, $\{v_1, v_2\}$ et le m√™me pour $B$, $\{w_1, w_2\}$. Donc, il existe une **unique application lin√©aire** pour chaque possible mani√®re unique de faire des correspondances entre les vecteurs de $A$ et $B$. Et donc, il existe une unique matrice qui d√©crit une unique application lin√©aire, il y a bijection.

<aside>
üí° On dit aussi que chaque A.L. est ***caract√©ris√©e*** par une unique matrice. **Si on conna√Æt les coefficients, on conna√Æt la A.L.**

</aside>

Il peut √™tre vite fatigant de devoir faire des op√©rations math√©matiques pour savoir quel vecteur $v$ de $A$ correspond √† quel vecteur $w$ de $B$. Donc, c'est plus vite si on s√©pare chaque vecteur comme un produit des coordonn√©es et les vecteurs bases.

$$
f(x)=f\left(\sum_{i=1}^nx_ie_i \right)=\sum_{i=1}^nx_if(e_i)=\sum_{i=1}^nx_iv_i
$$

Apr√®s, on juste passe chaque vecteur base par la fonction. Les vecteurs r√©sultants seront une base de l'image de $f(x)$.

<aside>
‚ùó **Attention** **!** Les vecteurs r√©sultants ne sont pas forc√©ment une base de tout $B$, juste la base de $\text{lm}(f)$. Ils pourrait √™tre la base de $F$ si et seulement si $f(x)$ est surjective, car donc $\text{Im}(f) = B$, et si les vecteurs sont ind√©pendants entre eux. Sinon, au moins la famille $\{f(e_1), ... , f(e_n)\}$ est g√©n√©ratrice.

</aside>

Deux notes pratiques :

- Il y a m√™me des fois ou on ne peut pas computer tous les vecteurs, mais cela suffit avec les images des vecteurs bases.
- Notons que les scalaires des vecteurs bases de partie sont les m√™mes de ceux d'arriv√©e !

## Rang, applications lin√©aires et th√©or√®me du rang

### Rangs et applications

Normalement, on parle de rang d'une famille de vecteurs comme la dimension de l'espace engendr√© de ce famille. C‚Äôest-√†-dire, $\text{rg}(\{v_1, . . . , v_p\}) = \text{dim } \text{Vect}(\{v_1, . . . , v_p\})$.

Quand on parle du *rang d'une A.L*., on parle du rang de la image de $f$, qui est un sous-ensemble de l'ensemble d'arriv√©e ou codomaine. Notons que la famille de vecteurs compos√©e de l'image de la base du d√©part engendre toute l'image de $f$. Donc, $\text{rg}(f) = \text{dim } \text{Im}(f) = \text{dim } \text{Vect}(\{f(e_1), . . . , f(e_n)\})$.

**Proposition**. Le rang est plus petit que la dimension de $E$ et aussi plus petit que la dimension de $F$, si $F$ est de dimension finie : $\text{rg}(f) \le \text{min}(\text{dim } E, \text{dim } F)$.

### Th√©or√®me du rang

Le th√©or√®me sous la forme original est $\text{dim } E = \text{dim Ker}(f) + \text{dim Im}(f)$. Cela dit, une autre √©criture est $\text{dim }E = \text{dim Ker}(f) + \text{rg}(f)$.

Dans la pratique, cette formule sert √† d√©terminer la dimension du noyau connaissant le rang, ou bien le rang connaissant la dimension du noyau. Normalement, on conna√Æt la dimension de $E$.

## Application lin√©aire entre $E$ et $F$ de m√™me dimension

Soit $f : E \mapsto F$ un isomorphisme (fonction bijective et lin√©aire) d‚Äôespaces vectoriels. Si $E$ (respectivement $F$) est de dimension finie, alors $F$ (respectivement $E$) est aussi de dimension finie et on a $\text{dim }E = \text{dim }F$.

- Une autre reformulation c'est : si $\text{dim }E = \text{dim }F$ et si $f$ est injective ou surjective.

## Matrices et applications lin√©aires

### Les applications lin√©aires √©crites comme des matrices

√Ä nouveau, une application lin√©aire est repr√©sent√©e par une ***unique*** matrice. On avait dit aussi que, si on passe une base de $E$ par la fonction, l'image de ce famille sera une base de $\text{Im}(f)$.

Mais, notons que finalement cette famille appartient √† l'ensemble d'arriv√©e, $F$. Donc, on peut exprimer les images de la base de d√©part avec la base d'arriv√©e de mani√®re ***unique***.

Dans la prochaine √©quation, on exprime l‚Äôimage d‚Äôun vecteur $e_j$ de la base $B$, avec la base d‚Äôarriv√© $B^\prime=(f_i)_{1\le i\le n}$ :

$$
f(e_j)=a_{1,j}f_1+a_{2,j}f_2+\dots+a_{n,j}f_n=\begin{bmatrix}a_{1,j}\\a_{2,j}\\\vdots\\a_{n,j}\end{bmatrix}_{B^\prime}
$$

<aside>
‚ùó **Attention !** On utilise la lettre $f$ pour la fonction/A.L. et aussi pour les vecteurs de la base $B^\prime$.

</aside>

La matrice d'application lin√©aire $f$ (par rapport aux bases $B$ et $B'$) est la matrice dont la $j$-i√®me colonne est constitu√©e par les coordonn√©es du vecteur $f(e_j)$ dans la base $B^\prime=(f_i)_{1\le i\le n}$.

![image (1).png](image_(1).png)

En termes plus simples, c‚Äôest la matrice dont les vecteurs colonnes sont l‚Äôimage par $f$ des vecteurs de la base de d√©part $B$, exprim√©e dans la base d‚Äôarriv√©e $B'$. On note cette matrice $\text{Mat}_{B,B'}(f)$.

Deux remarques √† faire :

- La taille de la matrice $\text{Mat}_{B,B'}(f)$ d√©pend uniquement de la dimension de $E$ et de celle de $F$.
- Par contre, les coefficients de la matrice d√©pendent du choix de la base $B$ de $E$ et de la base $B'$ de $F$. On ne peut pas choisir les dimensions de $E$ et $F$, mais on peut bien choisir des bases diff√©rentes de chaque espace vectoriel.

### Op√©rations sur les matrices des A.L.

$\text{Mat}_{B,B'}(f)$ repr√©sente une matrice d'une application lin√©aire. Cela dit, les suivantes propositions sont vraies :

- $\text{Mat}_{B,B'}(f + g)$ = $\text{Mat}_{B,B'}(f)$ + $\text{Mat}_{B,B'}(g)$
- $\text{Mat}_{B,B'}(Œªf) = Œª\cdot \text{Mat}_{B,B'}(f)$
- On dirait que $\text{Mat}_{B,B'}(f)$ est une application lin√©aire !
- Cela dit, $B$ et $B'$ doit rester fix√©s.

Aussi, et **la propri√©t√© la plus importante** : $\text{Mat}_{B,B'}(g \circ f) = \text{Mat}_{B,B'}(g) \cdot  \text{Mat}_{B,B'}(f)$, o√π $g\circ f = g\left(f(x)\right)$.

## Deux cas particuliers : endomorphismes et isomorphismes

### Matrice d‚Äôun endomorphisme : $E \mapsto E$

<aside>
‚ö†Ô∏è Avant d'aborder les A.L. plus connus exprim√©s en forme matricielle, un point sur la notation :

- Si on prend la m√™me base dans l'ensemble de d√©part et arriv√©e, alors on note $\text{Mat}_B(f)$.
- Si on prend deux bases diff√©rentes dans le m√™me ensemble, alors on note $\text{Mat}_{B,B'}(f)$ pour deux bases $B$ et $B'$.
- Notons aussi que la taille de la matrice est toujours carr√©e dans cette section.
- **Propri√©t√©**. Si $f$ est un endomorphisme, alors $\text{Mat}_B(f^p) = (\text{Mat}_B(f))^p$, o√π $f^p$ est $f$ compos√©e avec elle m√™me $p$ fois. Ceci est juste une reformulation de la √©quivalence de composition/multiplication des matrices de A.L.
</aside>

- **Identit√©** : $\text{Mat}_B(f) = I_n$. 
√âvident, car on prend la m√™me base en d√©part/arriv√©e.
Mais ce n'est plus vrai si on a deux bases diff√©rentes !
- **Homoth√©tie** : $\text{Mat}_B(f) = Œª\cdot I_n$. 
√âvident, une homoth√©tie escalade les composants du vecteur.
- **Sym√©trie centrale** : $\text{Mat}_B(f) = -I_n$. 
Une sym√©trie central est une homoth√©tie avec $Œª = -1$.
- **R√©flexion par rapport √† la fonction ou axe de sym√©trie $y = x$** :
    
    $$
    \text{Mat}_B(f)=\begin{bmatrix}0&1\\1&0\end{bmatrix}
    $$
    
- **Rotation (de $\theta$ radians) :**
    
    $$
    \text{Mat}_B(f)=\begin{bmatrix}\cos\theta&-\sin\theta\\\sin\theta&\cos\theta\end{bmatrix}
    $$
    
    - Particuli√®rement, quand il y a $p$ rotations √† la suite‚Ä¶
        
        $$
        \text{Mat}_B(f^p)=\begin{bmatrix}\cos(p\theta)&-\sin(p\theta)\\\sin(p\theta)&\cos(p\theta)\end{bmatrix}
        $$
        

### Matrice d'un isomorphisme : $f$ bijective et lin√©aire

$f$ est bijective (et donc isomorphe, parce que l‚Äôespace d‚Äôapplications lin√©aires et un espace vectoriel et donc v√©rifie la propri√©t√© de lin√©arit√©) si et seulement si la matrice $\text{Mat}_{B, B‚Äô}(f)$ est inversible.

- Bijectivit√© $\iff$ Inversibilit√©.

De plus, si $f : E \mapsto F$ est bijective, alors la matrice de l‚Äôapplication lin√©aire $f^{‚àí1} : F \mapsto E$ est la matrice $(\text{Mat}_{B,B'}(f))^{-1}$. Pour le d√©duire, par l‚Äô√©quivalence composition/multiplication, on observe facilement que $\text{Mat}_{B,B'}(f^{-1}) = (\text{Mat}_{B,B'}(f))^{-1}$.

# Changement de bases

## Pr√©paration : de $y=f(x)$ √† $\bold{Y}_{B'}=A\bold{X}_B$

Soit $E$ un espace vectoriel de dimension finie et soit $B = (e_1, e_2, \dots , e_p)$ une base de $E$. Comme annotation, si $B$ est la base canonique, on n'√©crit pas un souscrit $B$ sur un vecteur, il ne faut pas mentionner la base.

Rappelons que tout √©l√©ment $x$ de $E$ se repr√©sente comme : $x = x_1e_1 + x_2e_2 + \dots + x_pe_p$. On dit que le vecteur colonne $\{x_1, x_2,\dots, x_p\}_B^T$ contient les coordonn√©es de $x$ sous la base $B$ de $E$.

L‚Äôobjective est de faire une analogie de "$y = f(x)$" dans l'alg√®bre lin√©aire. Le premier pas est de fixer $f : E \mapsto F$ et $A = \text{Mat}_{B,B'}(f)$ (la matrice qui contient l'image de la base de $E$ exprim√© dans la base de $F$).

Apr√®s, soit $\bold{X}$ un vecteur colonne de $E$ (exprim√© dans la base $B$) et $\bold{Y}$ un vecteur colonne de $F$ (exprim√© dans la base $B'$). Finalement, l'analogie de ‚Äú$y = f(x)$" serait "$\bold{Y} = A\bold{X}$". Voyons que, dans le monde de l‚Äôalg√®bre lin√©aire, multiplier $\bold{X}$ par A √† gauche est l‚Äô√©quivalence d‚Äôappliquer une fonction. Multiplier est d‚Äôappliquer une fonction.

## Matrice de passage d'une base $B$ √† une autre $B^\prime$ : $P_{B,B^\prime}$

Soit $E$ un espace vectoriel de dimension finie $n$. On sait que toutes les bases de $E$ ont $n$ √©l√©ments. Fixons deux bases du m√™me ensemble $E$ : $B$ et $B'$.

La **matrice de passage** $P_{B,B'}$ est la matrice qui exprime les vecteurs de $B'$ en termes de l'ancienne base $B$. C‚Äôest le concept le plus important du changement de base.

Si on multiplie un vecteur √† la droite de $P_{B,B‚Äô}$, il se "traduit" de $B'$ √† $B$. L'arriv√©e est toujours la base $B$ avec laquelle on s'exprime actuellement. On la note aussi $\text{Mat}_B(B')$.

<aside>
‚ö†Ô∏è Quelques notes par rapport √† $P_{B,B‚Äô}$ :

- Puisqu'on reste dans un m√™me espace vectoriel $E$, on peut noter $\text{Mat}_{B',B}(\text{id}(E))$.
- Ici, $\text{id}(E)$ est une fonction de $E \mapsto E$ qui laisse un vecteur inchang√©. La seule chose qui change donc c'est l'expression de l'image de la base $B'$ exprim√©s en termes de la base $B$.
- Mais, puisque l'image "ne change rien", on juste exprime $B'$ en termes de $B$.
- **Fais attention √† l'inversion des bases dans la notation !**
</aside>

On devrait conna√Ætre trois propri√©t√©s importantes de la matrice de passage :

- $P_{B',B} = (P_{B,B'})^{-1}$
- Prenons 3 bases : $B$, $B'$, $B''$. Donc $P_{B,B''} = P_{B,B'} \cdot P_{B',B''}$.
- $\bold{X} = P_{B,B'} \cdot \bold{X'}$, pour $\bold{X}$ √©l√©ment de $E$ exprim√© en $B$ et $\bold{X'}$ qui est le m√™me √©l√©ment de $E$ mais exprim√© en $B'$.

## Formule de changement de base

### Sur deux espaces vectoriels $E$ et $F$

Rappelons : une **matrice d'application lin√©aire** d√©crit une fonction de $E \mapsto F$. Elle montre l'image de la base de d√©part dans la base d'arriv√©e. Une **matrice de passage** exprime un nouvelle base en terme d'une ancienne base toujours d'un m√™me ensemble $E$. Il est tr√®s important de comprendre la diff√©rence entre les deux !

<aside>
‚õëÔ∏è Aide : la notation $\text{Mat}_{1,2}$ se lit ‚Äúde $1$ √† $2$‚Äù tant que $P_{1,2}$ se lit ‚Äú√† $1$ de $2$‚Äù. $\text{Mat}$ se lit dans ‚Äúle bon ordre‚Äù, c√†d. de gauche √† droite ; et $P$ dans l‚Äôordre inverse.

</aside>

La formule de changement de base est la suivante :

$$
B=Q^{-1}AP
$$

- On consid√®re 4 bases : $\mathcal{B}_{E}$, $\mathcal{B}_{E}'$, $\mathcal{B}_{F}$, $\mathcal{B}_{F}'$.
- $A = \text{Mat}_{\mathcal{B}_{E},\mathcal{B}_{F}}(f)$ et $B = \text{Mat}_{\mathcal{B}_{E}',\mathcal{B}_{F}'}(f)$ sont des fonctions.
- $P = P_{\mathcal{B}_{E},\mathcal{B}_{E}'}$ et $Q = P_{\mathcal{B}_{F},\mathcal{B}_{F}'}$ sont des passages.

Lisons de droite √† gauche pour comprendre ce que elle veut nous dire :

$$
B\bold{Y}_{\mathcal{B}}=Q^{-1}AP\bold{Y}_{\mathcal{B}'}\\[4pt](e\mathcal{b}'_e)\rightarrow_{\text{id}_E} (E,\mathcal{B}_E)\rightarrow_f(F,\mathcal{B}_F)\rightarrow_{\text{id}_F}(F,\mathcal{B}'_F)
$$

1. $P$ : On prend un vecteur $\bold{Y}$ de notre ensemble $E$ et on change sa base de $\mathcal{B}_{E}'$ √† $\mathcal{B}_{E}$.
2. $A$ : Apr√®s, on applique $f$ et le r√©sultat sera en $\mathcal{B}_{F}$.
3. $Q^{-1}$ : Finalement, on va exprimer le vecteur r√©sultat de $\mathcal{B}_{F}$ √† $\mathcal{B}_{F}'$.
Attention ! Ce ne pas $Q$, mais $Q^{-1}$, √ßa m‚Äôa pos√© de probl√®mes.
4. $B$ : Mais, notons que tout cela serait le m√™me que prendre $\bold{Y}$ exprim√© en $\mathcal{B}_{E}'$ et prendre son image par $f$ exprim√© en $\mathcal{B}_{F}'$.

### Sur un m√™me espace vectoriel $E$

Le processus est plus simple si on parle d'un endomorphisme $E \mapsto E$ :

$$
B = P^{-1}AP
$$

- $P = P_{\mathcal{B},\mathcal{B}'}.$
- $A = \text{Mat}_{\mathcal{B}}(f)$.
- $B = \text{Mat}_{\mathcal{B}'}(f)$.

Interpr√©tation :

1. $P$ : On prend un vecteur $\bold{Y}$, √©crit en $\mathcal{B}'$, et on le r√©√©crit en $\mathcal{B}$.
2. $A$ : Apr√®s, on passe $E$ par la fonction $f$, ce qui nous laisse un r√©sultat en $\mathcal{B}$.
3. $P^{-1}$ : Finalement, on reprend l'image avec $B$ et on la r√©√©crit en $\mathcal{B}'$.
4. $B$ : Notons que tout cela serait le m√™me si on applique la fonction nous laissant un r√©sultat en $\mathcal{B}'$.

### Matrices semblables

On dit que la matrice $B$ est semblable √† la matrice $A$ s‚Äôil existe une matrice inversible $P ‚àà M_n(K)$ telle que $B = P^{‚àí1}AP$. Deux matrices semblables repr√©sentent le m√™me endomorphisme, mais exprim√© dans des bases diff√©rentes.

- La relation "√™tre semblable" est r√©flexive, sym√©trique et transitive.