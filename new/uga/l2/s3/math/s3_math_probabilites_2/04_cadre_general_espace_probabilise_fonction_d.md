# 04 // cadre g√©n√©ral : espace probabilis√©, fonction de r√©partition et de densit√©

# Espace probabilis√©

## Pr√©paration : $\sigma$-alg√®bre

Une $\sigma$-alg√®bre ou tribu sur un ensemble $\Omega$ est un ensemble $\mathcal{A} \subset \mathcal P(\Omega)$ qui v√©rifie les trois propri√©t√©s suivantes. Notons que $A$ est un sous-ensemble de $\mathcal A$.

1. $\Omega \in \mathcal A$, o√π $\Omega$ est consid√©r√© l‚Äôunivers dans le contexte
2. Stabilit√© par la compl√©mentation : $A \in \mathcal A \implies A^c \in \mathcal A$
3. Stabilit√© par l‚Äôunion d√©nombrable ($\sigma$-additivit√©) : si $(A_i)_{i \ge 1} \in \mathcal A$, alors $\bigcup_{i \ge 1} A_i \in \mathcal A$.

Appliquant les lois de De Morgan avec les propri√©t√©s 2 et 3, on arrive a la stabilit√© par l‚Äôintersection d√©nombrable, car $\left( \bigcup_{i‚â•1} A_i \right)^c = \bigcap_{i‚â•1} A_i^c$.

Les deux $\sigma$-alg√®bres les plus basiques et extr√™mes sur $\Omega$ sont $\{\empty, \Omega\}$ et $\mathcal P (\Omega)$. Toute autre $\sigma$-alg√®bre reste sur ces deux extr√™mes.

### Propositions

**Lemme**. L‚Äôintersection de deux $\sigma$-alg√®bres est un $\sigma$-alg√®bre sur $\Omega$.

Soit $C \in \mathcal P(\Omega)$. Il existe une plus petite $\sigma$-alg√®bre sur $\Omega$ contenant $C$. On l‚Äôappelle la $\sigma$-alg√®bre engendr√©e par $C$ et on la note $\sigma(C)$. C‚Äôest, en fait, l‚Äôintersection des toutes les $\sigma$-alg√®bres qui contiennent $C$.

En langage naturel, $C$ est un √©v√©nement, et on cherche la plus petite $\sigma$-alg√®bre sur les r√©sultats qui contient l‚Äô√©v√©nement.

## D√©finition des espaces probabilis√©s : $\{\Omega, \mathcal A, \mathbb P\}$

Pour toute la suite, on ne se limite pas √† juste parler de V.A. enti√®res mais aussi de V.A. r√©elles. Pour cela, il faut pr√©senter les axiomes de la mani√®re suivante, et en l‚Äôaccompagnant d‚Äôun exemple avec un d√©.

Un espace probabilis√© est un concept math√©matique pour modeler une exp√©rience al√©atoire, repr√©sent√© comme un triplet $\{ \Omega, \mathcal{A}, \mathbb{P} \}$.

- $\Omega$ : l‚Äôunivers, ou l‚Äôensemble des *r√©sultats* de l‚Äôexp√©rience. Pour un d√©. : $\{1,2,3,4,5,6\}$.
- $\mathcal{A}$ : une tribu, $\sigma$-alg√®bre ou simplement l‚Äôensemble d‚Äô*√©v√©nements*. Chaque membre de cet ensemble est formellement un ‚Äú√©v√©nement‚Äù, **qui est diff√©rent des ‚Äúr√©sultats‚Äù** de $\Omega$. Normalement, on prend comme $\mathcal{A}$ l‚Äôensemble des parties de $\Omega$, c‚Äôest-√†-dire $\mathcal{P}(\Omega)$.

Par exemple, un √©v√©nement peut √™tre simplement que le d√© montre $2$, dans ce cas $\{2\} \in \mathcal{A}$, mais aussi que le d√© montre un nombre pair, donc $\{2,4,6\} \in \mathcal{A}$.
- $\mathbb{P}$ : la loi de probabilit√©s ou mesure de probabilit√©, qui est une fonction qui associe une probabilit√© √† chaque √©v√©nement‚Äîformellement $\mathcal{A} \mapsto [0,1]$‚Äî et qui v√©rifie $\mathbb{P}(X\in\Omega)$ = 1 et $\mathbb P \left( \bigcup_{i‚â•1} A_i \right) = \sum_{i‚â•1} \mathbb{P}(A_i)$, supposons que les A_i sont 2-√†-2 disjoints. Ceci implique que la fonction ou application $\mathbb P$ est $\sigma$-additive.

Continuant avec l‚Äôexemple, si on suppose un d√© non-pip√©, donc $\mathbb{P}(X \in \{2\})=\frac{1}{6}$ et $\mathbb{P}(X \text{ pair}) = \mathbb{P}(X \in \{2,4,6\}) = \frac{1}{2}$.

<aside>
üí° Lorsqu‚Äôune exp√©rience est conduite, on imagine que la ‚Äúnature‚Äù ‚Äús√©lectionne‚Äù un **r√©sultat** unique $\omega$ de l‚Äôunivers $\Omega$, supposons $\omega = 2$. Tous les **√©v√©nements** de $\mathcal{A}$ qui contiennent le **r√©sultat** $\omega$ sont dit *produits*. Par exemple, l‚Äô√©v√©nement ‚Äúle d√© montre $2$‚Äù s‚Äôest produit, mais l‚Äô√©v√©nement ‚Äúle d√© montre un nombre pair‚Äù s‚Äôest aussi produit, et de m√™me pour l‚Äô√©v√©nement ‚Äúle d√© montre un nombre premier‚Äù, car $2$ est premier.

Cette "s√©lection" se produit de telle mani√®re que, si l'exp√©rience se r√©p√©tait plusieurs fois, le nombre d'occurrences de chaque √©v√©nement comme fraction du nombre total d'exp√©riences conduites tendrait tr√®s probablement vers la probabilit√© attribu√©e √† cet √©v√©nement par la fonction de probabilit√©s $\mathbb{P}$. Ceci c‚Äôest juste la loi des grands nombres.

</aside>

On peut se demander pourquoi ne pas choisir un autre $\mathcal{A}$ diff√©rent de $\mathcal{P}(\Omega)$. Pour le cas o√π $\Omega$ est d√©nombrable, on peut se contenter toujours faisant cette choix de $\mathcal{A}$.

Par contre, pour le cas non d√©nombrable comme $\Omega = \R$, si on d√©finissait $\mathcal{A}=\mathcal{P}(\R)$, il serait faux que choisir un sous-ensemble quelconque de $\Omega$ soit un √©v√©nement. Ceci est hors du cours, mais c‚Äôest une cons√©quence du th√©or√®me d‚ÄôUlam.

Pour cette raison, dans le cas non d√©nombrable, $\mathcal{A} \subset\mathcal{P}(\Omega)$  et non $\mathcal{A} \subseteq \mathcal{P}(\Omega)$ comme dans le cas d√©nombrable.

## Propri√©t√©s des probabilit√©s

<aside>
üí° Pour simplicit√© de notation, on note simplement $\mathbb P(a\in A) = \mathbb P(A)$.
$a$ est une variable d‚Äôint√©r√™t dont on parle souvent, donc ce n‚Äôest pas n√©cessaire de la mentionner tout le temps. $A$ peut √™tre un ensemble ou un intervalle.

</aside>

### Propri√©t√©s basiques

- $\mathbb P (\empty) = 0$.
- $\mathbb P (A^c) = 1 - \mathbb P (A)$.
- Additivit√© simple : $\mathbb P \left( \bigcup_{i=1}^n A_i \right) = \sum_{i=1}^n \mathbb P (A_i)$. Il faut que les $A_i$ soient disjoints.

### D‚Äôautres propri√©t√©s

**$\sigma$-additivit√© d‚Äôintervalles**. Imaginons que notre espace d‚Äô√©v√©nements $\mathcal A$ contient des intervalles, qu‚Äôon notera $B_n$. On en prend une famille d‚Äôintervalles et on imagine que les intervalles devient de plus en plus grandes, c‚Äôest-√†-dire, le prochain intervalle contient l‚Äôactuel, ou $B_n \subset B_{n+1}$.  Donc :

$$
\mathbb P \left( \bigcup_{n \ge 1} B_n\right ) = \lim_{n \rightarrow \infin} \mathbb P(B_n)
$$

En outre, et ignorant la condition d‚Äôagrandissement du prochain intervalle $B_{n+1}$, on a la propri√©t√© suivante. Elle n‚Äôest pas une √©galit√© stricte car il se peut que les $B_n$ ne soient pas disjoints.

$$
\mathbb P \left( \bigcup_{n \ge 1} B_n\right ) \le \sum_{n \ge 1} \mathbb P (B_n)
$$

# Fonction de r√©partition : $F_X(x)$

## La probabilit√© accumul√©e jusqu‚Äô√† un certain point $t$

Soit $\mathbb P$ une loi de probabilit√© sur $\R$. On d√©finit une fonction de r√©partition $F$ comme :

$$
F_X(t) = \mathbb P(X\le t)
$$

√âtant donn√© que les probabilit√©s sont toujours entre $0$ et 
$$, et que la somme de toutes les probabilit√©s est $1$ est la somme d‚Äôaucune est $0$, on conclut que $F$ est croissante et born√© entre $0$ et $1$.  En plus, on ajoute une possibilit√© d‚Äô√™tre ***continue √† droite*** (c√†d. si elle pr√©sente des sauts, le point o√π se produit le saut sera inclut dans le prochain √©chelon et pas l‚Äôactuel).

Toutes ces propri√©t√©s permettent de d√©finir une autre unique fonction ou loi de probabilit√© $f$.

# Fonction de densit√© : $f_X(x)$

## Presque identique √† une loi de probabilit√©

### Motivation

Dans le cadre d√©nombrable, on avait une fonction appel√©e ‚Äúloi de probabilit√©‚Äù $\mathbb P$ qui assigne chaque valeur possible d‚Äôune exp√©rience al√©atoire avec une probabilit√©. Elle nous permet de dire que ‚Äúla probabilit√© de l‚Äôexp√©rience $X$ r√©sulte en $x$ est $\mathbb P(X=x)$, ou que la probabilit√© qu‚Äôelle soit contenu dans un intervalle $[a,b]$ est $\mathbb P(a \le X \le b)$‚Äù.

Dans le cadre g√©n√©rale, ceci n‚Äôest plus le cas. Particuli√®rement, on ne peut pas parler de la probabilit√© que $X$ soit exactement √©gal √† $x$, car pour tout $x$, $\mathbb P(X=x)=0$, car il y a une infinit√© de valeurs que $X$ peut prendre. On peut, par contre, parler d‚Äôune probabilit√© de que $X$ soit contenu dans un intervalle, qui serait donn√© comme suit :

$$
\mathbb P(a \le X  \le b)  \iff F_X(b)-F_X(a)
$$

Pourquoi on se souci de faire remarquer tout √ßa ? **Parce qu‚Äôil ne faut pas penser que la fonction de densit√© est la m√™me chose que la loi de probabilit√©**, que c‚Äôest un erreur que j‚Äôai d√©j√† fait. Il peut √™tre utile penser que la fonction de densit√© parle des ‚Äúprobabilit√©s relatives‚Äù tant que la fonction de r√©partition parle des ‚Äúprobabilit√©s absolues ou r√©elles‚Äù.

### D√©finition et conditions

Pour d√©finir une fonction de densit√©, on part du principe qu‚Äôon peut d√©terminer (ou on conna√Æt d√©j√†) sa fonction de r√©partition. Particuli√®rement, s‚Äôil existe une telle fonction $f_X(x)$ qui v√©rifie le condition suivante en bas, $f_X(x)$ est donc la fonction de densit√© de $\mathbb{P}$.

$$
\underbrace{F(t)}_{\mathbb P(X\le t)}=\int_{-\infin}^tf_X(x)dx
$$

**Note**. Il se peut, parfois, que la fonction de r√©partition existe tant que la fonction de densit√© non !  Une condition pour que la densit√© existe est $\lim_{x \longrightarrow \epsilon} \int_{-\epsilon}^{\epsilon} f(x)dx = 0$. La source de probl√®mes est normalement que la fonction de r√©partition $F$ est discontinue.

La fonction de densit√© a quelques similitudes avec la loi de probabilit√© d‚Äôune V.A. d√©nombrable. Particuli√®rement, il faut qu‚Äôelle v√©rifie les deux axiomes d‚Äôune loi de probabilit√© : quelle soit toujours positive et que la somme de probabilit√©s soit √©gale √† $1$.

$$
\forall x\in \R,\hspace{4pt} f_X(x)\ge0 \space\text{ et }\space\int_\Omega f_X(x)dx = 1
$$

Ici, $\Omega$ est un ensemble des valeurs que la variable al√©atoire $X$ peut prendre.

# Propri√©t√©s g√©n√©ralis√©es du cas d√©nombrable

Les propri√©t√©s et notions suivantes se g√©n√©ralisent aussi du cas d√©nombrable au cadre g√©n√©ral: **probabilit√© conditionnelle, ind√©pendance, esp√©rance et moments**.

## Par rapport √† l‚Äôesp√©rance

Par la suite, on va s‚Äôint√©resser juste aux [V.As](http://V.As) qui ont admettent une fonction de densit√©. Donc, l‚Äôesp√©rance d‚Äôune telle V.A X serait comme suit :

$$
\mathbb{E}[X]=\int_\Omega \ x\cdot f_X(x)\space dx
$$

Un outil pour le calcul de l‚Äôesp√©rance, m√™me si elle n‚Äôas pas de densit√©, est le suivant : soit $X$ une V.A. r√©elle, on peut la d√©composer dans sa partie positive et n√©gative comme suit.

$$
X = X^+-X^-, \text{ o√π } X^+=\max(X,0) \text{ et }X^-=-\min(X,0)

\\
\\

\text{Par lin√©arit√© de l'esp√©rance, } \mathbb{E}[X] = \mathbb{E}[X^+] - \mathbb{E}[X^-]
$$

L‚Äôutilit√© de cette d√©composition est que si $\mathbb{E}[X^+]$ ou $\mathbb{E}[X^-]$ n‚Äôexiste pas, $\mathbb{E}[X]$ non plus.

On garde aussi la lin√©arit√© et la multiplicativit√© de l‚Äôesp√©rance (cette derni√®re si $X_1$ et $X_2$ sont des V.A. ind√©pendantes).

### Esp√©rances des fonctions de $X$ : on garde le domaine $\Omega_X$ et la densit√© $f_X$

Pour calculer une esp√©rance, on doit r√©soudre une int√©grale d√©fini dont l‚Äôint√©grande (membre de l‚Äôint√©grale ou la fonction √† int√©grer) est un produit : une variable et sa fonction de densit√©, normalement.

Par contre, si on veut calculer l‚Äôesp√©rance d‚Äôune fonction de la V.A. $X$, il faut appliquer telle fonction √† la variable dans l‚Äôint√©grande en gardant toujours la densit√© de la V.A. originale $X$, et non pas voyant la densit√© de la fonction de la V.A $X$. Supposons qu‚Äôon veut d√©terminer l‚Äôesp√©rance de $e^X$, donc :

$$
\begin{array}{ll}
\text{Mani√®re correcte : } 
&
\mathbb E [e^X] = \int_{\Omega_X} (e^x) \cdot f_X(x) \space dx
\\[5pt]
\text{Mani√®re incorrecte : } 
&
\mathbb E [e^X] = \int_{\Omega_{e^X}} (e^x) \cdot f_{e^X}(x) \space dx
\end{array}
$$

Les deux diff√©rences sont le change de $\Omega_X$ √† $\Omega_{e^X}$ et le change de $f_X(x)$ √† $f_{e^X}(x)$. En d√©finitive, l‚Äôesp√©rance suppose **contextuellement** qu‚Äôon garde le domaine de d√©finition de l‚Äôint√©grale $\Omega_X$ et la densit√© de la V.A. $f_X$. Quand on calcule une esp√©rance, c‚Äôest une bonne pratique de se demander par rapport √† quelle densit√© $f$ et √† quel domaine $\Omega$.

On pourrait, effectivement, d√©duire une densit√© $f_{e^X}$ et un domaine de d√©finition $\Omega_{e^X}$, mais cela on va l‚Äôexplorer dans la derni√®re section de cette note.

## Par rapport √† la variance

Supposant encore que les V.As d‚Äôint√©r√™t admettent une fonction de densit√©, une fois on conna√Æt l‚Äôesp√©rance $\mathbb E[X]$ d‚Äôun cadre continu, on peut en d√©duire la variance $\text{Var}(X)$ :

$$
\text{Var}(X)=\mathbb E[(X-\mathbb E[X])^2] = \mathbb E[X^2]-\underbrace{\mathbb E[X]^2}_{\text{connu}},\text{ o√π } \mathbb E [X^2]=\int_{\Omega_X} x^2 \cdot f_X(x)\space dx
$$

Puisque $\mathbb E[X]$ est une constante (si elle existe; que ce n‚Äôest pas toujours le cas), $\mathbb E[X]^2$ est aussi une constante. 

# Densit√© d‚Äôune V.A. fonction de $X$ : $Y=\varphi(X)$

## D√©rivation

Soit $X$ une V.A. de densit√© $d_X(x)$. Soit $\varphi$ une fonction monotonique et contin√ªment d√©rivable, soit $Y = \varphi(X)$ la V.A. dont on veut trouver sa densit√©. Donc, la densit√© $f_Y(y)$ serait :

$$
f_Y(y)=\left[ (\varphi^{-1})'\right]_y \times f_X(\varphi^{-1}(y))
$$

La d√©rivation est tellement simple est jolie que je la laisse ici en bas. Partons de la fonction de r√©partition $F_Y(y)$ :

$$
F_Y(y)=\mathbb P(Y\le y)=\mathbb P(\varphi(X) \le y)= \mathbb P(X \le \varphi^{-1}(y))=F_X(\varphi^{-1}(y))
$$

Si on garde les membres les plus √† gauche et √† droite et on les d√©rive par rapport √† $y$, on obtient la th√©or√®me pr√©sent√© initialement.